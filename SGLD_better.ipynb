{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/SGLD_better.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneWjImThPub",
        "outputId": "c931ceab-c216-42c6-a3ab-d459da83a786"
      },
      "id": "YneWjImThPub",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94c44267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c44267",
        "outputId": "fdadb5b1-9377-4fc1-c8f9-b3bc69fc7238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.7.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565154 sha256=0d0fa9b17f1d3925cdee75b6944e0159f84c331468bd70ada039b80dcfe7ffe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=dc5ed5521084d24dc311a7c84fce76c733c001b5509a0351fed7bf1bfdd8c8cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XkP-XKI5mOhv"
      },
      "id": "XkP-XKI5mOhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7f11def2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f11def2",
        "outputId": "5590532b-6048-4471-842b-0e65638e032c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "613d2696",
      "metadata": {
        "id": "613d2696"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b95dd8e4",
      "metadata": {
        "id": "b95dd8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6a5ba769",
      "metadata": {
        "id": "6a5ba769"
      },
      "outputs": [],
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dc555aa8",
      "metadata": {
        "id": "dc555aa8"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "24ee5406",
      "metadata": {
        "id": "24ee5406"
      },
      "outputs": [],
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "189bc6ae",
      "metadata": {
        "id": "189bc6ae"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, num_units)\n",
        "        self.layer3 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        #x_train = (x_train - x_means)/x_stds\n",
        "        #y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        #x_test = (x_test - x_means)/x_stds\n",
        "        #y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=Langevin_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=batch_size, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 0: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    return nets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "3b9afbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9afbcd",
        "outputId": "ed0c46f2-958a-40c5-fea6-7ded817f1b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0:\n",
            "Epoch:    0, Train loss: 19.199 Test loss:  9.730 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.553 Test loss:  0.691 Ensemble loss:    nan RMSE: 1.340 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.081 Test loss:  0.626 Ensemble loss:  0.492 RMSE: 1.050 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.033 Test loss:  0.996 Ensemble loss:  0.478 RMSE: 1.053 Num. networks:  8\n",
            "Epoch: 2000, Train loss:  0.214 Test loss:  1.201 Ensemble loss:  0.459 RMSE: 1.008 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.096 Test loss:  1.220 Ensemble loss:  0.434 RMSE: 0.986 Num. networks: 14\n",
            "Epoch: 3000, Train loss:  0.337 Test loss:  0.577 Ensemble loss:  0.411 RMSE: 0.953 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.131 Test loss:  1.572 Ensemble loss:  0.400 RMSE: 0.939 Num. networks: 20\n",
            "FOLD 1:\n",
            "Epoch:    0, Train loss: 18.830 Test loss: 10.330 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.609 Test loss:  0.608 Ensemble loss:    nan RMSE: 1.475 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.030 Test loss:  0.728 Ensemble loss:  0.510 RMSE: 0.986 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.066 Test loss:  1.273 Ensemble loss:  0.315 RMSE: 0.873 Num. networks:  8\n",
            "Epoch: 2000, Train loss:  0.082 Test loss:  0.782 Ensemble loss:  0.312 RMSE: 0.849 Num. networks: 11\n",
            "Epoch: 2500, Train loss:  0.104 Test loss:  4.176 Ensemble loss:  0.299 RMSE: 0.859 Num. networks: 14\n",
            "Epoch: 3000, Train loss:  0.194 Test loss:  2.064 Ensemble loss:  0.301 RMSE: 0.890 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.029 Test loss:  3.729 Ensemble loss:  0.312 RMSE: 0.907 Num. networks: 20\n",
            "FOLD 2:\n",
            "Epoch:    0, Train loss: 18.494 Test loss: 12.446 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.566 Test loss:  0.500 Ensemble loss:    nan RMSE: 1.346 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.243 Test loss:  1.798 Ensemble loss:  0.498 RMSE: 1.033 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.037 Test loss:  1.135 Ensemble loss:  0.531 RMSE: 1.080 Num. networks:  8\n",
            "Epoch: 2000, Train loss:  0.125 Test loss:  1.323 Ensemble loss:  0.533 RMSE: 1.086 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.092 Test loss:  1.256 Ensemble loss:  0.506 RMSE: 1.076 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.125 Test loss:  2.503 Ensemble loss:  0.452 RMSE: 1.057 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.036 Test loss:  2.953 Ensemble loss:  0.449 RMSE: 1.058 Num. networks: 20\n",
            "FOLD 3:\n",
            "Epoch:    0, Train loss: 18.218 Test loss: 16.585 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.564 Test loss:  0.388 Ensemble loss:    nan RMSE: 1.436 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.085 Test loss:  0.412 Ensemble loss:  0.364 RMSE: 0.942 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.112 Test loss:  1.252 Ensemble loss:  0.285 RMSE: 0.880 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.017 Test loss:  1.403 Ensemble loss:  0.298 RMSE: 0.892 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.057 Test loss:  1.080 Ensemble loss:  0.265 RMSE: 0.883 Num. networks: 14\n",
            "Epoch: 3000, Train loss:  0.335 Test loss:  1.187 Ensemble loss:  0.288 RMSE: 0.895 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.111 Test loss:  1.831 Ensemble loss:  0.304 RMSE: 0.906 Num. networks: 20\n",
            "FOLD 4:\n",
            "Epoch:    0, Train loss: 18.022 Test loss: 14.557 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.468 Test loss:  0.751 Ensemble loss:    nan RMSE: 1.561 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.086 Test loss:  0.972 Ensemble loss:  0.639 RMSE: 1.300 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.079 Test loss:  2.665 Ensemble loss:  0.623 RMSE: 1.219 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.040 Test loss:  1.673 Ensemble loss:  0.590 RMSE: 1.182 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.056 Test loss:  4.333 Ensemble loss:  0.571 RMSE: 1.153 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.155 Test loss:  5.274 Ensemble loss:  0.587 RMSE: 1.144 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.140 Test loss:  8.854 Ensemble loss:  0.597 RMSE: 1.153 Num. networks: 20\n",
            "FOLD 5:\n",
            "Epoch:    0, Train loss: 18.742 Test loss: 13.321 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.579 Test loss:  0.782 Ensemble loss:    nan RMSE: 1.639 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.054 Test loss:  1.053 Ensemble loss:  0.507 RMSE: 1.161 Num. networks:  4\n",
            "Epoch: 1500, Train loss:  0.243 Test loss:  0.708 Ensemble loss:  0.392 RMSE: 0.987 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.099 Test loss:  1.407 Ensemble loss:  0.340 RMSE: 0.906 Num. networks: 11\n",
            "Epoch: 2500, Train loss:  0.074 Test loss:  0.615 Ensemble loss:  0.322 RMSE: 0.876 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.007 Test loss:  2.354 Ensemble loss:  0.318 RMSE: 0.841 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.174 Test loss:  2.535 Ensemble loss:  0.314 RMSE: 0.844 Num. networks: 20\n",
            "FOLD 6:\n",
            "Epoch:    0, Train loss: 17.881 Test loss: 15.276 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.417 Test loss:  0.398 Ensemble loss:    nan RMSE: 1.144 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.018 Test loss:  1.257 Ensemble loss:  0.265 RMSE: 0.846 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.044 Test loss:  0.930 Ensemble loss:  0.301 RMSE: 0.864 Num. networks:  8\n",
            "Epoch: 2000, Train loss:  0.073 Test loss:  1.147 Ensemble loss:  0.304 RMSE: 0.846 Num. networks: 11\n",
            "Epoch: 2500, Train loss:  0.363 Test loss:  0.651 Ensemble loss:  0.291 RMSE: 0.848 Num. networks: 14\n",
            "Epoch: 3000, Train loss:  0.050 Test loss:  0.538 Ensemble loss:  0.281 RMSE: 0.837 Num. networks: 18\n",
            "Epoch: 3300, Train loss:  0.215 Test loss:  0.649 Ensemble loss:  0.283 RMSE: 0.836 Num. networks: 20\n",
            "FOLD 7:\n",
            "Epoch:    0, Train loss: 18.153 Test loss: 15.948 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.515 Test loss:  0.670 Ensemble loss:    nan RMSE: 2.256 Num. networks:  1\n",
            "Epoch: 1000, Train loss: -0.044 Test loss:  1.257 Ensemble loss:  0.667 RMSE: 1.272 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.001 Test loss:  0.732 Ensemble loss:  0.493 RMSE: 1.088 Num. networks:  8\n",
            "Epoch: 2000, Train loss:  0.030 Test loss:  1.759 Ensemble loss:  0.484 RMSE: 1.070 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.020 Test loss:  4.524 Ensemble loss:  0.459 RMSE: 1.002 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.151 Test loss:  3.427 Ensemble loss:  0.418 RMSE: 0.934 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.184 Test loss:  4.077 Ensemble loss:  0.404 RMSE: 0.920 Num. networks: 20\n",
            "FOLD 8:\n",
            "Epoch:    0, Train loss: 18.267 Test loss: 12.737 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.503 Test loss:  0.986 Ensemble loss:    nan RMSE: 1.577 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.073 Test loss:  1.129 Ensemble loss:  0.995 RMSE: 1.307 Num. networks:  4\n",
            "Epoch: 1500, Train loss:  0.011 Test loss:  0.912 Ensemble loss:  0.522 RMSE: 1.169 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.068 Test loss:  1.016 Ensemble loss:  0.464 RMSE: 1.103 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.017 Test loss:  2.111 Ensemble loss:  0.349 RMSE: 1.037 Num. networks: 14\n",
            "Epoch: 3000, Train loss:  0.375 Test loss:  3.165 Ensemble loss:  0.362 RMSE: 1.000 Num. networks: 18\n",
            "Epoch: 3300, Train loss:  0.488 Test loss:  2.407 Ensemble loss:  0.355 RMSE: 0.975 Num. networks: 20\n",
            "FOLD 9:\n",
            "Epoch:    0, Train loss: 18.605 Test loss: 12.508 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.600 Test loss:  0.682 Ensemble loss:    nan RMSE: 1.278 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.104 Test loss:  0.854 Ensemble loss:  0.384 RMSE: 0.958 Num. networks:  4\n",
            "Epoch: 1500, Train loss:  0.121 Test loss:  2.427 Ensemble loss:  0.231 RMSE: 0.837 Num. networks:  8\n",
            "Epoch: 2000, Train loss:  0.484 Test loss:  0.640 Ensemble loss:  0.199 RMSE: 0.810 Num. networks: 11\n",
            "Epoch: 2500, Train loss:  0.068 Test loss:  1.552 Ensemble loss:  0.203 RMSE: 0.797 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.094 Test loss:  3.442 Ensemble loss:  0.211 RMSE: 0.790 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.115 Test loss:  4.318 Ensemble loss:  0.216 RMSE: 0.787 Num. networks: 20\n",
            "Train log. lik. =  -0.132 +/-  0.048\n",
            "Test  log. lik. =  -0.458 +/-  0.103\n",
            "Train RMSE      =   0.636 +/-  0.023\n",
            "Test  RMSE      =   0.932 +/-  0.103\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "data = pd.read_csv('/content/drive/MyDrive/bayesian-deep-learning-master/data/Datasets.csv').values\n",
        "ensemble = train_mc_dropout(data=data, n_splits=10, burn_in=350, mix_time=150, num_nets=20,\n",
        "                            num_units=140, learn_rate=1e-2/len(data), weight_decay=1, log_every=500)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
