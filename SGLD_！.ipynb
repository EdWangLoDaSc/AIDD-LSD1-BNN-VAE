{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/SGLD_%EF%BC%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneWjImThPub",
        "outputId": "c931ceab-c216-42c6-a3ab-d459da83a786"
      },
      "id": "YneWjImThPub",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94c44267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c44267",
        "outputId": "fdadb5b1-9377-4fc1-c8f9-b3bc69fc7238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.7.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565154 sha256=0d0fa9b17f1d3925cdee75b6944e0159f84c331468bd70ada039b80dcfe7ffe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=dc5ed5521084d24dc311a7c84fce76c733c001b5509a0351fed7bf1bfdd8c8cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XkP-XKI5mOhv"
      },
      "id": "XkP-XKI5mOhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7f11def2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f11def2",
        "outputId": "5590532b-6048-4471-842b-0e65638e032c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "613d2696",
      "metadata": {
        "id": "613d2696"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b95dd8e4",
      "metadata": {
        "id": "b95dd8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6a5ba769",
      "metadata": {
        "id": "6a5ba769"
      },
      "outputs": [],
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dc555aa8",
      "metadata": {
        "id": "dc555aa8"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "24ee5406",
      "metadata": {
        "id": "24ee5406"
      },
      "outputs": [],
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "189bc6ae",
      "metadata": {
        "id": "189bc6ae"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, num_units)\n",
        "        self.layer3 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        #x_train = (x_train - x_means)/x_stds\n",
        "        #y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        #x_test = (x_test - x_means)/x_stds\n",
        "        #y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=Langevin_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=batch_size, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 0: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    return nets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "3b9afbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9afbcd",
        "outputId": "48aa6fab-16af-40ac-9cf3-15b9bad09333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0:\n",
            "Epoch:    0, Train loss: 18.938 Test loss:  9.680 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.954 Test loss:  1.106 Ensemble loss:    nan RMSE: 1.716 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.104 Test loss:  0.683 Ensemble loss:  0.607 RMSE: 1.133 Num. networks:  3\n",
            "Epoch: 1500, Train loss:  0.204 Test loss:  0.585 Ensemble loss:  0.545 RMSE: 1.086 Num. networks:  5\n",
            "Epoch: 2000, Train loss: -0.039 Test loss:  0.591 Ensemble loss:  0.497 RMSE: 1.077 Num. networks:  7\n",
            "Epoch: 2500, Train loss: -0.051 Test loss:  0.904 Ensemble loss:  0.485 RMSE: 1.073 Num. networks:  9\n",
            "Epoch: 3000, Train loss: -0.080 Test loss:  1.720 Ensemble loss:  0.455 RMSE: 1.060 Num. networks: 11\n",
            "Epoch: 3500, Train loss:  0.013 Test loss:  0.674 Ensemble loss:  0.440 RMSE: 1.041 Num. networks: 13\n",
            "Epoch: 4000, Train loss:  0.189 Test loss:  1.242 Ensemble loss:  0.429 RMSE: 1.003 Num. networks: 15\n",
            "Epoch: 4500, Train loss: -0.082 Test loss:  2.042 Ensemble loss:  0.422 RMSE: 0.979 Num. networks: 17\n",
            "Epoch: 5000, Train loss: -0.072 Test loss:  2.153 Ensemble loss:  0.409 RMSE: 0.958 Num. networks: 19\n",
            "Epoch: 5500, Train loss:  0.196 Test loss:  1.074 Ensemble loss:  0.421 RMSE: 0.946 Num. networks: 21\n",
            "Epoch: 6000, Train loss:  0.821 Test loss:  0.912 Ensemble loss:  0.434 RMSE: 0.939 Num. networks: 23\n",
            "Epoch: 6500, Train loss: -0.057 Test loss:  1.469 Ensemble loss:  0.445 RMSE: 0.938 Num. networks: 25\n",
            "Epoch: 6600, Train loss:  0.153 Test loss:  1.021 Ensemble loss:  0.445 RMSE: 0.938 Num. networks: 25\n",
            "FOLD 1:\n",
            "Epoch:    0, Train loss: 18.634 Test loss: 10.501 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.542 Test loss:  0.617 Ensemble loss:    nan RMSE: 1.192 Num. networks:  1\n",
            "Epoch: 1000, Train loss: -0.019 Test loss:  2.995 Ensemble loss:  0.638 RMSE: 1.223 Num. networks:  3\n",
            "Epoch: 1500, Train loss:  0.051 Test loss:  2.324 Ensemble loss:  0.676 RMSE: 1.210 Num. networks:  5\n",
            "Epoch: 2000, Train loss:  0.241 Test loss:  0.736 Ensemble loss:  0.517 RMSE: 1.090 Num. networks:  7\n",
            "Epoch: 2500, Train loss: -0.074 Test loss:  0.927 Ensemble loss:  0.457 RMSE: 0.986 Num. networks:  9\n",
            "Epoch: 3000, Train loss: -0.011 Test loss:  2.034 Ensemble loss:  0.477 RMSE: 0.973 Num. networks: 11\n",
            "Epoch: 3500, Train loss: -0.149 Test loss:  3.685 Ensemble loss:  0.483 RMSE: 0.976 Num. networks: 13\n",
            "Epoch: 4000, Train loss:  0.212 Test loss: 22.528 Ensemble loss:  0.474 RMSE: 0.972 Num. networks: 15\n",
            "Epoch: 4500, Train loss:  0.015 Test loss:  9.836 Ensemble loss:  0.468 RMSE: 0.964 Num. networks: 17\n",
            "Epoch: 5000, Train loss:  0.155 Test loss:  4.701 Ensemble loss:  0.477 RMSE: 0.978 Num. networks: 19\n",
            "Epoch: 5500, Train loss:  0.087 Test loss:  1.232 Ensemble loss:  0.484 RMSE: 0.976 Num. networks: 21\n",
            "Epoch: 6000, Train loss:  0.067 Test loss:  1.523 Ensemble loss:  0.485 RMSE: 0.981 Num. networks: 23\n",
            "Epoch: 6500, Train loss:  0.533 Test loss:  1.089 Ensemble loss:  0.501 RMSE: 0.971 Num. networks: 25\n",
            "Epoch: 6600, Train loss: -0.150 Test loss:  4.345 Ensemble loss:  0.501 RMSE: 0.971 Num. networks: 25\n",
            "FOLD 2:\n",
            "Epoch:    0, Train loss: 18.769 Test loss: 12.648 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.605 Test loss:  0.522 Ensemble loss:    nan RMSE: 1.067 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.053 Test loss:  0.528 Ensemble loss:  0.303 RMSE: 0.907 Num. networks:  3\n",
            "Epoch: 1500, Train loss:  0.096 Test loss:  0.956 Ensemble loss:  0.336 RMSE: 0.942 Num. networks:  5\n",
            "Epoch: 2000, Train loss:  0.063 Test loss:  1.090 Ensemble loss:  0.361 RMSE: 0.944 Num. networks:  7\n",
            "Epoch: 2500, Train loss: -0.188 Test loss:  1.425 Ensemble loss:  0.293 RMSE: 0.917 Num. networks:  9\n",
            "Epoch: 3000, Train loss: -0.186 Test loss:  4.601 Ensemble loss:  0.292 RMSE: 0.913 Num. networks: 11\n",
            "Epoch: 3500, Train loss: -0.042 Test loss:  4.217 Ensemble loss:  0.316 RMSE: 0.927 Num. networks: 13\n",
            "Epoch: 4000, Train loss: -0.054 Test loss:  3.253 Ensemble loss:  0.324 RMSE: 0.929 Num. networks: 15\n",
            "Epoch: 4500, Train loss:  0.233 Test loss:  2.683 Ensemble loss:  0.342 RMSE: 0.950 Num. networks: 17\n",
            "Epoch: 5000, Train loss:  0.571 Test loss:  4.866 Ensemble loss:  0.365 RMSE: 0.954 Num. networks: 19\n",
            "Epoch: 5500, Train loss:  0.175 Test loss:  3.620 Ensemble loss:  0.368 RMSE: 0.957 Num. networks: 21\n",
            "Epoch: 6000, Train loss: -0.129 Test loss: 34.624 Ensemble loss:  0.371 RMSE: 0.968 Num. networks: 23\n",
            "Epoch: 6500, Train loss:  0.209 Test loss:  3.248 Ensemble loss:  0.378 RMSE: 0.977 Num. networks: 25\n",
            "Epoch: 6600, Train loss: -0.059 Test loss:  6.187 Ensemble loss:  0.378 RMSE: 0.977 Num. networks: 25\n",
            "FOLD 3:\n",
            "Epoch:    0, Train loss: 17.905 Test loss: 16.333 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.704 Test loss:  0.744 Ensemble loss:    nan RMSE: 1.421 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.041 Test loss:  0.533 Ensemble loss:  0.366 RMSE: 0.934 Num. networks:  3\n",
            "Epoch: 1500, Train loss:  0.010 Test loss:  1.000 Ensemble loss:  0.294 RMSE: 0.857 Num. networks:  5\n",
            "Epoch: 2000, Train loss:  0.122 Test loss:  1.227 Ensemble loss:  0.266 RMSE: 0.835 Num. networks:  7\n",
            "Epoch: 2500, Train loss: -0.113 Test loss:  0.873 Ensemble loss:  0.233 RMSE: 0.806 Num. networks:  9\n",
            "Epoch: 3000, Train loss:  0.302 Test loss:  0.800 Ensemble loss:  0.222 RMSE: 0.798 Num. networks: 11\n",
            "Epoch: 3500, Train loss: -0.100 Test loss:  3.591 Ensemble loss:  0.199 RMSE: 0.787 Num. networks: 13\n",
            "Epoch: 4000, Train loss: -0.201 Test loss:  1.743 Ensemble loss:  0.185 RMSE: 0.784 Num. networks: 15\n",
            "Epoch: 4500, Train loss: -0.156 Test loss:  3.641 Ensemble loss:  0.187 RMSE: 0.786 Num. networks: 17\n",
            "Epoch: 5000, Train loss: -0.045 Test loss:  6.490 Ensemble loss:  0.188 RMSE: 0.791 Num. networks: 19\n",
            "Epoch: 5500, Train loss: -0.080 Test loss:  3.364 Ensemble loss:  0.197 RMSE: 0.799 Num. networks: 21\n",
            "Epoch: 6000, Train loss:  0.207 Test loss:  1.422 Ensemble loss:  0.217 RMSE: 0.802 Num. networks: 23\n",
            "Epoch: 6500, Train loss: -0.091 Test loss:  1.498 Ensemble loss:  0.216 RMSE: 0.801 Num. networks: 25\n",
            "Epoch: 6600, Train loss: -0.049 Test loss:  1.651 Ensemble loss:  0.216 RMSE: 0.801 Num. networks: 25\n",
            "FOLD 4:\n",
            "Epoch:    0, Train loss: 18.400 Test loss: 14.892 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.961 Test loss:  1.055 Ensemble loss:    nan RMSE: 1.955 Num. networks:  1\n",
            "Epoch: 1000, Train loss: -0.010 Test loss:  1.759 Ensemble loss:  0.746 RMSE: 1.385 Num. networks:  3\n",
            "Epoch: 1500, Train loss: -0.089 Test loss:  3.295 Ensemble loss:  0.631 RMSE: 1.250 Num. networks:  5\n",
            "Epoch: 2000, Train loss:  0.215 Test loss:  4.084 Ensemble loss:  0.631 RMSE: 1.203 Num. networks:  7\n",
            "Epoch: 2500, Train loss:  0.359 Test loss:  3.841 Ensemble loss:  0.671 RMSE: 1.206 Num. networks:  9\n",
            "Epoch: 3000, Train loss: -0.237 Test loss: 23.347 Ensemble loss:  0.689 RMSE: 1.212 Num. networks: 11\n",
            "Epoch: 3500, Train loss:  0.517 Test loss:  2.796 Ensemble loss:  0.684 RMSE: 1.222 Num. networks: 13\n",
            "Epoch: 4000, Train loss: -0.071 Test loss:  5.574 Ensemble loss:  0.692 RMSE: 1.236 Num. networks: 15\n",
            "Epoch: 4500, Train loss: -0.209 Test loss: 25.965 Ensemble loss:  0.722 RMSE: 1.262 Num. networks: 17\n",
            "Epoch: 5000, Train loss: -0.259 Test loss:  8.868 Ensemble loss:  0.712 RMSE: 1.260 Num. networks: 19\n",
            "Epoch: 5500, Train loss: -0.199 Test loss:  6.148 Ensemble loss:  0.686 RMSE: 1.235 Num. networks: 21\n",
            "Epoch: 6000, Train loss:  0.362 Test loss:  1.451 Ensemble loss:  0.685 RMSE: 1.225 Num. networks: 23\n",
            "Epoch: 6500, Train loss:  0.238 Test loss:  1.727 Ensemble loss:  0.680 RMSE: 1.222 Num. networks: 25\n",
            "Epoch: 6600, Train loss: -0.156 Test loss:  4.150 Ensemble loss:  0.680 RMSE: 1.222 Num. networks: 25\n",
            "FOLD 5:\n",
            "Epoch:    0, Train loss: 18.248 Test loss: 13.403 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.491 Test loss:  0.453 Ensemble loss:    nan RMSE: 1.199 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.038 Test loss:  0.408 Ensemble loss:  0.309 RMSE: 0.967 Num. networks:  3\n",
            "Epoch: 1500, Train loss:  0.026 Test loss:  0.737 Ensemble loss:  0.268 RMSE: 0.899 Num. networks:  5\n",
            "Epoch: 2000, Train loss:  0.134 Test loss:  0.601 Ensemble loss:  0.243 RMSE: 0.893 Num. networks:  7\n",
            "Epoch: 2500, Train loss: -0.107 Test loss:  0.852 Ensemble loss:  0.210 RMSE: 0.859 Num. networks:  9\n",
            "Epoch: 3000, Train loss: -0.148 Test loss:  1.268 Ensemble loss:  0.210 RMSE: 0.859 Num. networks: 11\n",
            "Epoch: 3500, Train loss: -0.138 Test loss:  1.776 Ensemble loss:  0.225 RMSE: 0.865 Num. networks: 13\n",
            "Epoch: 4000, Train loss: -0.139 Test loss:  2.223 Ensemble loss:  0.236 RMSE: 0.855 Num. networks: 15\n",
            "Epoch: 4500, Train loss: -0.144 Test loss:  4.158 Ensemble loss:  0.258 RMSE: 0.873 Num. networks: 17\n",
            "Epoch: 5000, Train loss: -0.056 Test loss:  2.205 Ensemble loss:  0.277 RMSE: 0.889 Num. networks: 19\n",
            "Epoch: 5500, Train loss: -0.158 Test loss:  3.915 Ensemble loss:  0.273 RMSE: 0.885 Num. networks: 21\n",
            "Epoch: 6000, Train loss:  0.713 Test loss:  0.662 Ensemble loss:  0.286 RMSE: 0.883 Num. networks: 23\n",
            "Epoch: 6500, Train loss: -0.071 Test loss:  2.863 Ensemble loss:  0.293 RMSE: 0.887 Num. networks: 25\n",
            "Epoch: 6600, Train loss:  0.133 Test loss:  1.493 Ensemble loss:  0.293 RMSE: 0.887 Num. networks: 25\n",
            "FOLD 6:\n",
            "Epoch:    0, Train loss: 18.367 Test loss: 15.393 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.720 Test loss:  0.815 Ensemble loss:    nan RMSE: 1.544 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.046 Test loss:  0.603 Ensemble loss:  0.527 RMSE: 1.065 Num. networks:  3\n",
            "Epoch: 1500, Train loss:  0.210 Test loss:  0.495 Ensemble loss:  0.399 RMSE: 0.914 Num. networks:  5\n",
            "Epoch: 2000, Train loss:  0.066 Test loss:  2.432 Ensemble loss:  0.353 RMSE: 0.828 Num. networks:  7\n",
            "Epoch: 2500, Train loss:  0.269 Test loss:  0.908 Ensemble loss:  0.338 RMSE: 0.802 Num. networks:  9\n",
            "Epoch: 3000, Train loss:  0.197 Test loss:  1.483 Ensemble loss:  0.316 RMSE: 0.783 Num. networks: 11\n",
            "Epoch: 3500, Train loss: -0.035 Test loss:  2.303 Ensemble loss:  0.296 RMSE: 0.770 Num. networks: 13\n",
            "Epoch: 4000, Train loss: -0.116 Test loss:  1.382 Ensemble loss:  0.292 RMSE: 0.767 Num. networks: 15\n",
            "Epoch: 4500, Train loss: -0.209 Test loss:  3.487 Ensemble loss:  0.281 RMSE: 0.768 Num. networks: 17\n",
            "Epoch: 5000, Train loss: -0.034 Test loss:  1.947 Ensemble loss:  0.287 RMSE: 0.776 Num. networks: 19\n",
            "Epoch: 5500, Train loss: -0.047 Test loss:  2.498 Ensemble loss:  0.306 RMSE: 0.782 Num. networks: 21\n",
            "Epoch: 6000, Train loss: -0.106 Test loss:  5.776 Ensemble loss:  0.305 RMSE: 0.793 Num. networks: 23\n",
            "Epoch: 6500, Train loss:  0.263 Test loss:  1.469 Ensemble loss:  0.297 RMSE: 0.792 Num. networks: 25\n",
            "Epoch: 6600, Train loss:  0.113 Test loss:  1.346 Ensemble loss:  0.297 RMSE: 0.792 Num. networks: 25\n",
            "FOLD 7:\n",
            "Epoch:    0, Train loss: 18.329 Test loss: 15.989 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.495 Test loss:  0.733 Ensemble loss:    nan RMSE: 1.441 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.075 Test loss:  0.719 Ensemble loss:  0.535 RMSE: 1.145 Num. networks:  3\n",
            "Epoch: 1500, Train loss:  0.044 Test loss:  0.807 Ensemble loss:  0.424 RMSE: 1.079 Num. networks:  5\n",
            "Epoch: 2000, Train loss: -0.124 Test loss:  0.660 Ensemble loss:  0.306 RMSE: 0.982 Num. networks:  7\n",
            "Epoch: 2500, Train loss: -0.147 Test loss:  1.104 Ensemble loss:  0.251 RMSE: 0.922 Num. networks:  9\n",
            "Epoch: 3000, Train loss: -0.121 Test loss:  1.359 Ensemble loss:  0.246 RMSE: 0.904 Num. networks: 11\n",
            "Epoch: 3500, Train loss: -0.199 Test loss:  0.847 Ensemble loss:  0.203 RMSE: 0.870 Num. networks: 13\n",
            "Epoch: 4000, Train loss:  0.156 Test loss:  0.670 Ensemble loss:  0.191 RMSE: 0.857 Num. networks: 15\n",
            "Epoch: 4500, Train loss:  0.087 Test loss:  1.679 Ensemble loss:  0.212 RMSE: 0.863 Num. networks: 17\n",
            "Epoch: 5000, Train loss: -0.177 Test loss:  2.541 Ensemble loss:  0.230 RMSE: 0.880 Num. networks: 19\n",
            "Epoch: 5500, Train loss: -0.030 Test loss:  2.186 Ensemble loss:  0.234 RMSE: 0.877 Num. networks: 21\n",
            "Epoch: 6000, Train loss: -0.118 Test loss:  3.702 Ensemble loss:  0.218 RMSE: 0.861 Num. networks: 23\n",
            "Epoch: 6500, Train loss:  0.245 Test loss:  1.037 Ensemble loss:  0.210 RMSE: 0.852 Num. networks: 25\n",
            "Epoch: 6600, Train loss: -0.111 Test loss:  1.184 Ensemble loss:  0.210 RMSE: 0.852 Num. networks: 25\n",
            "FOLD 8:\n",
            "Epoch:    0, Train loss: 18.339 Test loss: 12.863 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.553 Test loss:  1.154 Ensemble loss:    nan RMSE: 1.322 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.093 Test loss:  8.070 Ensemble loss:  0.913 RMSE: 1.128 Num. networks:  3\n",
            "Epoch: 1500, Train loss: -0.078 Test loss: 11.756 Ensemble loss:  0.830 RMSE: 1.090 Num. networks:  5\n",
            "Epoch: 2000, Train loss: -0.058 Test loss: 13.435 Ensemble loss:  0.797 RMSE: 1.113 Num. networks:  7\n",
            "Epoch: 2500, Train loss:  0.087 Test loss:  1.188 Ensemble loss:  0.538 RMSE: 1.096 Num. networks:  9\n",
            "Epoch: 3000, Train loss:  0.139 Test loss: 13.911 Ensemble loss:  0.533 RMSE: 1.094 Num. networks: 11\n",
            "Epoch: 3500, Train loss:  0.224 Test loss:  0.739 Ensemble loss:  0.464 RMSE: 1.078 Num. networks: 13\n",
            "Epoch: 4000, Train loss: -0.097 Test loss:  1.247 Ensemble loss:  0.384 RMSE: 1.051 Num. networks: 15\n",
            "Epoch: 4500, Train loss:  0.219 Test loss:  1.717 Ensemble loss:  0.335 RMSE: 1.006 Num. networks: 17\n",
            "Epoch: 5000, Train loss: -0.021 Test loss:  2.089 Ensemble loss:  0.272 RMSE: 0.968 Num. networks: 19\n",
            "Epoch: 5500, Train loss: -0.046 Test loss:  1.776 Ensemble loss:  0.248 RMSE: 0.943 Num. networks: 21\n",
            "Epoch: 6000, Train loss:  0.839 Test loss:  1.091 Ensemble loss:  0.274 RMSE: 0.938 Num. networks: 23\n",
            "Epoch: 6500, Train loss:  0.105 Test loss:  2.286 Ensemble loss:  0.285 RMSE: 0.952 Num. networks: 25\n",
            "Epoch: 6600, Train loss:  0.046 Test loss:  6.901 Ensemble loss:  0.285 RMSE: 0.952 Num. networks: 25\n",
            "FOLD 9:\n",
            "Epoch:    0, Train loss: 18.797 Test loss: 12.466 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.792 Test loss:  0.654 Ensemble loss:    nan RMSE: 1.224 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.085 Test loss:  0.334 Ensemble loss:  0.368 RMSE: 0.946 Num. networks:  3\n",
            "Epoch: 1500, Train loss:  0.033 Test loss:  0.350 Ensemble loss:  0.270 RMSE: 0.848 Num. networks:  5\n",
            "Epoch: 2000, Train loss: -0.041 Test loss:  2.738 Ensemble loss:  0.225 RMSE: 0.826 Num. networks:  7\n",
            "Epoch: 2500, Train loss: -0.158 Test loss:  2.077 Ensemble loss:  0.210 RMSE: 0.818 Num. networks:  9\n",
            "Epoch: 3000, Train loss: -0.126 Test loss:  0.827 Ensemble loss:  0.192 RMSE: 0.809 Num. networks: 11\n",
            "Epoch: 3500, Train loss:  0.065 Test loss:  2.702 Ensemble loss:  0.182 RMSE: 0.799 Num. networks: 13\n",
            "Epoch: 4000, Train loss: -0.141 Test loss:  6.999 Ensemble loss:  0.190 RMSE: 0.814 Num. networks: 15\n",
            "Epoch: 4500, Train loss:  0.269 Test loss:  1.686 Ensemble loss:  0.188 RMSE: 0.806 Num. networks: 17\n",
            "Epoch: 5000, Train loss:  0.094 Test loss:  0.939 Ensemble loss:  0.186 RMSE: 0.801 Num. networks: 19\n",
            "Epoch: 5500, Train loss: -0.158 Test loss:  4.936 Ensemble loss:  0.185 RMSE: 0.800 Num. networks: 21\n",
            "Epoch: 6000, Train loss:  0.374 Test loss:  1.958 Ensemble loss:  0.199 RMSE: 0.812 Num. networks: 23\n",
            "Epoch: 6500, Train loss:  0.044 Test loss:  2.196 Ensemble loss:  0.205 RMSE: 0.818 Num. networks: 25\n",
            "Epoch: 6600, Train loss: -0.060 Test loss:  2.134 Ensemble loss:  0.205 RMSE: 0.818 Num. networks: 25\n",
            "Train log. lik. =  -0.069 +/-  0.035\n",
            "Test  log. lik. =  -0.446 +/-  0.141\n",
            "Train RMSE      =   0.544 +/-  0.026\n",
            "Test  RMSE      =   0.921 +/-  0.120\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "data = pd.read_csv('/content/drive/MyDrive/bayesian-deep-learning-master/data/Datasets.csv').values\n",
        "ensemble = train_mc_dropout(data=data, n_splits=10, burn_in=350, mix_time=250, num_nets=25,\n",
        "                            num_units=120, learn_rate=1e-2/len(data), weight_decay=1, log_every=500)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}