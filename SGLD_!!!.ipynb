{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/SGLD_!!!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneWjImThPub",
        "outputId": "c931ceab-c216-42c6-a3ab-d459da83a786"
      },
      "id": "YneWjImThPub",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94c44267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c44267",
        "outputId": "fdadb5b1-9377-4fc1-c8f9-b3bc69fc7238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.7.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565154 sha256=0d0fa9b17f1d3925cdee75b6944e0159f84c331468bd70ada039b80dcfe7ffe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=dc5ed5521084d24dc311a7c84fce76c733c001b5509a0351fed7bf1bfdd8c8cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XkP-XKI5mOhv"
      },
      "id": "XkP-XKI5mOhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7f11def2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f11def2",
        "outputId": "5590532b-6048-4471-842b-0e65638e032c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "613d2696",
      "metadata": {
        "id": "613d2696"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b95dd8e4",
      "metadata": {
        "id": "b95dd8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6a5ba769",
      "metadata": {
        "id": "6a5ba769"
      },
      "outputs": [],
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dc555aa8",
      "metadata": {
        "id": "dc555aa8"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "24ee5406",
      "metadata": {
        "id": "24ee5406"
      },
      "outputs": [],
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "189bc6ae",
      "metadata": {
        "id": "189bc6ae"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, num_units)\n",
        "        self.layer3 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        #x_train = (x_train - x_means)/x_stds\n",
        "        #y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        #x_test = (x_test - x_means)/x_stds\n",
        "        #y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=Langevin_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=batch_size, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 0: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    return nets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "3b9afbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9afbcd",
        "outputId": "b471175e-2034-498c-c38f-ec33d942e663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0:\n",
            "Epoch:    0, Train loss: 19.092 Test loss:  9.658 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  1.984 Test loss:  1.822 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.160 Test loss:  1.117 Ensemble loss:  0.786 RMSE: 1.331 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.029 Test loss:  1.397 Ensemble loss:  0.737 RMSE: 1.254 Num. networks:  4\n",
            "Epoch: 2000, Train loss:  0.020 Test loss:  1.118 Ensemble loss:  0.714 RMSE: 1.207 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.044 Test loss:  2.108 Ensemble loss:  0.628 RMSE: 1.133 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.128 Test loss:  1.177 Ensemble loss:  0.580 RMSE: 1.105 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.104 Test loss:  1.757 Ensemble loss:  0.574 RMSE: 1.107 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.115 Test loss:  4.366 Ensemble loss:  0.592 RMSE: 1.132 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.153 Test loss:  6.418 Ensemble loss:  0.584 RMSE: 1.119 Num. networks: 14\n",
            "Epoch: 5000, Train loss:  0.194 Test loss:  1.346 Ensemble loss:  0.579 RMSE: 1.112 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.191 Test loss: 10.872 Ensemble loss:  0.558 RMSE: 1.100 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.457 Test loss:  1.718 Ensemble loss:  0.539 RMSE: 1.085 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.316 Test loss:  1.466 Ensemble loss:  0.528 RMSE: 1.062 Num. networks: 20\n",
            "Epoch: 7000, Train loss:  0.463 Test loss:  1.624 Ensemble loss:  0.523 RMSE: 1.048 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.173 Test loss:  0.697 Ensemble loss:  0.498 RMSE: 1.029 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.190 Test loss:  2.173 Ensemble loss:  0.496 RMSE: 1.026 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.209 Test loss:  4.196 Ensemble loss:  0.495 RMSE: 1.022 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.092 Test loss: 15.648 Ensemble loss:  0.501 RMSE: 1.022 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.213 Test loss:  4.829 Ensemble loss:  0.500 RMSE: 1.020 Num. networks: 30\n",
            "FOLD 1:\n",
            "Epoch:    0, Train loss: 18.750 Test loss: 10.567 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.712 Test loss:  0.720 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.146 Test loss:  0.714 Ensemble loss:  0.661 RMSE: 1.180 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.045 Test loss:  1.224 Ensemble loss:  0.512 RMSE: 1.077 Num. networks:  4\n",
            "Epoch: 2000, Train loss:  0.033 Test loss:  1.321 Ensemble loss:  0.459 RMSE: 1.030 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.089 Test loss:  0.981 Ensemble loss:  0.462 RMSE: 1.010 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.167 Test loss:  2.376 Ensemble loss:  0.405 RMSE: 0.968 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.137 Test loss:  2.360 Ensemble loss:  0.402 RMSE: 0.956 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.122 Test loss:  1.138 Ensemble loss:  0.390 RMSE: 0.912 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.124 Test loss:  3.702 Ensemble loss:  0.382 RMSE: 0.906 Num. networks: 14\n",
            "Epoch: 5000, Train loss:  0.075 Test loss:  2.585 Ensemble loss:  0.385 RMSE: 0.911 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.180 Test loss:  6.404 Ensemble loss:  0.377 RMSE: 0.905 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.279 Test loss:  1.171 Ensemble loss:  0.366 RMSE: 0.900 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.203 Test loss:  1.011 Ensemble loss:  0.360 RMSE: 0.896 Num. networks: 20\n",
            "Epoch: 7000, Train loss:  0.326 Test loss:  2.392 Ensemble loss:  0.352 RMSE: 0.886 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.253 Test loss:  2.909 Ensemble loss:  0.361 RMSE: 0.889 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.065 Test loss:  1.913 Ensemble loss:  0.360 RMSE: 0.889 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.176 Test loss:  2.643 Ensemble loss:  0.358 RMSE: 0.885 Num. networks: 27\n",
            "Epoch: 9000, Train loss:  0.156 Test loss:  1.068 Ensemble loss:  0.358 RMSE: 0.889 Num. networks: 29\n",
            "Epoch: 9350, Train loss:  0.077 Test loss:  1.039 Ensemble loss:  0.359 RMSE: 0.891 Num. networks: 30\n",
            "FOLD 2:\n",
            "Epoch:    0, Train loss: 18.541 Test loss: 12.577 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.659 Test loss:  0.534 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.008 Test loss:  0.523 Ensemble loss:  0.280 RMSE: 0.958 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.070 Test loss:  0.630 Ensemble loss:  0.206 RMSE: 0.925 Num. networks:  4\n",
            "Epoch: 2000, Train loss:  0.124 Test loss:  1.137 Ensemble loss:  0.252 RMSE: 0.955 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.073 Test loss:  2.222 Ensemble loss:  0.244 RMSE: 0.947 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.154 Test loss:  1.493 Ensemble loss:  0.243 RMSE: 0.937 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.355 Test loss:  0.736 Ensemble loss:  0.235 RMSE: 0.936 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.021 Test loss:  3.647 Ensemble loss:  0.261 RMSE: 0.939 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.015 Test loss:  5.514 Ensemble loss:  0.283 RMSE: 0.950 Num. networks: 14\n",
            "Epoch: 5000, Train loss:  0.127 Test loss:  2.784 Ensemble loss:  0.295 RMSE: 0.961 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.127 Test loss:  2.518 Ensemble loss:  0.288 RMSE: 0.958 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.178 Test loss:  1.128 Ensemble loss:  0.288 RMSE: 0.952 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.142 Test loss:  1.625 Ensemble loss:  0.276 RMSE: 0.950 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.074 Test loss:  3.058 Ensemble loss:  0.271 RMSE: 0.946 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.094 Test loss:  3.396 Ensemble loss:  0.288 RMSE: 0.953 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.166 Test loss:  3.249 Ensemble loss:  0.290 RMSE: 0.955 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.033 Test loss:  1.177 Ensemble loss:  0.284 RMSE: 0.949 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.147 Test loss:  3.055 Ensemble loss:  0.281 RMSE: 0.947 Num. networks: 29\n",
            "Epoch: 9350, Train loss:  0.396 Test loss:  1.121 Ensemble loss:  0.273 RMSE: 0.944 Num. networks: 30\n",
            "FOLD 3:\n",
            "Epoch:    0, Train loss: 17.900 Test loss: 16.366 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.646 Test loss:  0.570 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.031 Test loss:  0.924 Ensemble loss:  0.258 RMSE: 0.841 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.081 Test loss:  0.627 Ensemble loss:  0.156 RMSE: 0.824 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.115 Test loss:  0.842 Ensemble loss:  0.169 RMSE: 0.799 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.626 Test loss:  1.082 Ensemble loss:  0.200 RMSE: 0.777 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.171 Test loss:  2.979 Ensemble loss:  0.187 RMSE: 0.775 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.197 Test loss:  1.835 Ensemble loss:  0.197 RMSE: 0.781 Num. networks: 10\n",
            "Epoch: 4000, Train loss:  0.476 Test loss:  0.888 Ensemble loss:  0.215 RMSE: 0.793 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.051 Test loss:  0.919 Ensemble loss:  0.196 RMSE: 0.775 Num. networks: 14\n",
            "Epoch: 5000, Train loss:  0.302 Test loss:  1.379 Ensemble loss:  0.186 RMSE: 0.770 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.190 Test loss:  6.516 Ensemble loss:  0.179 RMSE: 0.774 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.002 Test loss: 11.723 Ensemble loss:  0.196 RMSE: 0.782 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.026 Test loss:  4.989 Ensemble loss:  0.208 RMSE: 0.788 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.159 Test loss:  2.491 Ensemble loss:  0.223 RMSE: 0.802 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.152 Test loss:  4.532 Ensemble loss:  0.238 RMSE: 0.820 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.116 Test loss:  3.211 Ensemble loss:  0.240 RMSE: 0.823 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.239 Test loss:  1.236 Ensemble loss:  0.243 RMSE: 0.830 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.108 Test loss:  3.885 Ensemble loss:  0.243 RMSE: 0.832 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.082 Test loss:  5.573 Ensemble loss:  0.246 RMSE: 0.830 Num. networks: 30\n",
            "FOLD 4:\n",
            "Epoch:    0, Train loss: 18.035 Test loss: 14.662 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  1.057 Test loss:  1.152 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.021 Test loss:  1.112 Ensemble loss:  0.815 RMSE: 1.354 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.019 Test loss:  1.007 Ensemble loss:  0.674 RMSE: 1.295 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.136 Test loss:  1.668 Ensemble loss:  0.644 RMSE: 1.260 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.031 Test loss:  1.163 Ensemble loss:  0.619 RMSE: 1.239 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.229 Test loss:  3.145 Ensemble loss:  0.605 RMSE: 1.211 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.143 Test loss:  2.422 Ensemble loss:  0.601 RMSE: 1.195 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.143 Test loss:  2.183 Ensemble loss:  0.588 RMSE: 1.183 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.052 Test loss:  2.045 Ensemble loss:  0.569 RMSE: 1.172 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.117 Test loss: 14.172 Ensemble loss:  0.559 RMSE: 1.159 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.264 Test loss:  4.132 Ensemble loss:  0.544 RMSE: 1.131 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.231 Test loss:  5.928 Ensemble loss:  0.533 RMSE: 1.115 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.393 Test loss:  1.667 Ensemble loss:  0.533 RMSE: 1.118 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.225 Test loss:  4.152 Ensemble loss:  0.519 RMSE: 1.115 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.249 Test loss:  3.039 Ensemble loss:  0.519 RMSE: 1.100 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.086 Test loss:  1.937 Ensemble loss:  0.511 RMSE: 1.093 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.228 Test loss: 11.562 Ensemble loss:  0.501 RMSE: 1.084 Num. networks: 27\n",
            "Epoch: 9000, Train loss:  0.120 Test loss:  5.015 Ensemble loss:  0.503 RMSE: 1.077 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.157 Test loss:  5.696 Ensemble loss:  0.502 RMSE: 1.072 Num. networks: 30\n",
            "FOLD 5:\n",
            "Epoch:    0, Train loss: 18.212 Test loss: 13.255 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.618 Test loss:  0.724 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.037 Test loss:  0.575 Ensemble loss:  0.455 RMSE: 1.148 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.034 Test loss:  0.996 Ensemble loss:  0.367 RMSE: 0.980 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.031 Test loss:  1.230 Ensemble loss:  0.383 RMSE: 0.977 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.244 Test loss:  1.060 Ensemble loss:  0.366 RMSE: 0.954 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.092 Test loss:  2.214 Ensemble loss:  0.357 RMSE: 0.944 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.070 Test loss:  2.780 Ensemble loss:  0.359 RMSE: 0.947 Num. networks: 10\n",
            "Epoch: 4000, Train loss:  0.263 Test loss:  1.635 Ensemble loss:  0.351 RMSE: 0.935 Num. networks: 12\n",
            "Epoch: 4500, Train loss:  0.493 Test loss:  0.732 Ensemble loss:  0.349 RMSE: 0.925 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.115 Test loss:  3.197 Ensemble loss:  0.340 RMSE: 0.918 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.128 Test loss:  1.264 Ensemble loss:  0.339 RMSE: 0.908 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.061 Test loss:  1.219 Ensemble loss:  0.323 RMSE: 0.888 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.177 Test loss:  2.489 Ensemble loss:  0.317 RMSE: 0.879 Num. networks: 20\n",
            "Epoch: 7000, Train loss:  0.009 Test loss:  1.333 Ensemble loss:  0.313 RMSE: 0.862 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.023 Test loss:  1.583 Ensemble loss:  0.302 RMSE: 0.848 Num. networks: 24\n",
            "Epoch: 8000, Train loss:  0.463 Test loss:  1.767 Ensemble loss:  0.305 RMSE: 0.848 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.019 Test loss:  2.910 Ensemble loss:  0.309 RMSE: 0.846 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.029 Test loss:  2.508 Ensemble loss:  0.309 RMSE: 0.852 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.129 Test loss:  2.244 Ensemble loss:  0.311 RMSE: 0.858 Num. networks: 30\n",
            "FOLD 6:\n",
            "Epoch:    0, Train loss: 18.272 Test loss: 15.359 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.802 Test loss:  0.766 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.176 Test loss:  0.460 Ensemble loss:  0.377 RMSE: 1.099 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.026 Test loss:  0.474 Ensemble loss:  0.279 RMSE: 0.951 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.016 Test loss:  1.104 Ensemble loss:  0.268 RMSE: 0.925 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.071 Test loss:  1.810 Ensemble loss:  0.278 RMSE: 0.909 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.036 Test loss:  0.928 Ensemble loss:  0.269 RMSE: 0.889 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.228 Test loss:  2.035 Ensemble loss:  0.287 RMSE: 0.903 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.107 Test loss:  2.572 Ensemble loss:  0.319 RMSE: 0.914 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.073 Test loss:  2.962 Ensemble loss:  0.340 RMSE: 0.931 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.120 Test loss: 12.586 Ensemble loss:  0.331 RMSE: 0.923 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.142 Test loss:  3.049 Ensemble loss:  0.337 RMSE: 0.921 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.134 Test loss:  2.769 Ensemble loss:  0.347 RMSE: 0.930 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.294 Test loss:  1.549 Ensemble loss:  0.350 RMSE: 0.936 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.196 Test loss:  2.639 Ensemble loss:  0.350 RMSE: 0.936 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.142 Test loss:  1.847 Ensemble loss:  0.342 RMSE: 0.933 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.145 Test loss:  2.555 Ensemble loss:  0.344 RMSE: 0.934 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.475 Test loss:  1.623 Ensemble loss:  0.346 RMSE: 0.940 Num. networks: 27\n",
            "Epoch: 9000, Train loss:  0.595 Test loss:  1.522 Ensemble loss:  0.356 RMSE: 0.949 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.126 Test loss:  3.564 Ensemble loss:  0.354 RMSE: 0.944 Num. networks: 30\n",
            "FOLD 7:\n",
            "Epoch:    0, Train loss: 18.349 Test loss: 16.067 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.606 Test loss:  0.549 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.109 Test loss:  0.762 Ensemble loss:  0.276 RMSE: 0.968 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.038 Test loss:  1.089 Ensemble loss:  0.271 RMSE: 0.944 Num. networks:  4\n",
            "Epoch: 2000, Train loss:  0.004 Test loss:  1.832 Ensemble loss:  0.211 RMSE: 0.890 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.157 Test loss:  2.961 Ensemble loss:  0.254 RMSE: 0.889 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.027 Test loss:  2.071 Ensemble loss:  0.294 RMSE: 0.925 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.156 Test loss:  3.598 Ensemble loss:  0.303 RMSE: 0.942 Num. networks: 10\n",
            "Epoch: 4000, Train loss:  0.689 Test loss:  0.898 Ensemble loss:  0.310 RMSE: 0.929 Num. networks: 12\n",
            "Epoch: 4500, Train loss:  0.844 Test loss:  1.337 Ensemble loss:  0.345 RMSE: 0.918 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.149 Test loss:  1.864 Ensemble loss:  0.343 RMSE: 0.915 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.121 Test loss:  1.267 Ensemble loss:  0.322 RMSE: 0.896 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.359 Test loss:  2.716 Ensemble loss:  0.314 RMSE: 0.894 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.118 Test loss:  1.419 Ensemble loss:  0.322 RMSE: 0.901 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.089 Test loss:  2.589 Ensemble loss:  0.319 RMSE: 0.916 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.110 Test loss:  3.103 Ensemble loss:  0.335 RMSE: 0.930 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.006 Test loss:  5.048 Ensemble loss:  0.328 RMSE: 0.932 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.205 Test loss:  2.281 Ensemble loss:  0.354 RMSE: 0.939 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.171 Test loss:  7.183 Ensemble loss:  0.367 RMSE: 0.939 Num. networks: 29\n",
            "Epoch: 9350, Train loss:  0.038 Test loss:  4.499 Ensemble loss:  0.367 RMSE: 0.934 Num. networks: 30\n",
            "FOLD 8:\n",
            "Epoch:    0, Train loss: 18.482 Test loss: 12.914 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.551 Test loss:  1.370 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.032 Test loss:  3.594 Ensemble loss:  0.916 RMSE: 1.182 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.049 Test loss:  0.351 Ensemble loss:  0.578 RMSE: 0.997 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.068 Test loss:  4.446 Ensemble loss:  0.517 RMSE: 0.956 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.021 Test loss:  0.923 Ensemble loss:  0.446 RMSE: 0.961 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.141 Test loss:  4.065 Ensemble loss:  0.395 RMSE: 0.956 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.431 Test loss:  0.699 Ensemble loss:  0.363 RMSE: 0.950 Num. networks: 10\n",
            "Epoch: 4000, Train loss:  0.581 Test loss:  0.609 Ensemble loss:  0.250 RMSE: 0.912 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.072 Test loss:  1.725 Ensemble loss:  0.201 RMSE: 0.870 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.177 Test loss:  2.579 Ensemble loss:  0.202 RMSE: 0.867 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.140 Test loss:  7.948 Ensemble loss:  0.197 RMSE: 0.846 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.024 Test loss:  5.020 Ensemble loss:  0.200 RMSE: 0.845 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.130 Test loss:  6.918 Ensemble loss:  0.202 RMSE: 0.844 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.083 Test loss:  1.558 Ensemble loss:  0.210 RMSE: 0.840 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.129 Test loss:  1.438 Ensemble loss:  0.196 RMSE: 0.822 Num. networks: 24\n",
            "Epoch: 8000, Train loss:  0.041 Test loss:  1.396 Ensemble loss:  0.197 RMSE: 0.812 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.057 Test loss:  1.476 Ensemble loss:  0.190 RMSE: 0.792 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.026 Test loss:  1.783 Ensemble loss:  0.191 RMSE: 0.775 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.032 Test loss:  1.391 Ensemble loss:  0.201 RMSE: 0.768 Num. networks: 30\n",
            "FOLD 9:\n",
            "Epoch:    0, Train loss: 18.520 Test loss: 12.503 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  1.712 Test loss:  1.698 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.155 Test loss:  0.439 Ensemble loss:  0.540 RMSE: 1.104 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.191 Test loss:  0.204 Ensemble loss:  0.343 RMSE: 0.884 Num. networks:  4\n",
            "Epoch: 2000, Train loss:  0.104 Test loss:  0.581 Ensemble loss:  0.288 RMSE: 0.816 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.098 Test loss:  2.098 Ensemble loss:  0.245 RMSE: 0.759 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.123 Test loss:  1.578 Ensemble loss:  0.204 RMSE: 0.740 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.077 Test loss:  2.404 Ensemble loss:  0.191 RMSE: 0.735 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.130 Test loss:  4.857 Ensemble loss:  0.185 RMSE: 0.735 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.058 Test loss:  1.797 Ensemble loss:  0.184 RMSE: 0.746 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.144 Test loss:  1.151 Ensemble loss:  0.188 RMSE: 0.744 Num. networks: 15\n",
            "Epoch: 5500, Train loss:  0.157 Test loss:  1.731 Ensemble loss:  0.206 RMSE: 0.752 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.266 Test loss:  0.970 Ensemble loss:  0.189 RMSE: 0.748 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.309 Test loss: 12.669 Ensemble loss:  0.185 RMSE: 0.749 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.082 Test loss:  3.651 Ensemble loss:  0.176 RMSE: 0.738 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.054 Test loss:  4.272 Ensemble loss:  0.180 RMSE: 0.731 Num. networks: 24\n",
            "Epoch: 8000, Train loss:  0.224 Test loss:  1.167 Ensemble loss:  0.177 RMSE: 0.727 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.115 Test loss:  0.503 Ensemble loss:  0.172 RMSE: 0.720 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.133 Test loss:  2.171 Ensemble loss:  0.170 RMSE: 0.717 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.075 Test loss:  1.413 Ensemble loss:  0.168 RMSE: 0.716 Num. networks: 30\n",
            "Train log. lik. =  -0.033 +/-  0.048\n",
            "Test  log. lik. =  -0.423 +/-  0.106\n",
            "Train RMSE      =   0.491 +/-  0.015\n",
            "Test  RMSE      =   0.898 +/-  0.104\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "data = pd.read_csv('/content/drive/MyDrive/bayesian-deep-learning-master/data/Datasets.csv').values\n",
        "ensemble = train_mc_dropout(data=data, n_splits=10, burn_in=350, mix_time=300, num_nets=30,\n",
        "                            num_units=110, learn_rate=1e-2/len(data), weight_decay=0.9, log_every=500)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}