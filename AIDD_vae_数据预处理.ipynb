{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIDD_vae.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEnggIbQ/HsSBzA+vyHdJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/AIDD_vae_%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg6uv-1skpsI",
        "outputId": "68f49a0c-3edd-48b1-ea3b-4563098c6f61"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at ./content; to attempt to forcibly remount, call drive.mount(\"./content\", force_remount=True).\n",
            "Drive already mounted at ./content; to attempt to forcibly remount, call drive.mount(\"./content\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as kr\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ESOvuAO612E2"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/content/MyDrive/CGAN_Regression/CGAN_code/Datasets/Datasets.csv',dtype = np.float32)\n"
      ],
      "metadata": {
        "id": "1pVDb3uH2xU9"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset.drop(['Calculated-pChEMBL'], axis = 1).values\n",
        "y = dataset['Calculated-pChEMBL'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "      X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "Hv0Awye422qX"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = len(X_train[0])\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "hidden_1 = 128 \n",
        "hidden_2 = 64"
      ],
      "metadata": {
        "id": "v41kTD9t3tX6"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train = torch.tensor(X_train)\n",
        "#y_train = torch.tensor(y_train)\n",
        "#X_test = torch.tensor(X_test)\n",
        "#y_test = torch.tensor(y_test)\n",
        "\n",
        "#train = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "#test = torch.utils.data.TensorDataset(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLL5Fu5F4L5u",
        "outputId": "a30530a2-4b07-4987-b1ea-19eefe9acbac"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vh0xpI6q0Ayk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9CYh7sZpS9I",
        "outputId": "756796e0-66c9-4b54-84c3-dd7d556ebfba"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. ... 0. 0. 1.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_dataloader = DataLoader(train, batch_size = 128, shuffle = True)\n",
        "#test_dataloader = DataLoader(test, batch_size = 1, shuffle = False)"
      ],
      "metadata": {
        "id": "h44ZlYOz50py"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqDWhL3f59mJ",
        "outputId": "46a2eda9-35ce-4f81-9776-0d95c3794e6b"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_digits(X, y, encoder, batch_size=128):\n",
        "    \"\"\"Plots labels and MNIST digits as function of 2D latent vector\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    encoder: Model\n",
        "        A Keras Model instance\n",
        "    X: np.ndarray\n",
        "        Test data\n",
        "    y: np.ndarray\n",
        "        Test data labels\n",
        "    batch_size: int\n",
        "        Prediction batch size\n",
        "    \"\"\"\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(X, batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0] Latent Dimension\")\n",
        "    plt.ylabel(\"z[1] Latent Dimension\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1mMW9BnGpfC0"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = kr.Input(shape=(num_features, ))\n",
        "encoder = kr.layers.Dense(hidden_1, activation='relu')(inputs)\n",
        "encoder = kr.layers.Dense(hidden_2, activation='relu')(encoder)\n",
        "encoder_model = kr.Model(inputs, encoder, name='encoder')\n",
        "encoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCYrhDZJpnTO",
        "outputId": "5702e9e8-dd24-474c-9100-1f019ed69dfe"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 512)]             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 73,920\n",
            "Trainable params: 73,920\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = kr.Input(shape=(hidden_2, ))\n",
        "decoder = kr.layers.Dense(hidden_1, activation='relu')(latent_dim)\n",
        "decoder = kr.layers.Dense(num_features, activation='relu')(decoder)\n",
        "decoder_model = kr.Model(latent_dim, decoder, name='decoder')\n",
        "decoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYk03Ocipp6L",
        "outputId": "9d403abf-5134-45ce-b590-710877848a0e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 64)]              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 512)               66048     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 74,368\n",
            "Trainable params: 74,368\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = decoder_model(encoder_model(inputs))\n",
        "mnist_model = kr.Model(inputs, outputs )\n",
        "mnist_model.compile(optimizer='adam', loss='mse')\n",
        "mnist_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtWFU2D_pscK",
        "outputId": "0e159e68-a96d-4e74-c7b9-74c21d254879"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 512)]             0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 64)                73920     \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 512)               74368     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 148,288\n",
            "Trainable params: 148,288\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_model.fit(x=X_train, y=X_train, batch_size=100, shuffle=False, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mvmov1fp0LO",
        "outputId": "7756ddae-b1c4-411d-f97e-d8b61063ad9e"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0316\n",
            "Epoch 2/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0314\n",
            "Epoch 3/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0311\n",
            "Epoch 4/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0312\n",
            "Epoch 5/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0313\n",
            "Epoch 6/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0320\n",
            "Epoch 7/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.0326\n",
            "Epoch 8/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0335\n",
            "Epoch 9/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0327\n",
            "Epoch 10/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0319\n",
            "Epoch 11/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0313\n",
            "Epoch 12/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0308\n",
            "Epoch 13/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0304\n",
            "Epoch 14/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0302\n",
            "Epoch 15/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0299\n",
            "Epoch 16/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0297\n",
            "Epoch 17/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0297\n",
            "Epoch 18/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0294\n",
            "Epoch 19/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0293\n",
            "Epoch 20/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0291\n",
            "Epoch 21/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0289\n",
            "Epoch 22/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0287\n",
            "Epoch 23/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0285\n",
            "Epoch 24/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0284\n",
            "Epoch 25/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0284\n",
            "Epoch 26/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0282\n",
            "Epoch 27/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0281\n",
            "Epoch 28/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0280\n",
            "Epoch 29/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0280\n",
            "Epoch 30/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0277\n",
            "Epoch 31/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0276\n",
            "Epoch 32/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0276\n",
            "Epoch 33/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0274\n",
            "Epoch 34/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0273\n",
            "Epoch 35/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0272\n",
            "Epoch 36/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0271\n",
            "Epoch 37/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0271\n",
            "Epoch 38/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0271\n",
            "Epoch 39/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0271\n",
            "Epoch 40/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0270\n",
            "Epoch 41/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0272\n",
            "Epoch 42/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0270\n",
            "Epoch 43/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0270\n",
            "Epoch 44/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0270\n",
            "Epoch 45/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0268\n",
            "Epoch 46/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0269\n",
            "Epoch 47/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0266\n",
            "Epoch 48/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0266\n",
            "Epoch 49/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0265\n",
            "Epoch 50/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0265\n",
            "Epoch 51/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0263\n",
            "Epoch 52/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0264\n",
            "Epoch 53/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0262\n",
            "Epoch 54/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0261\n",
            "Epoch 55/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0261\n",
            "Epoch 56/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0259\n",
            "Epoch 57/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0257\n",
            "Epoch 58/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0257\n",
            "Epoch 59/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0256\n",
            "Epoch 60/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0255\n",
            "Epoch 61/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0255\n",
            "Epoch 62/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0256\n",
            "Epoch 63/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0255\n",
            "Epoch 64/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0255\n",
            "Epoch 65/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0255\n",
            "Epoch 66/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0256\n",
            "Epoch 67/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0255\n",
            "Epoch 68/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0256\n",
            "Epoch 69/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0257\n",
            "Epoch 70/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0257\n",
            "Epoch 71/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0257\n",
            "Epoch 72/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0256\n",
            "Epoch 73/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0255\n",
            "Epoch 74/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0255\n",
            "Epoch 75/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0253\n",
            "Epoch 76/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0252\n",
            "Epoch 77/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0251\n",
            "Epoch 78/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0250\n",
            "Epoch 79/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0249\n",
            "Epoch 80/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0249\n",
            "Epoch 81/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0248\n",
            "Epoch 82/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0248\n",
            "Epoch 83/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0249\n",
            "Epoch 84/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0248\n",
            "Epoch 85/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0248\n",
            "Epoch 86/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0249\n",
            "Epoch 87/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0248\n",
            "Epoch 88/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0248\n",
            "Epoch 89/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.0248\n",
            "Epoch 90/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0249\n",
            "Epoch 91/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0248\n",
            "Epoch 92/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0248\n",
            "Epoch 93/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0249\n",
            "Epoch 94/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0247\n",
            "Epoch 95/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0248\n",
            "Epoch 96/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0247\n",
            "Epoch 97/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0246\n",
            "Epoch 98/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0246\n",
            "Epoch 99/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0244\n",
            "Epoch 100/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0244\n",
            "Epoch 101/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0243\n",
            "Epoch 102/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0243\n",
            "Epoch 103/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0241\n",
            "Epoch 104/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0241\n",
            "Epoch 105/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0241\n",
            "Epoch 106/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0240\n",
            "Epoch 107/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0240\n",
            "Epoch 108/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0240\n",
            "Epoch 109/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0240\n",
            "Epoch 110/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0241\n",
            "Epoch 111/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0240\n",
            "Epoch 112/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0240\n",
            "Epoch 113/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0241\n",
            "Epoch 114/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0241\n",
            "Epoch 115/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0241\n",
            "Epoch 116/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0242\n",
            "Epoch 117/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0241\n",
            "Epoch 118/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0242\n",
            "Epoch 119/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0242\n",
            "Epoch 120/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0242\n",
            "Epoch 121/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0242\n",
            "Epoch 122/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0241\n",
            "Epoch 123/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0241\n",
            "Epoch 124/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0239\n",
            "Epoch 125/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0240\n",
            "Epoch 126/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0238\n",
            "Epoch 127/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0238\n",
            "Epoch 128/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.0238\n",
            "Epoch 129/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0239\n",
            "Epoch 130/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0238\n",
            "Epoch 131/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0239\n",
            "Epoch 132/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0238\n",
            "Epoch 133/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0239\n",
            "Epoch 134/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0239\n",
            "Epoch 135/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0240\n",
            "Epoch 136/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0239\n",
            "Epoch 137/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0241\n",
            "Epoch 138/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0238\n",
            "Epoch 139/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0239\n",
            "Epoch 140/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0240\n",
            "Epoch 141/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0239\n",
            "Epoch 142/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0240\n",
            "Epoch 143/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0237\n",
            "Epoch 144/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0238\n",
            "Epoch 145/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0237\n",
            "Epoch 146/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0236\n",
            "Epoch 147/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0236\n",
            "Epoch 148/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0237\n",
            "Epoch 149/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0235\n",
            "Epoch 150/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0237\n",
            "Epoch 151/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0237\n",
            "Epoch 152/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0236\n",
            "Epoch 153/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0236\n",
            "Epoch 154/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0235\n",
            "Epoch 155/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.0235\n",
            "Epoch 156/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0235\n",
            "Epoch 157/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0235\n",
            "Epoch 158/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0234\n",
            "Epoch 159/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0234\n",
            "Epoch 160/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0234\n",
            "Epoch 161/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0234\n",
            "Epoch 162/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0234\n",
            "Epoch 163/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0235\n",
            "Epoch 164/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0235\n",
            "Epoch 165/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0235\n",
            "Epoch 166/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0236\n",
            "Epoch 167/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.0235\n",
            "Epoch 168/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0236\n",
            "Epoch 169/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0234\n",
            "Epoch 170/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0234\n",
            "Epoch 171/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0233\n",
            "Epoch 172/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0233\n",
            "Epoch 173/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0232\n",
            "Epoch 174/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0231\n",
            "Epoch 175/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0230\n",
            "Epoch 176/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0230\n",
            "Epoch 177/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0230\n",
            "Epoch 178/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0229\n",
            "Epoch 179/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0229\n",
            "Epoch 180/200\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 0.0229\n",
            "Epoch 181/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0229\n",
            "Epoch 182/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0229\n",
            "Epoch 183/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0230\n",
            "Epoch 184/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0230\n",
            "Epoch 185/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0231\n",
            "Epoch 186/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0232\n",
            "Epoch 187/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.0232\n",
            "Epoch 188/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0234\n",
            "Epoch 189/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0235\n",
            "Epoch 190/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0234\n",
            "Epoch 191/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0236\n",
            "Epoch 192/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0236\n",
            "Epoch 193/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0236\n",
            "Epoch 194/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0235\n",
            "Epoch 195/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0235\n",
            "Epoch 196/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0233\n",
            "Epoch 197/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0233\n",
            "Epoch 198/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.0232\n",
            "Epoch 199/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0232\n",
            "Epoch 200/200\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.0231\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa71c303d10>"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(y_true, y_pred):    \n",
        "    f, ax = plt.subplots(2, 10, figsize=(15, 4))\n",
        "    for i in range(10):\n",
        "        ax[0][i].imshow(np.reshape(y_true[i])\n",
        "        ax[1][i].imshow(np.reshape(y_pred[i])\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "m5l2p5qUrgXJ"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_vae = mnist_model.predict(X)\n",
        "new_x = pd.DataFrame(y_pred_vae)\n",
        "new_x.to_csv('3_512_vae.csv')\n",
        "\n",
        "n = pd.read_csv('3_512_vae.csv')\n",
        "print(n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUJbF4p_rB0e",
        "outputId": "60cfbba1-f92a-4522-a1c6-e94c01e2a506"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Unnamed: 0         0         1         2         3         4         5  \\\n",
            "0             0  0.000000  1.177279  0.728479  0.000000  0.000000  1.108953   \n",
            "1             1  0.089629  1.081324  0.000000  0.876963  0.013758  0.947484   \n",
            "2             2  0.088274  0.894519  0.000000  0.980179  0.000000  1.100354   \n",
            "3             3  0.000000  0.000000  0.674096  0.000000  0.000000  0.000000   \n",
            "4             4  0.000000  0.000000  0.000000  0.000000  0.000000  0.887474   \n",
            "..          ...       ...       ...       ...       ...       ...       ...   \n",
            "926         926  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "927         927  1.057692  0.000000  0.000000  0.000000  0.000000  0.003997   \n",
            "928         928  1.099436  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "929         929  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "930         930  0.903378  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "       6    7         8  ...       502  503  504       505  506       507  \\\n",
            "0    0.0  0.0  0.000000  ...  0.952549  0.0  0.0  0.000000  0.0  0.038403   \n",
            "1    0.0  0.0  0.123546  ...  0.087508  0.0  0.0  0.000000  0.0  1.039144   \n",
            "2    0.0  0.0  0.150629  ...  0.000000  0.0  0.0  0.108846  0.0  0.977905   \n",
            "3    0.0  0.0  0.000000  ...  1.078365  0.0  0.0  0.000000  0.0  0.089365   \n",
            "4    0.0  0.0  1.002243  ...  1.027154  0.0  0.0  0.000000  0.0  0.845684   \n",
            "..   ...  ...       ...  ...       ...  ...  ...       ...  ...       ...   \n",
            "926  0.0  0.0  0.000000  ...  0.000000  0.0  0.0  0.000000  0.0  0.562432   \n",
            "927  0.0  0.0  0.000000  ...  0.000000  0.0  0.0  0.000000  0.0  1.059057   \n",
            "928  0.0  0.0  0.012955  ...  0.000000  0.0  0.0  0.000000  0.0  0.871749   \n",
            "929  0.0  0.0  0.000000  ...  0.000000  0.0  0.0  0.000000  0.0  0.002728   \n",
            "930  0.0  0.0  0.000000  ...  0.000000  0.0  0.0  0.000000  0.0  0.177492   \n",
            "\n",
            "          508  509  510       511  \n",
            "0    1.083589  0.0  0.0  0.000000  \n",
            "1    0.000000  0.0  0.0  0.127072  \n",
            "2    0.000000  0.0  0.0  0.362053  \n",
            "3    0.000000  0.0  0.0  0.000000  \n",
            "4    0.000000  0.0  0.0  0.000000  \n",
            "..        ...  ...  ...       ...  \n",
            "926  0.000000  0.0  0.0  0.000000  \n",
            "927  0.000000  0.0  0.0  0.000000  \n",
            "928  0.000000  0.0  0.0  0.000000  \n",
            "929  0.535086  0.0  0.0  0.000000  \n",
            "930  0.000000  0.0  0.0  0.000000  \n",
            "\n",
            "[931 rows x 513 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OhDcBc_C6AM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dAnRVoNi6AVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vqI7D5Vb6AX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-xdBJJnX6AaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yCZt1n2z6Acp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dNo6Steu6Aj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick. Instead of sampling from Q(z|X), \n",
        "    sample eps = N(0,I) z = z_mean + sqrt(var)*eps.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    args: list of Tensors\n",
        "        Mean and log of variance of Q(z|X)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    z: Tensor\n",
        "        Sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32, mean=0., stddev=1.0, name='epsilon')\n",
        "    z = z_mean + tf.exp(z_log_var / 2) * eps\n",
        "    return z\n"
      ],
      "metadata": {
        "id": "vq2IaEUDyEBN"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 128\n",
        "latent_dim = 10"
      ],
      "metadata": {
        "id": "5BIHidlfyfRO"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = kr.layers.Input(shape=(num_features, ), name='input')\n",
        "x = kr.layers.Dense(hidden_dim, activation='relu')(inputs)\n",
        "z_mean = kr.layers.Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = kr.layers.Dense(latent_dim, name='z_log_var')(x)"
      ],
      "metadata": {
        "id": "9MJ-G1K5yfTz"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = kr.layers.Lambda(sampling, name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = kr.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyyDxRgizz5f",
        "outputId": "a459b4a1-bd1d-495d-cfeb-2569ecd38c13"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 128)          65664       ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 10)           1290        ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 10)           1290        ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " z (Lambda)                     (None, 10)           0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 68,244\n",
            "Trainable params: 68,244\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_inputs = kr.layers.Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = kr.layers.Dense(hidden_dim, activation='relu')(latent_inputs)\n",
        "outputs = kr.layers.Dense(num_features, activation='relu')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = kr.Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVwLYCfl1Kes",
        "outputId": "02753862-ab88-4896-b7f5-39a214d0b2c7"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " z_sampling (InputLayer)     [(None, 10)]              0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 128)               1408      \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 512)               66048     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67,456\n",
            "Trainable params: 67,456\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = decoder(encoder(inputs)[2])  # Select the Z value from outputs of the encoder\n",
        "vae = kr.Model(inputs, outputs, name='vae')\n"
      ],
      "metadata": {
        "id": "fGPML7DY1u6t"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_loss = tf.losses.mean_squared_error(inputs, outputs)\n",
        "reconstruction_loss = reconstruction_loss * num_features\n",
        "\n",
        "# KL Divergence loss\n",
        "kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "kl_loss = -0.5 * tf.reduce_sum(kl_loss, axis=-1)\n",
        "vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "vae.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzTN2kmS1ywS",
        "outputId": "9c6a602f-6cc2-4e0f-f297-884ce4ef8fb2"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vae\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " encoder (Functional)           [(None, 10),         68244       ['input[0][0]']                  \n",
            "                                 (None, 10),                                                      \n",
            "                                 (None, 10)]                                                      \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, 512)          67456       ['encoder[0][2]']                \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 128)          65664       ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 10)           1290        ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 10)           1290        ['dense_36[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_22 (TFOpL  (None, 10)          0           ['z_log_var[0][0]']              \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.math.square_11 (TFOpLambda)  (None, 10)          0           ['z_mean[0][0]']                 \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_11 (TFOpL  (None, 512)         0           ['decoder[0][0]']                \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_11 (TFOpLambda)        (None, 512)          0           ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.math.subtract_22 (TFOpLambd  (None, 10)          0           ['tf.__operators__.add_22[0][0]',\n",
            " a)                                                               'tf.math.square_11[0][0]']      \n",
            "                                                                                                  \n",
            " tf.math.exp_11 (TFOpLambda)    (None, 10)           0           ['z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_11   (None, 512)         0           ['tf.convert_to_tensor_11[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_11[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.subtract_23 (TFOpLambd  (None, 10)          0           ['tf.math.subtract_22[0][0]',    \n",
            " a)                                                               'tf.math.exp_11[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_22 (TFOpLa  (None,)             0           ['tf.math.squared_difference_11[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum_11 (TFOpLam  (None,)             0           ['tf.math.subtract_23[0][0]']    \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.multiply_22 (TFOpLambd  (None,)             0           ['tf.math.reduce_mean_22[0][0]'] \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.multiply_23 (TFOpLambd  (None,)             0           ['tf.math.reduce_sum_11[0][0]']  \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_23 (TFOpL  (None,)             0           ['tf.math.multiply_22[0][0]',    \n",
            " ambda)                                                           'tf.math.multiply_23[0][0]']    \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_23 (TFOpLa  ()                  0           ['tf.__operators__.add_23[0][0]']\n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_11 (AddLoss)          ()                   0           ['tf.math.reduce_mean_23[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,700\n",
            "Trainable params: 135,700\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae.fit(X_train, epochs=1000, batch_size=256, validation_data=(X_test, None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqbDUzyM14e-",
        "outputId": "95f0cd47-ec55-4fb1-bad0-ded596cbfbf6"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "3/3 [==============================] - 1s 90ms/step - loss: 69.6533 - val_loss: 67.6013\n",
            "Epoch 2/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 65.7942 - val_loss: 64.9063\n",
            "Epoch 3/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 63.6212 - val_loss: 62.8399\n",
            "Epoch 4/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 61.9590 - val_loss: 60.5383\n",
            "Epoch 5/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 59.3056 - val_loss: 58.7741\n",
            "Epoch 6/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 58.1368 - val_loss: 57.6297\n",
            "Epoch 7/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 56.7602 - val_loss: 55.9069\n",
            "Epoch 8/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 55.0495 - val_loss: 55.0547\n",
            "Epoch 9/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 54.0289 - val_loss: 54.0802\n",
            "Epoch 10/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 53.0695 - val_loss: 53.1308\n",
            "Epoch 11/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 52.4490 - val_loss: 52.4880\n",
            "Epoch 12/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 52.3477 - val_loss: 51.7341\n",
            "Epoch 13/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 50.7560 - val_loss: 51.3446\n",
            "Epoch 14/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 50.9157 - val_loss: 50.7571\n",
            "Epoch 15/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 50.2939 - val_loss: 50.4756\n",
            "Epoch 16/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 49.8776 - val_loss: 50.1055\n",
            "Epoch 17/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 49.8474 - val_loss: 49.2629\n",
            "Epoch 18/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 49.1492 - val_loss: 49.0290\n",
            "Epoch 19/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 49.0152 - val_loss: 48.9820\n",
            "Epoch 20/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 48.3555 - val_loss: 48.4120\n",
            "Epoch 21/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 48.0614 - val_loss: 48.2659\n",
            "Epoch 22/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 47.7419 - val_loss: 47.8688\n",
            "Epoch 23/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 47.4786 - val_loss: 47.2747\n",
            "Epoch 24/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 47.4311 - val_loss: 47.2287\n",
            "Epoch 25/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 46.4613 - val_loss: 46.8065\n",
            "Epoch 26/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 46.7090 - val_loss: 46.4561\n",
            "Epoch 27/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 45.7920 - val_loss: 46.0562\n",
            "Epoch 28/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 45.7589 - val_loss: 45.6302\n",
            "Epoch 29/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 45.1623 - val_loss: 45.4547\n",
            "Epoch 30/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 44.7077 - val_loss: 45.2175\n",
            "Epoch 31/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 44.8841 - val_loss: 45.1547\n",
            "Epoch 32/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 44.5014 - val_loss: 44.7195\n",
            "Epoch 33/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 44.1114 - val_loss: 44.6115\n",
            "Epoch 34/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 43.7201 - val_loss: 44.1997\n",
            "Epoch 35/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 43.6049 - val_loss: 44.3964\n",
            "Epoch 36/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 43.2866 - val_loss: 44.1425\n",
            "Epoch 37/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 42.9011 - val_loss: 43.8267\n",
            "Epoch 38/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 42.8139 - val_loss: 43.4571\n",
            "Epoch 39/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 42.3325 - val_loss: 43.4117\n",
            "Epoch 40/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 42.3438 - val_loss: 43.6965\n",
            "Epoch 41/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 42.0668 - val_loss: 43.1935\n",
            "Epoch 42/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 41.9177 - val_loss: 43.4572\n",
            "Epoch 43/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 42.0318 - val_loss: 43.1780\n",
            "Epoch 44/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 41.8629 - val_loss: 43.0832\n",
            "Epoch 45/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 42.1023 - val_loss: 42.8869\n",
            "Epoch 46/1000\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 41.6005 - val_loss: 42.9496\n",
            "Epoch 47/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 41.5061 - val_loss: 42.2888\n",
            "Epoch 48/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 41.2332 - val_loss: 42.4750\n",
            "Epoch 49/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 41.0397 - val_loss: 42.1512\n",
            "Epoch 50/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 41.0325 - val_loss: 42.4981\n",
            "Epoch 51/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 40.8335 - val_loss: 42.4593\n",
            "Epoch 52/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 40.5466 - val_loss: 42.3833\n",
            "Epoch 53/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 40.6215 - val_loss: 41.8972\n",
            "Epoch 54/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 40.7342 - val_loss: 41.9653\n",
            "Epoch 55/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 40.1788 - val_loss: 41.9014\n",
            "Epoch 56/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 40.0651 - val_loss: 41.9508\n",
            "Epoch 57/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 40.1897 - val_loss: 41.4067\n",
            "Epoch 58/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 39.8477 - val_loss: 41.5379\n",
            "Epoch 59/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 39.7076 - val_loss: 41.6360\n",
            "Epoch 60/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 39.6624 - val_loss: 41.8543\n",
            "Epoch 61/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 39.7153 - val_loss: 41.6198\n",
            "Epoch 62/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 39.2888 - val_loss: 40.9457\n",
            "Epoch 63/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 39.5922 - val_loss: 41.2525\n",
            "Epoch 64/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 39.1785 - val_loss: 41.3686\n",
            "Epoch 65/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 39.3945 - val_loss: 40.7980\n",
            "Epoch 66/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 39.1954 - val_loss: 41.0648\n",
            "Epoch 67/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 39.4012 - val_loss: 41.1040\n",
            "Epoch 68/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 39.1390 - val_loss: 40.7898\n",
            "Epoch 69/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 38.9291 - val_loss: 40.7234\n",
            "Epoch 70/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 38.7107 - val_loss: 40.9760\n",
            "Epoch 71/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 39.1147 - val_loss: 40.3061\n",
            "Epoch 72/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 38.6451 - val_loss: 40.4869\n",
            "Epoch 73/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 38.8889 - val_loss: 40.3535\n",
            "Epoch 74/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 38.7253 - val_loss: 40.3427\n",
            "Epoch 75/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 38.6801 - val_loss: 40.5044\n",
            "Epoch 76/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 38.3103 - val_loss: 40.2974\n",
            "Epoch 77/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 38.3170 - val_loss: 40.4966\n",
            "Epoch 78/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 37.9604 - val_loss: 40.1947\n",
            "Epoch 79/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 38.3550 - val_loss: 39.9272\n",
            "Epoch 80/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.7884 - val_loss: 40.6258\n",
            "Epoch 81/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 37.9189 - val_loss: 40.0588\n",
            "Epoch 82/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 38.1068 - val_loss: 40.2393\n",
            "Epoch 83/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.5834 - val_loss: 40.2482\n",
            "Epoch 84/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 37.3607 - val_loss: 39.9090\n",
            "Epoch 85/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 37.7804 - val_loss: 40.3652\n",
            "Epoch 86/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.6035 - val_loss: 40.2308\n",
            "Epoch 87/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.8780 - val_loss: 40.2802\n",
            "Epoch 88/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.7367 - val_loss: 39.7058\n",
            "Epoch 89/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 37.8663 - val_loss: 39.5741\n",
            "Epoch 90/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 37.8890 - val_loss: 39.8607\n",
            "Epoch 91/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.6302 - val_loss: 40.1381\n",
            "Epoch 92/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.4911 - val_loss: 39.6746\n",
            "Epoch 93/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 37.3398 - val_loss: 39.5881\n",
            "Epoch 94/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 37.4283 - val_loss: 39.6587\n",
            "Epoch 95/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 37.0294 - val_loss: 39.4970\n",
            "Epoch 96/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 37.1682 - val_loss: 39.3495\n",
            "Epoch 97/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 37.2553 - val_loss: 39.5433\n",
            "Epoch 98/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 37.2635 - val_loss: 39.7359\n",
            "Epoch 99/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 37.0561 - val_loss: 39.4349\n",
            "Epoch 100/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.9208 - val_loss: 39.3995\n",
            "Epoch 101/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 36.9280 - val_loss: 39.3245\n",
            "Epoch 102/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.6194 - val_loss: 39.3468\n",
            "Epoch 103/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 36.6504 - val_loss: 39.2925\n",
            "Epoch 104/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.8695 - val_loss: 39.2621\n",
            "Epoch 105/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 36.6946 - val_loss: 39.2183\n",
            "Epoch 106/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 36.8042 - val_loss: 39.0541\n",
            "Epoch 107/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.7458 - val_loss: 38.9685\n",
            "Epoch 108/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.3587 - val_loss: 39.4791\n",
            "Epoch 109/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.9710 - val_loss: 38.8323\n",
            "Epoch 110/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.4530 - val_loss: 39.0858\n",
            "Epoch 111/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 36.8086 - val_loss: 39.3273\n",
            "Epoch 112/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.6569 - val_loss: 39.2568\n",
            "Epoch 113/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 36.3265 - val_loss: 39.1469\n",
            "Epoch 114/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 36.1622 - val_loss: 38.9731\n",
            "Epoch 115/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 36.0857 - val_loss: 38.7775\n",
            "Epoch 116/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 36.0917 - val_loss: 38.9390\n",
            "Epoch 117/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.0868 - val_loss: 39.0323\n",
            "Epoch 118/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.2827 - val_loss: 38.6344\n",
            "Epoch 119/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.9953 - val_loss: 38.8622\n",
            "Epoch 120/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 35.8684 - val_loss: 38.8353\n",
            "Epoch 121/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 35.8479 - val_loss: 38.8338\n",
            "Epoch 122/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.2119 - val_loss: 38.8000\n",
            "Epoch 123/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 35.6983 - val_loss: 38.8173\n",
            "Epoch 124/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 36.2480 - val_loss: 38.4946\n",
            "Epoch 125/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 36.1825 - val_loss: 38.5714\n",
            "Epoch 126/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 36.1655 - val_loss: 39.0237\n",
            "Epoch 127/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.7293 - val_loss: 38.9728\n",
            "Epoch 128/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 35.7516 - val_loss: 38.6390\n",
            "Epoch 129/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 35.7565 - val_loss: 38.6553\n",
            "Epoch 130/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 35.8893 - val_loss: 38.4782\n",
            "Epoch 131/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 35.7788 - val_loss: 38.8503\n",
            "Epoch 132/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.7018 - val_loss: 38.4737\n",
            "Epoch 133/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.5988 - val_loss: 38.3685\n",
            "Epoch 134/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 35.5129 - val_loss: 38.7451\n",
            "Epoch 135/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 35.5498 - val_loss: 38.4173\n",
            "Epoch 136/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.3523 - val_loss: 38.5488\n",
            "Epoch 137/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 35.2909 - val_loss: 38.5882\n",
            "Epoch 138/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 35.3979 - val_loss: 38.2298\n",
            "Epoch 139/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 35.3090 - val_loss: 38.4291\n",
            "Epoch 140/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 35.3724 - val_loss: 38.4194\n",
            "Epoch 141/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 35.5738 - val_loss: 38.8255\n",
            "Epoch 142/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.2821 - val_loss: 37.9314\n",
            "Epoch 143/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.1709 - val_loss: 38.2073\n",
            "Epoch 144/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 35.0001 - val_loss: 38.3304\n",
            "Epoch 145/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.5017 - val_loss: 38.0734\n",
            "Epoch 146/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.0243 - val_loss: 38.2650\n",
            "Epoch 147/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.1435 - val_loss: 38.0444\n",
            "Epoch 148/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.0138 - val_loss: 38.2233\n",
            "Epoch 149/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 34.9735 - val_loss: 37.8889\n",
            "Epoch 150/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 35.0011 - val_loss: 38.1194\n",
            "Epoch 151/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.3078 - val_loss: 37.9875\n",
            "Epoch 152/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 34.9842 - val_loss: 38.3134\n",
            "Epoch 153/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 34.9910 - val_loss: 38.1915\n",
            "Epoch 154/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 35.2613 - val_loss: 37.9987\n",
            "Epoch 155/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 34.8897 - val_loss: 38.0956\n",
            "Epoch 156/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 34.7338 - val_loss: 37.9304\n",
            "Epoch 157/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 34.8999 - val_loss: 37.8114\n",
            "Epoch 158/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 34.7240 - val_loss: 37.9021\n",
            "Epoch 159/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 34.9899 - val_loss: 38.4197\n",
            "Epoch 160/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.8742 - val_loss: 38.0211\n",
            "Epoch 161/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 35.0392 - val_loss: 38.1387\n",
            "Epoch 162/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.7889 - val_loss: 37.9302\n",
            "Epoch 163/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 34.6435 - val_loss: 37.6527\n",
            "Epoch 164/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 34.7334 - val_loss: 38.1438\n",
            "Epoch 165/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 34.6469 - val_loss: 38.3004\n",
            "Epoch 166/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.5704 - val_loss: 38.0458\n",
            "Epoch 167/1000\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 34.8993 - val_loss: 37.6848\n",
            "Epoch 168/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.4993 - val_loss: 38.2328\n",
            "Epoch 169/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 35.3329 - val_loss: 37.5009\n",
            "Epoch 170/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 34.8642 - val_loss: 37.8631\n",
            "Epoch 171/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 34.4031 - val_loss: 37.9767\n",
            "Epoch 172/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 34.6379 - val_loss: 37.6896\n",
            "Epoch 173/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 34.5720 - val_loss: 37.7028\n",
            "Epoch 174/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.5842 - val_loss: 37.7028\n",
            "Epoch 175/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 34.3845 - val_loss: 38.0127\n",
            "Epoch 176/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 34.3997 - val_loss: 37.8920\n",
            "Epoch 177/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 34.5399 - val_loss: 37.5773\n",
            "Epoch 178/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.2849 - val_loss: 37.8395\n",
            "Epoch 179/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 34.4552 - val_loss: 37.5719\n",
            "Epoch 180/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 34.5379 - val_loss: 37.8718\n",
            "Epoch 181/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.0880 - val_loss: 37.4625\n",
            "Epoch 182/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.2469 - val_loss: 37.8802\n",
            "Epoch 183/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.9686 - val_loss: 37.6926\n",
            "Epoch 184/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 34.3026 - val_loss: 37.6886\n",
            "Epoch 185/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.2263 - val_loss: 37.4207\n",
            "Epoch 186/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.1264 - val_loss: 38.2104\n",
            "Epoch 187/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 34.0678 - val_loss: 37.5813\n",
            "Epoch 188/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 33.9187 - val_loss: 37.4302\n",
            "Epoch 189/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 34.0476 - val_loss: 37.0960\n",
            "Epoch 190/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 33.9225 - val_loss: 37.2553\n",
            "Epoch 191/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.1289 - val_loss: 37.6206\n",
            "Epoch 192/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.8705 - val_loss: 37.5825\n",
            "Epoch 193/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.4889 - val_loss: 38.2077\n",
            "Epoch 194/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 33.9860 - val_loss: 37.6682\n",
            "Epoch 195/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.9766 - val_loss: 37.5683\n",
            "Epoch 196/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.0552 - val_loss: 37.1551\n",
            "Epoch 197/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.6697 - val_loss: 37.6457\n",
            "Epoch 198/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.9718 - val_loss: 37.6100\n",
            "Epoch 199/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 33.9365 - val_loss: 37.4785\n",
            "Epoch 200/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.8327 - val_loss: 37.2673\n",
            "Epoch 201/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 33.8093 - val_loss: 37.4735\n",
            "Epoch 202/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 33.8628 - val_loss: 37.4753\n",
            "Epoch 203/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 34.0507 - val_loss: 37.2096\n",
            "Epoch 204/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 34.1208 - val_loss: 37.3061\n",
            "Epoch 205/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 33.9268 - val_loss: 37.9681\n",
            "Epoch 206/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 33.4136 - val_loss: 37.5725\n",
            "Epoch 207/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 33.7380 - val_loss: 37.5206\n",
            "Epoch 208/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.7375 - val_loss: 37.2581\n",
            "Epoch 209/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.4669 - val_loss: 37.2803\n",
            "Epoch 210/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.7821 - val_loss: 37.5271\n",
            "Epoch 211/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.3514 - val_loss: 37.0100\n",
            "Epoch 212/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 33.9754 - val_loss: 37.3024\n",
            "Epoch 213/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.2875 - val_loss: 37.6124\n",
            "Epoch 214/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 33.6094 - val_loss: 37.9560\n",
            "Epoch 215/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.6036 - val_loss: 37.2600\n",
            "Epoch 216/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.7654 - val_loss: 37.3473\n",
            "Epoch 217/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.3780 - val_loss: 37.2131\n",
            "Epoch 218/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.6082 - val_loss: 37.4057\n",
            "Epoch 219/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.4619 - val_loss: 37.3373\n",
            "Epoch 220/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 33.1030 - val_loss: 37.5924\n",
            "Epoch 221/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.7177 - val_loss: 37.2193\n",
            "Epoch 222/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.3144 - val_loss: 37.2402\n",
            "Epoch 223/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.0887 - val_loss: 37.1016\n",
            "Epoch 224/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.5228 - val_loss: 36.9377\n",
            "Epoch 225/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.4666 - val_loss: 37.3907\n",
            "Epoch 226/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.6664 - val_loss: 37.2667\n",
            "Epoch 227/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 33.1001 - val_loss: 37.0709\n",
            "Epoch 228/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.4600 - val_loss: 37.6289\n",
            "Epoch 229/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.3658 - val_loss: 37.0176\n",
            "Epoch 230/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.4303 - val_loss: 37.0541\n",
            "Epoch 231/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 33.2835 - val_loss: 36.8271\n",
            "Epoch 232/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.4816 - val_loss: 36.8371\n",
            "Epoch 233/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.1166 - val_loss: 37.2944\n",
            "Epoch 234/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.1600 - val_loss: 37.2789\n",
            "Epoch 235/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.1668 - val_loss: 36.9466\n",
            "Epoch 236/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.2624 - val_loss: 37.5439\n",
            "Epoch 237/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.0793 - val_loss: 37.3385\n",
            "Epoch 238/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.3133 - val_loss: 37.0117\n",
            "Epoch 239/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.8410 - val_loss: 37.2617\n",
            "Epoch 240/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 32.9110 - val_loss: 36.5555\n",
            "Epoch 241/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.0407 - val_loss: 37.1274\n",
            "Epoch 242/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.0840 - val_loss: 36.7133\n",
            "Epoch 243/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 33.2131 - val_loss: 37.4233\n",
            "Epoch 244/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 33.1680 - val_loss: 37.2613\n",
            "Epoch 245/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.4782 - val_loss: 36.9389\n",
            "Epoch 246/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 33.4227 - val_loss: 37.2119\n",
            "Epoch 247/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.8412 - val_loss: 37.0512\n",
            "Epoch 248/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.0393 - val_loss: 37.1659\n",
            "Epoch 249/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 32.8807 - val_loss: 36.9109\n",
            "Epoch 250/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.9951 - val_loss: 36.9608\n",
            "Epoch 251/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 33.0534 - val_loss: 37.2948\n",
            "Epoch 252/1000\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 32.9925 - val_loss: 37.3112\n",
            "Epoch 253/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.0636 - val_loss: 37.2864\n",
            "Epoch 254/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.5641 - val_loss: 36.9217\n",
            "Epoch 255/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 33.0578 - val_loss: 37.0059\n",
            "Epoch 256/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 32.8458 - val_loss: 36.7284\n",
            "Epoch 257/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.8843 - val_loss: 36.9007\n",
            "Epoch 258/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.8268 - val_loss: 37.1418\n",
            "Epoch 259/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 33.2678 - val_loss: 37.0379\n",
            "Epoch 260/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.7810 - val_loss: 37.0494\n",
            "Epoch 261/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.8968 - val_loss: 37.0982\n",
            "Epoch 262/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 33.1061 - val_loss: 37.2945\n",
            "Epoch 263/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.8538 - val_loss: 37.2120\n",
            "Epoch 264/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 32.9655 - val_loss: 37.1531\n",
            "Epoch 265/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 32.7565 - val_loss: 36.9357\n",
            "Epoch 266/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.8373 - val_loss: 36.9251\n",
            "Epoch 267/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.7777 - val_loss: 36.9523\n",
            "Epoch 268/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.4369 - val_loss: 37.2240\n",
            "Epoch 269/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.5087 - val_loss: 37.0797\n",
            "Epoch 270/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 32.4817 - val_loss: 37.1235\n",
            "Epoch 271/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.6570 - val_loss: 37.0928\n",
            "Epoch 272/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.5590 - val_loss: 36.5804\n",
            "Epoch 273/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 33.0072 - val_loss: 37.0360\n",
            "Epoch 274/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 33.2658 - val_loss: 36.6987\n",
            "Epoch 275/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.6328 - val_loss: 36.9999\n",
            "Epoch 276/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.5787 - val_loss: 37.0539\n",
            "Epoch 277/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 32.6246 - val_loss: 37.2374\n",
            "Epoch 278/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 32.9256 - val_loss: 36.5743\n",
            "Epoch 279/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.6497 - val_loss: 37.0100\n",
            "Epoch 280/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.6670 - val_loss: 37.0727\n",
            "Epoch 281/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.9939 - val_loss: 36.9320\n",
            "Epoch 282/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.7370 - val_loss: 36.8557\n",
            "Epoch 283/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 32.6648 - val_loss: 37.2760\n",
            "Epoch 284/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.6457 - val_loss: 36.7814\n",
            "Epoch 285/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.4581 - val_loss: 36.5640\n",
            "Epoch 286/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.6706 - val_loss: 36.9373\n",
            "Epoch 287/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.5981 - val_loss: 36.8217\n",
            "Epoch 288/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.4158 - val_loss: 36.9006\n",
            "Epoch 289/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.2315 - val_loss: 36.6136\n",
            "Epoch 290/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.5251 - val_loss: 36.4695\n",
            "Epoch 291/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.4682 - val_loss: 36.7651\n",
            "Epoch 292/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 32.1396 - val_loss: 36.6466\n",
            "Epoch 293/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.1366 - val_loss: 36.3935\n",
            "Epoch 294/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 32.0189 - val_loss: 36.5104\n",
            "Epoch 295/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.1480 - val_loss: 36.1638\n",
            "Epoch 296/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.0995 - val_loss: 36.5596\n",
            "Epoch 297/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 31.9178 - val_loss: 36.0043\n",
            "Epoch 298/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.9967 - val_loss: 36.5725\n",
            "Epoch 299/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.5166 - val_loss: 36.6740\n",
            "Epoch 300/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.0803 - val_loss: 36.2339\n",
            "Epoch 301/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.9030 - val_loss: 36.4666\n",
            "Epoch 302/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 32.2392 - val_loss: 36.5379\n",
            "Epoch 303/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 32.2437 - val_loss: 36.5340\n",
            "Epoch 304/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.0597 - val_loss: 36.4360\n",
            "Epoch 305/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.1271 - val_loss: 36.4310\n",
            "Epoch 306/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.8845 - val_loss: 36.8511\n",
            "Epoch 307/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.8518 - val_loss: 36.3841\n",
            "Epoch 308/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.9285 - val_loss: 36.3318\n",
            "Epoch 309/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.7436 - val_loss: 36.3859\n",
            "Epoch 310/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.1899 - val_loss: 36.1908\n",
            "Epoch 311/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.0037 - val_loss: 36.6289\n",
            "Epoch 312/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.9367 - val_loss: 36.3191\n",
            "Epoch 313/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.0472 - val_loss: 36.8749\n",
            "Epoch 314/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.8432 - val_loss: 36.6380\n",
            "Epoch 315/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 32.0904 - val_loss: 36.6597\n",
            "Epoch 316/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 31.6254 - val_loss: 36.6346\n",
            "Epoch 317/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 32.1240 - val_loss: 36.6556\n",
            "Epoch 318/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.9978 - val_loss: 36.1843\n",
            "Epoch 319/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.8688 - val_loss: 37.0113\n",
            "Epoch 320/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.9600 - val_loss: 36.3118\n",
            "Epoch 321/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.9243 - val_loss: 36.3918\n",
            "Epoch 322/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.9193 - val_loss: 36.0450\n",
            "Epoch 323/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.9321 - val_loss: 36.0147\n",
            "Epoch 324/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.9096 - val_loss: 36.1441\n",
            "Epoch 325/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.8500 - val_loss: 36.0666\n",
            "Epoch 326/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.8659 - val_loss: 36.2459\n",
            "Epoch 327/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 31.4032 - val_loss: 36.0319\n",
            "Epoch 328/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 32.3835 - val_loss: 36.6536\n",
            "Epoch 329/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.4181 - val_loss: 36.2303\n",
            "Epoch 330/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.1243 - val_loss: 36.4870\n",
            "Epoch 331/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 31.8020 - val_loss: 36.0110\n",
            "Epoch 332/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.8583 - val_loss: 36.3167\n",
            "Epoch 333/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 31.5665 - val_loss: 36.1760\n",
            "Epoch 334/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.6308 - val_loss: 36.1781\n",
            "Epoch 335/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.8453 - val_loss: 36.0682\n",
            "Epoch 336/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.6829 - val_loss: 35.9577\n",
            "Epoch 337/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 32.1463 - val_loss: 36.3954\n",
            "Epoch 338/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.7927 - val_loss: 36.2026\n",
            "Epoch 339/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.9335 - val_loss: 36.4632\n",
            "Epoch 340/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.8375 - val_loss: 36.0214\n",
            "Epoch 341/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 31.4456 - val_loss: 36.2648\n",
            "Epoch 342/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.8149 - val_loss: 35.8147\n",
            "Epoch 343/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.8324 - val_loss: 36.3667\n",
            "Epoch 344/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.8137 - val_loss: 36.0773\n",
            "Epoch 345/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.5141 - val_loss: 36.1636\n",
            "Epoch 346/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.8323 - val_loss: 36.0115\n",
            "Epoch 347/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.6800 - val_loss: 36.1245\n",
            "Epoch 348/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.7224 - val_loss: 36.2639\n",
            "Epoch 349/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.7245 - val_loss: 35.7331\n",
            "Epoch 350/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.7733 - val_loss: 36.3598\n",
            "Epoch 351/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 32.0202 - val_loss: 36.2446\n",
            "Epoch 352/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.3992 - val_loss: 36.3217\n",
            "Epoch 353/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.6736 - val_loss: 36.5275\n",
            "Epoch 354/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 31.5430 - val_loss: 36.3665\n",
            "Epoch 355/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.6488 - val_loss: 36.5615\n",
            "Epoch 356/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 31.8117 - val_loss: 36.6583\n",
            "Epoch 357/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.5993 - val_loss: 36.2311\n",
            "Epoch 358/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.4920 - val_loss: 36.3360\n",
            "Epoch 359/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.8560 - val_loss: 36.1503\n",
            "Epoch 360/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.9562 - val_loss: 36.4039\n",
            "Epoch 361/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.1197 - val_loss: 35.8801\n",
            "Epoch 362/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.5754 - val_loss: 36.2371\n",
            "Epoch 363/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.5990 - val_loss: 35.9901\n",
            "Epoch 364/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.4614 - val_loss: 35.9922\n",
            "Epoch 365/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.8365 - val_loss: 36.2940\n",
            "Epoch 366/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 31.2343 - val_loss: 36.0970\n",
            "Epoch 367/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.2520 - val_loss: 36.6243\n",
            "Epoch 368/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.1838 - val_loss: 36.4078\n",
            "Epoch 369/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.2178 - val_loss: 36.1883\n",
            "Epoch 370/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 32.0224 - val_loss: 36.2941\n",
            "Epoch 371/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.1163 - val_loss: 36.2340\n",
            "Epoch 372/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.4177 - val_loss: 36.3785\n",
            "Epoch 373/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.4037 - val_loss: 35.9371\n",
            "Epoch 374/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.4141 - val_loss: 36.3590\n",
            "Epoch 375/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.3169 - val_loss: 36.6593\n",
            "Epoch 376/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.4067 - val_loss: 36.3390\n",
            "Epoch 377/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.7284 - val_loss: 36.2607\n",
            "Epoch 378/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.2821 - val_loss: 36.1799\n",
            "Epoch 379/1000\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 31.3748 - val_loss: 36.2553\n",
            "Epoch 380/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.5158 - val_loss: 36.4131\n",
            "Epoch 381/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.3296 - val_loss: 35.7752\n",
            "Epoch 382/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.1720 - val_loss: 36.3258\n",
            "Epoch 383/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.3399 - val_loss: 36.1815\n",
            "Epoch 384/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 31.4875 - val_loss: 36.0744\n",
            "Epoch 385/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.2937 - val_loss: 35.8582\n",
            "Epoch 386/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.1422 - val_loss: 36.8139\n",
            "Epoch 387/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.6608 - val_loss: 36.2212\n",
            "Epoch 388/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.0394 - val_loss: 35.8243\n",
            "Epoch 389/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.1790 - val_loss: 36.1339\n",
            "Epoch 390/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 31.7190 - val_loss: 36.4230\n",
            "Epoch 391/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 31.2685 - val_loss: 36.3036\n",
            "Epoch 392/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.9611 - val_loss: 36.4292\n",
            "Epoch 393/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.2013 - val_loss: 36.5791\n",
            "Epoch 394/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.0372 - val_loss: 35.9319\n",
            "Epoch 395/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 31.3213 - val_loss: 36.1893\n",
            "Epoch 396/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.5650 - val_loss: 36.0246\n",
            "Epoch 397/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.4420 - val_loss: 35.8241\n",
            "Epoch 398/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.2405 - val_loss: 36.1179\n",
            "Epoch 399/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.1350 - val_loss: 36.2843\n",
            "Epoch 400/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.4991 - val_loss: 36.1500\n",
            "Epoch 401/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 31.0799 - val_loss: 35.9786\n",
            "Epoch 402/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.3134 - val_loss: 36.4499\n",
            "Epoch 403/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.2353 - val_loss: 35.9521\n",
            "Epoch 404/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.5266 - val_loss: 36.2112\n",
            "Epoch 405/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 31.6667 - val_loss: 36.4055\n",
            "Epoch 406/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 31.1821 - val_loss: 36.2461\n",
            "Epoch 407/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.3213 - val_loss: 36.3490\n",
            "Epoch 408/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.1467 - val_loss: 36.0979\n",
            "Epoch 409/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.0436 - val_loss: 36.0256\n",
            "Epoch 410/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.2897 - val_loss: 36.2506\n",
            "Epoch 411/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.9617 - val_loss: 35.9301\n",
            "Epoch 412/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.4625 - val_loss: 36.7357\n",
            "Epoch 413/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.1191 - val_loss: 36.3657\n",
            "Epoch 414/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 31.1099 - val_loss: 36.1577\n",
            "Epoch 415/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.2336 - val_loss: 36.1606\n",
            "Epoch 416/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.0444 - val_loss: 36.1074\n",
            "Epoch 417/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.4010 - val_loss: 36.1335\n",
            "Epoch 418/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.1443 - val_loss: 36.4989\n",
            "Epoch 419/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 31.0851 - val_loss: 36.3174\n",
            "Epoch 420/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.0939 - val_loss: 36.2557\n",
            "Epoch 421/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.8689 - val_loss: 36.3615\n",
            "Epoch 422/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 31.2567 - val_loss: 36.2950\n",
            "Epoch 423/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.7872 - val_loss: 36.0392\n",
            "Epoch 424/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.9074 - val_loss: 36.0048\n",
            "Epoch 425/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 31.1038 - val_loss: 36.2515\n",
            "Epoch 426/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.0360 - val_loss: 36.4058\n",
            "Epoch 427/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.1216 - val_loss: 36.0457\n",
            "Epoch 428/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.3832 - val_loss: 36.2796\n",
            "Epoch 429/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.9722 - val_loss: 35.9077\n",
            "Epoch 430/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.3683 - val_loss: 36.0242\n",
            "Epoch 431/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.8914 - val_loss: 36.0750\n",
            "Epoch 432/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.9579 - val_loss: 35.5692\n",
            "Epoch 433/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.0569 - val_loss: 36.2405\n",
            "Epoch 434/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.8752 - val_loss: 36.3140\n",
            "Epoch 435/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.9084 - val_loss: 35.8209\n",
            "Epoch 436/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.8851 - val_loss: 36.3143\n",
            "Epoch 437/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.9630 - val_loss: 35.6144\n",
            "Epoch 438/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.7350 - val_loss: 36.1829\n",
            "Epoch 439/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 30.8083 - val_loss: 36.6619\n",
            "Epoch 440/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.0061 - val_loss: 35.8638\n",
            "Epoch 441/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 30.9072 - val_loss: 36.4485\n",
            "Epoch 442/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.2956 - val_loss: 36.1160\n",
            "Epoch 443/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.7790 - val_loss: 36.2823\n",
            "Epoch 444/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.3060 - val_loss: 36.0774\n",
            "Epoch 445/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.9272 - val_loss: 35.9211\n",
            "Epoch 446/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.0892 - val_loss: 36.1514\n",
            "Epoch 447/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.2617 - val_loss: 35.5905\n",
            "Epoch 448/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.8412 - val_loss: 35.9570\n",
            "Epoch 449/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.3801 - val_loss: 36.2179\n",
            "Epoch 450/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.7741 - val_loss: 36.1435\n",
            "Epoch 451/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.7523 - val_loss: 35.8509\n",
            "Epoch 452/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 31.1119 - val_loss: 36.0643\n",
            "Epoch 453/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.9054 - val_loss: 36.1627\n",
            "Epoch 454/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.8800 - val_loss: 36.3599\n",
            "Epoch 455/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 31.1573 - val_loss: 36.3052\n",
            "Epoch 456/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.1646 - val_loss: 36.3868\n",
            "Epoch 457/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.0063 - val_loss: 35.8399\n",
            "Epoch 458/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.5954 - val_loss: 35.9481\n",
            "Epoch 459/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.0799 - val_loss: 36.3675\n",
            "Epoch 460/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 31.1124 - val_loss: 36.0695\n",
            "Epoch 461/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.8963 - val_loss: 35.9087\n",
            "Epoch 462/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.7490 - val_loss: 35.9481\n",
            "Epoch 463/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.8942 - val_loss: 35.7570\n",
            "Epoch 464/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.9225 - val_loss: 36.2751\n",
            "Epoch 465/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.8316 - val_loss: 35.9293\n",
            "Epoch 466/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.1042 - val_loss: 36.0978\n",
            "Epoch 467/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.8289 - val_loss: 36.2766\n",
            "Epoch 468/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.6801 - val_loss: 35.8125\n",
            "Epoch 469/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.1274 - val_loss: 35.9219\n",
            "Epoch 470/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.7496 - val_loss: 35.7509\n",
            "Epoch 471/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 30.5757 - val_loss: 36.1279\n",
            "Epoch 472/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.9590 - val_loss: 36.0572\n",
            "Epoch 473/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.0682 - val_loss: 36.1703\n",
            "Epoch 474/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 30.6956 - val_loss: 36.3106\n",
            "Epoch 475/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.8723 - val_loss: 36.0251\n",
            "Epoch 476/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.9174 - val_loss: 36.0386\n",
            "Epoch 477/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 30.7898 - val_loss: 36.2356\n",
            "Epoch 478/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.5941 - val_loss: 36.4040\n",
            "Epoch 479/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.9201 - val_loss: 36.3378\n",
            "Epoch 480/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 31.0765 - val_loss: 35.9831\n",
            "Epoch 481/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.7351 - val_loss: 36.2946\n",
            "Epoch 482/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 31.0478 - val_loss: 35.9481\n",
            "Epoch 483/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.8751 - val_loss: 35.7746\n",
            "Epoch 484/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.7818 - val_loss: 35.5196\n",
            "Epoch 485/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.6525 - val_loss: 35.8882\n",
            "Epoch 486/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 30.4983 - val_loss: 35.5785\n",
            "Epoch 487/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.4022 - val_loss: 36.0905\n",
            "Epoch 488/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.3994 - val_loss: 35.8314\n",
            "Epoch 489/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.8734 - val_loss: 36.1189\n",
            "Epoch 490/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.7661 - val_loss: 36.0601\n",
            "Epoch 491/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.7857 - val_loss: 35.8158\n",
            "Epoch 492/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.8256 - val_loss: 35.6880\n",
            "Epoch 493/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.5147 - val_loss: 36.0916\n",
            "Epoch 494/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.7755 - val_loss: 36.2082\n",
            "Epoch 495/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.5706 - val_loss: 35.6156\n",
            "Epoch 496/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.5934 - val_loss: 35.8879\n",
            "Epoch 497/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 30.7565 - val_loss: 35.6215\n",
            "Epoch 498/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 30.7843 - val_loss: 36.0108\n",
            "Epoch 499/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.6450 - val_loss: 36.0873\n",
            "Epoch 500/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.6296 - val_loss: 35.9740\n",
            "Epoch 501/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.6785 - val_loss: 35.8424\n",
            "Epoch 502/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.7222 - val_loss: 35.7960\n",
            "Epoch 503/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 31.1403 - val_loss: 35.9063\n",
            "Epoch 504/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.6224 - val_loss: 35.5251\n",
            "Epoch 505/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.7366 - val_loss: 35.8218\n",
            "Epoch 506/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.8575 - val_loss: 35.6679\n",
            "Epoch 507/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.7325 - val_loss: 35.7279\n",
            "Epoch 508/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.3304 - val_loss: 35.8742\n",
            "Epoch 509/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.7614 - val_loss: 35.5664\n",
            "Epoch 510/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 30.5747 - val_loss: 35.7460\n",
            "Epoch 511/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 30.8599 - val_loss: 35.7998\n",
            "Epoch 512/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.4588 - val_loss: 36.1097\n",
            "Epoch 513/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.6453 - val_loss: 35.8817\n",
            "Epoch 514/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.8006 - val_loss: 36.0288\n",
            "Epoch 515/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.6391 - val_loss: 35.9050\n",
            "Epoch 516/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 31.2382 - val_loss: 35.7969\n",
            "Epoch 517/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.3647 - val_loss: 35.7153\n",
            "Epoch 518/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.3354 - val_loss: 36.1382\n",
            "Epoch 519/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.3486 - val_loss: 35.9956\n",
            "Epoch 520/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.4065 - val_loss: 35.8806\n",
            "Epoch 521/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 30.5896 - val_loss: 35.9913\n",
            "Epoch 522/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.6403 - val_loss: 35.6997\n",
            "Epoch 523/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.4553 - val_loss: 35.8171\n",
            "Epoch 524/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.3767 - val_loss: 35.5278\n",
            "Epoch 525/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.6923 - val_loss: 35.7565\n",
            "Epoch 526/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.5060 - val_loss: 36.0358\n",
            "Epoch 527/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.5115 - val_loss: 35.8430\n",
            "Epoch 528/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.5200 - val_loss: 36.4493\n",
            "Epoch 529/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.5510 - val_loss: 35.6981\n",
            "Epoch 530/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.4364 - val_loss: 36.0157\n",
            "Epoch 531/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.4963 - val_loss: 35.9272\n",
            "Epoch 532/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.8753 - val_loss: 36.3489\n",
            "Epoch 533/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.4796 - val_loss: 36.3331\n",
            "Epoch 534/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.3994 - val_loss: 36.1009\n",
            "Epoch 535/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.6017 - val_loss: 36.1180\n",
            "Epoch 536/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.7206 - val_loss: 35.8345\n",
            "Epoch 537/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.7440 - val_loss: 35.8298\n",
            "Epoch 538/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.2326 - val_loss: 35.4889\n",
            "Epoch 539/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.4079 - val_loss: 35.8779\n",
            "Epoch 540/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.4633 - val_loss: 36.0459\n",
            "Epoch 541/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 30.3035 - val_loss: 35.6608\n",
            "Epoch 542/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 30.5186 - val_loss: 36.2242\n",
            "Epoch 543/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.3067 - val_loss: 35.6994\n",
            "Epoch 544/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.5680 - val_loss: 35.6793\n",
            "Epoch 545/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.5652 - val_loss: 35.8516\n",
            "Epoch 546/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.6606 - val_loss: 35.9974\n",
            "Epoch 547/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.3590 - val_loss: 35.6565\n",
            "Epoch 548/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.5904 - val_loss: 35.6027\n",
            "Epoch 549/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.1919 - val_loss: 36.0410\n",
            "Epoch 550/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.6017 - val_loss: 35.5793\n",
            "Epoch 551/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.3365 - val_loss: 35.5514\n",
            "Epoch 552/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.2968 - val_loss: 35.5575\n",
            "Epoch 553/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.6868 - val_loss: 36.1835\n",
            "Epoch 554/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.2007 - val_loss: 35.7641\n",
            "Epoch 555/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.4823 - val_loss: 35.7138\n",
            "Epoch 556/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.5720 - val_loss: 36.0190\n",
            "Epoch 557/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 30.5861 - val_loss: 35.8708\n",
            "Epoch 558/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.6734 - val_loss: 35.7539\n",
            "Epoch 559/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.6374 - val_loss: 35.7489\n",
            "Epoch 560/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.2726 - val_loss: 35.5733\n",
            "Epoch 561/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.5535 - val_loss: 35.7474\n",
            "Epoch 562/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.3539 - val_loss: 36.1495\n",
            "Epoch 563/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 30.4038 - val_loss: 35.4157\n",
            "Epoch 564/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.2187 - val_loss: 36.1412\n",
            "Epoch 565/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.5191 - val_loss: 36.3068\n",
            "Epoch 566/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.4295 - val_loss: 35.5736\n",
            "Epoch 567/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.6840 - val_loss: 36.0549\n",
            "Epoch 568/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 30.4413 - val_loss: 35.9024\n",
            "Epoch 569/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.4861 - val_loss: 35.7987\n",
            "Epoch 570/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.7300 - val_loss: 35.8665\n",
            "Epoch 571/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.3796 - val_loss: 35.9984\n",
            "Epoch 572/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.2425 - val_loss: 35.5996\n",
            "Epoch 573/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 30.3104 - val_loss: 35.8539\n",
            "Epoch 574/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.1816 - val_loss: 35.8143\n",
            "Epoch 575/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.2796 - val_loss: 36.0655\n",
            "Epoch 576/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.3232 - val_loss: 35.9313\n",
            "Epoch 577/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.3233 - val_loss: 35.6372\n",
            "Epoch 578/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.4355 - val_loss: 35.8859\n",
            "Epoch 579/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.4961 - val_loss: 35.9896\n",
            "Epoch 580/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 30.3729 - val_loss: 35.9728\n",
            "Epoch 581/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.5483 - val_loss: 35.8705\n",
            "Epoch 582/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.6253 - val_loss: 35.9322\n",
            "Epoch 583/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.6597 - val_loss: 36.1977\n",
            "Epoch 584/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.0161 - val_loss: 35.7032\n",
            "Epoch 585/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.1723 - val_loss: 35.6299\n",
            "Epoch 586/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.4877 - val_loss: 35.9062\n",
            "Epoch 587/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 30.2336 - val_loss: 36.1346\n",
            "Epoch 588/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.3727 - val_loss: 36.0803\n",
            "Epoch 589/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.1138 - val_loss: 35.7949\n",
            "Epoch 590/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.0325 - val_loss: 36.1077\n",
            "Epoch 591/1000\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 30.2723 - val_loss: 35.9225\n",
            "Epoch 592/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.2204 - val_loss: 35.2160\n",
            "Epoch 593/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.2986 - val_loss: 35.9547\n",
            "Epoch 594/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.3388 - val_loss: 35.4916\n",
            "Epoch 595/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.1567 - val_loss: 36.3029\n",
            "Epoch 596/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.2119 - val_loss: 35.5884\n",
            "Epoch 597/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.2345 - val_loss: 35.3962\n",
            "Epoch 598/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.2182 - val_loss: 35.9201\n",
            "Epoch 599/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.4853 - val_loss: 35.8864\n",
            "Epoch 600/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.4825 - val_loss: 35.7390\n",
            "Epoch 601/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.1003 - val_loss: 35.4793\n",
            "Epoch 602/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.3327 - val_loss: 35.3059\n",
            "Epoch 603/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 30.3164 - val_loss: 35.9589\n",
            "Epoch 604/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.3589 - val_loss: 35.8303\n",
            "Epoch 605/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.1977 - val_loss: 35.8677\n",
            "Epoch 606/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.3625 - val_loss: 35.6524\n",
            "Epoch 607/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 30.3799 - val_loss: 36.0135\n",
            "Epoch 608/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.1637 - val_loss: 35.6741\n",
            "Epoch 609/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.2116 - val_loss: 36.0139\n",
            "Epoch 610/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.8442 - val_loss: 36.1753\n",
            "Epoch 611/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.3093 - val_loss: 35.7900\n",
            "Epoch 612/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.2597 - val_loss: 35.8938\n",
            "Epoch 613/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.0232 - val_loss: 35.7921\n",
            "Epoch 614/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.3438 - val_loss: 35.9798\n",
            "Epoch 615/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.1399 - val_loss: 35.9250\n",
            "Epoch 616/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.4722 - val_loss: 35.5713\n",
            "Epoch 617/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.1992 - val_loss: 35.8545\n",
            "Epoch 618/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.4554 - val_loss: 35.8918\n",
            "Epoch 619/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.8389 - val_loss: 36.0510\n",
            "Epoch 620/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.1402 - val_loss: 35.8073\n",
            "Epoch 621/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.2340 - val_loss: 35.5791\n",
            "Epoch 622/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.7429 - val_loss: 35.8623\n",
            "Epoch 623/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.8643 - val_loss: 35.8416\n",
            "Epoch 624/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.0927 - val_loss: 35.9709\n",
            "Epoch 625/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.6871 - val_loss: 35.8385\n",
            "Epoch 626/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 30.1533 - val_loss: 35.5991\n",
            "Epoch 627/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.3162 - val_loss: 35.9857\n",
            "Epoch 628/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.1380 - val_loss: 35.5988\n",
            "Epoch 629/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.4639 - val_loss: 35.9283\n",
            "Epoch 630/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.4006 - val_loss: 35.7504\n",
            "Epoch 631/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.1579 - val_loss: 36.2215\n",
            "Epoch 632/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.9326 - val_loss: 36.0098\n",
            "Epoch 633/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.1893 - val_loss: 36.0372\n",
            "Epoch 634/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.4050 - val_loss: 35.5463\n",
            "Epoch 635/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 30.2223 - val_loss: 35.6756\n",
            "Epoch 636/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.1277 - val_loss: 36.0161\n",
            "Epoch 637/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 30.4509 - val_loss: 35.9429\n",
            "Epoch 638/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 30.2627 - val_loss: 35.8065\n",
            "Epoch 639/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.1196 - val_loss: 35.6701\n",
            "Epoch 640/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.1884 - val_loss: 35.6401\n",
            "Epoch 641/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.1641 - val_loss: 35.1783\n",
            "Epoch 642/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.1700 - val_loss: 36.1655\n",
            "Epoch 643/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.2946 - val_loss: 35.5216\n",
            "Epoch 644/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.2508 - val_loss: 35.5071\n",
            "Epoch 645/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.8980 - val_loss: 35.8550\n",
            "Epoch 646/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.1380 - val_loss: 35.2532\n",
            "Epoch 647/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 30.0414 - val_loss: 35.8971\n",
            "Epoch 648/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.8932 - val_loss: 35.7194\n",
            "Epoch 649/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 30.1277 - val_loss: 35.6323\n",
            "Epoch 650/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.2745 - val_loss: 35.8696\n",
            "Epoch 651/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.2721 - val_loss: 35.9898\n",
            "Epoch 652/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.3919 - val_loss: 35.8086\n",
            "Epoch 653/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.0426 - val_loss: 35.8279\n",
            "Epoch 654/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.2375 - val_loss: 36.2272\n",
            "Epoch 655/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.2319 - val_loss: 35.7604\n",
            "Epoch 656/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.9086 - val_loss: 35.8496\n",
            "Epoch 657/1000\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 30.1791 - val_loss: 36.3664\n",
            "Epoch 658/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.9805 - val_loss: 35.9021\n",
            "Epoch 659/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.1152 - val_loss: 35.8161\n",
            "Epoch 660/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.0491 - val_loss: 35.7028\n",
            "Epoch 661/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.0487 - val_loss: 35.2880\n",
            "Epoch 662/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.9452 - val_loss: 35.8694\n",
            "Epoch 663/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.1207 - val_loss: 35.6518\n",
            "Epoch 664/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.7639 - val_loss: 35.7353\n",
            "Epoch 665/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.9304 - val_loss: 35.7538\n",
            "Epoch 666/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.9747 - val_loss: 35.5436\n",
            "Epoch 667/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.1561 - val_loss: 35.9306\n",
            "Epoch 668/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.3047 - val_loss: 36.3338\n",
            "Epoch 669/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.0328 - val_loss: 36.0088\n",
            "Epoch 670/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.9342 - val_loss: 35.5851\n",
            "Epoch 671/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.8189 - val_loss: 35.7332\n",
            "Epoch 672/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.9872 - val_loss: 35.3928\n",
            "Epoch 673/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 30.1579 - val_loss: 35.5800\n",
            "Epoch 674/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.0414 - val_loss: 36.1011\n",
            "Epoch 675/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.2092 - val_loss: 35.5407\n",
            "Epoch 676/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.0395 - val_loss: 35.5090\n",
            "Epoch 677/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 30.0558 - val_loss: 35.8089\n",
            "Epoch 678/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.9159 - val_loss: 35.9078\n",
            "Epoch 679/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.6294 - val_loss: 35.9091\n",
            "Epoch 680/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 30.0055 - val_loss: 35.5361\n",
            "Epoch 681/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.9458 - val_loss: 36.0845\n",
            "Epoch 682/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 30.1728 - val_loss: 35.8921\n",
            "Epoch 683/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.9962 - val_loss: 35.3338\n",
            "Epoch 684/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.9908 - val_loss: 35.4531\n",
            "Epoch 685/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 29.9459 - val_loss: 35.5268\n",
            "Epoch 686/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.0041 - val_loss: 35.4796\n",
            "Epoch 687/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.7811 - val_loss: 35.8597\n",
            "Epoch 688/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.2104 - val_loss: 35.6069\n",
            "Epoch 689/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.9411 - val_loss: 35.7997\n",
            "Epoch 690/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.7419 - val_loss: 35.8580\n",
            "Epoch 691/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.0477 - val_loss: 36.0668\n",
            "Epoch 692/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.6567 - val_loss: 35.7281\n",
            "Epoch 693/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.9337 - val_loss: 35.6787\n",
            "Epoch 694/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.8417 - val_loss: 35.8821\n",
            "Epoch 695/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 29.8928 - val_loss: 35.4070\n",
            "Epoch 696/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.7989 - val_loss: 35.9526\n",
            "Epoch 697/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 29.9773 - val_loss: 35.6751\n",
            "Epoch 698/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 29.6608 - val_loss: 35.7038\n",
            "Epoch 699/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.8716 - val_loss: 35.7259\n",
            "Epoch 700/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.7436 - val_loss: 35.7310\n",
            "Epoch 701/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 30.1180 - val_loss: 36.0061\n",
            "Epoch 702/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.9216 - val_loss: 36.1575\n",
            "Epoch 703/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.9292 - val_loss: 35.8542\n",
            "Epoch 704/1000\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 29.7713 - val_loss: 35.8900\n",
            "Epoch 705/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.6039 - val_loss: 35.3665\n",
            "Epoch 706/1000\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 29.9519 - val_loss: 35.7692\n",
            "Epoch 707/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 30.0602 - val_loss: 35.5970\n",
            "Epoch 708/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.7239 - val_loss: 35.9478\n",
            "Epoch 709/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.9256 - val_loss: 35.6517\n",
            "Epoch 710/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.0538 - val_loss: 35.5034\n",
            "Epoch 711/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 29.7389 - val_loss: 35.9465\n",
            "Epoch 712/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.8951 - val_loss: 35.8783\n",
            "Epoch 713/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.5497 - val_loss: 35.7801\n",
            "Epoch 714/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.8746 - val_loss: 35.6542\n",
            "Epoch 715/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.9200 - val_loss: 35.7140\n",
            "Epoch 716/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.8156 - val_loss: 35.6100\n",
            "Epoch 717/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.7385 - val_loss: 35.6211\n",
            "Epoch 718/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.0547 - val_loss: 35.7069\n",
            "Epoch 719/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.9767 - val_loss: 36.2434\n",
            "Epoch 720/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.9361 - val_loss: 35.6477\n",
            "Epoch 721/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.8399 - val_loss: 35.3996\n",
            "Epoch 722/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.0422 - val_loss: 35.3069\n",
            "Epoch 723/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 30.1611 - val_loss: 36.0370\n",
            "Epoch 724/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.8269 - val_loss: 35.7070\n",
            "Epoch 725/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 30.0408 - val_loss: 35.5528\n",
            "Epoch 726/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.9627 - val_loss: 35.7418\n",
            "Epoch 727/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.6816 - val_loss: 35.6526\n",
            "Epoch 728/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.5427 - val_loss: 35.4808\n",
            "Epoch 729/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 29.9045 - val_loss: 35.7785\n",
            "Epoch 730/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.8262 - val_loss: 35.8297\n",
            "Epoch 731/1000\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 29.6868 - val_loss: 36.4031\n",
            "Epoch 732/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.7589 - val_loss: 35.7569\n",
            "Epoch 733/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.7979 - val_loss: 35.7173\n",
            "Epoch 734/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.7181 - val_loss: 35.6830\n",
            "Epoch 735/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4367 - val_loss: 35.5960\n",
            "Epoch 736/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.8442 - val_loss: 35.7609\n",
            "Epoch 737/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.1270 - val_loss: 36.0688\n",
            "Epoch 738/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.9054 - val_loss: 35.7190\n",
            "Epoch 739/1000\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 29.7020 - val_loss: 35.7775\n",
            "Epoch 740/1000\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 29.6901 - val_loss: 35.9162\n",
            "Epoch 741/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 29.8019 - val_loss: 35.6695\n",
            "Epoch 742/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.9850 - val_loss: 35.5657\n",
            "Epoch 743/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.9974 - val_loss: 35.7683\n",
            "Epoch 744/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.5737 - val_loss: 36.2222\n",
            "Epoch 745/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.8803 - val_loss: 35.7356\n",
            "Epoch 746/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.7356 - val_loss: 35.5175\n",
            "Epoch 747/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.8585 - val_loss: 35.6336\n",
            "Epoch 748/1000\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 29.6282 - val_loss: 35.8819\n",
            "Epoch 749/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 30.0104 - val_loss: 35.4452\n",
            "Epoch 750/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.7124 - val_loss: 35.7136\n",
            "Epoch 751/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.6433 - val_loss: 35.4332\n",
            "Epoch 752/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.5319 - val_loss: 36.3322\n",
            "Epoch 753/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.5424 - val_loss: 35.8379\n",
            "Epoch 754/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.7287 - val_loss: 35.9907\n",
            "Epoch 755/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.6377 - val_loss: 35.3682\n",
            "Epoch 756/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.8358 - val_loss: 35.6251\n",
            "Epoch 757/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.7344 - val_loss: 35.9113\n",
            "Epoch 758/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.9313 - val_loss: 35.5490\n",
            "Epoch 759/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.0388 - val_loss: 35.6994\n",
            "Epoch 760/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.8294 - val_loss: 35.8264\n",
            "Epoch 761/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.7517 - val_loss: 36.0055\n",
            "Epoch 762/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.6721 - val_loss: 35.8901\n",
            "Epoch 763/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.6100 - val_loss: 35.6161\n",
            "Epoch 764/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.5078 - val_loss: 35.8150\n",
            "Epoch 765/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.5802 - val_loss: 36.1761\n",
            "Epoch 766/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.0492 - val_loss: 35.9149\n",
            "Epoch 767/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.6344 - val_loss: 35.9159\n",
            "Epoch 768/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.8015 - val_loss: 35.8728\n",
            "Epoch 769/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4439 - val_loss: 35.4418\n",
            "Epoch 770/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.8473 - val_loss: 36.0253\n",
            "Epoch 771/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.7592 - val_loss: 35.5707\n",
            "Epoch 772/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.6659 - val_loss: 35.7192\n",
            "Epoch 773/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.6088 - val_loss: 35.6514\n",
            "Epoch 774/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 29.7655 - val_loss: 35.9241\n",
            "Epoch 775/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 29.5222 - val_loss: 35.9054\n",
            "Epoch 776/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.6813 - val_loss: 35.5395\n",
            "Epoch 777/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.7147 - val_loss: 35.6875\n",
            "Epoch 778/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.8203 - val_loss: 35.5227\n",
            "Epoch 779/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.5320 - val_loss: 35.4456\n",
            "Epoch 780/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.6244 - val_loss: 35.8136\n",
            "Epoch 781/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 29.7126 - val_loss: 36.0310\n",
            "Epoch 782/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 30.1366 - val_loss: 35.5017\n",
            "Epoch 783/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.5351 - val_loss: 35.6712\n",
            "Epoch 784/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.5189 - val_loss: 36.1574\n",
            "Epoch 785/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.6710 - val_loss: 36.0093\n",
            "Epoch 786/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.2714 - val_loss: 35.3676\n",
            "Epoch 787/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.8426 - val_loss: 35.7312\n",
            "Epoch 788/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4666 - val_loss: 35.8962\n",
            "Epoch 789/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.5188 - val_loss: 35.8881\n",
            "Epoch 790/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 29.4741 - val_loss: 35.9500\n",
            "Epoch 791/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.7923 - val_loss: 35.8149\n",
            "Epoch 792/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.5347 - val_loss: 35.8227\n",
            "Epoch 793/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.6772 - val_loss: 36.0022\n",
            "Epoch 794/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.3634 - val_loss: 35.4780\n",
            "Epoch 795/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.1276 - val_loss: 35.4902\n",
            "Epoch 796/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.8170 - val_loss: 35.7889\n",
            "Epoch 797/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.7077 - val_loss: 35.8902\n",
            "Epoch 798/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.6155 - val_loss: 35.9485\n",
            "Epoch 799/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.9061 - val_loss: 35.4156\n",
            "Epoch 800/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.7930 - val_loss: 35.6030\n",
            "Epoch 801/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.5588 - val_loss: 36.0261\n",
            "Epoch 802/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.4331 - val_loss: 35.5825\n",
            "Epoch 803/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.3646 - val_loss: 35.6651\n",
            "Epoch 804/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.9262 - val_loss: 35.8341\n",
            "Epoch 805/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.4738 - val_loss: 35.7022\n",
            "Epoch 806/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 29.5733 - val_loss: 35.5886\n",
            "Epoch 807/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.5971 - val_loss: 36.1274\n",
            "Epoch 808/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.5339 - val_loss: 35.5570\n",
            "Epoch 809/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.6508 - val_loss: 36.0791\n",
            "Epoch 810/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.6378 - val_loss: 35.5523\n",
            "Epoch 811/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.2940 - val_loss: 35.7007\n",
            "Epoch 812/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.7773 - val_loss: 35.9529\n",
            "Epoch 813/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.3149 - val_loss: 35.5061\n",
            "Epoch 814/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.7153 - val_loss: 35.6806\n",
            "Epoch 815/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3987 - val_loss: 35.5014\n",
            "Epoch 816/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.9029 - val_loss: 36.0413\n",
            "Epoch 817/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.7374 - val_loss: 35.6748\n",
            "Epoch 818/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.4265 - val_loss: 35.6571\n",
            "Epoch 819/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.5866 - val_loss: 35.7717\n",
            "Epoch 820/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.6827 - val_loss: 35.8696\n",
            "Epoch 821/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.2750 - val_loss: 35.3991\n",
            "Epoch 822/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3248 - val_loss: 35.6195\n",
            "Epoch 823/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.5116 - val_loss: 35.0973\n",
            "Epoch 824/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.8257 - val_loss: 35.5287\n",
            "Epoch 825/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4312 - val_loss: 35.3728\n",
            "Epoch 826/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.4527 - val_loss: 35.2463\n",
            "Epoch 827/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.6985 - val_loss: 35.3648\n",
            "Epoch 828/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.5277 - val_loss: 35.4923\n",
            "Epoch 829/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.3090 - val_loss: 35.8338\n",
            "Epoch 830/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.6441 - val_loss: 35.9934\n",
            "Epoch 831/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.4742 - val_loss: 35.8372\n",
            "Epoch 832/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.6346 - val_loss: 35.1421\n",
            "Epoch 833/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4155 - val_loss: 35.9707\n",
            "Epoch 834/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.5866 - val_loss: 35.6647\n",
            "Epoch 835/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.3148 - val_loss: 35.5869\n",
            "Epoch 836/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.7044 - val_loss: 35.2901\n",
            "Epoch 837/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.6791 - val_loss: 35.5208\n",
            "Epoch 838/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.5830 - val_loss: 35.4779\n",
            "Epoch 839/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4994 - val_loss: 35.7446\n",
            "Epoch 840/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 29.5716 - val_loss: 35.3681\n",
            "Epoch 841/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.4643 - val_loss: 35.8555\n",
            "Epoch 842/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4080 - val_loss: 35.3800\n",
            "Epoch 843/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.3237 - val_loss: 35.5252\n",
            "Epoch 844/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.2293 - val_loss: 35.5044\n",
            "Epoch 845/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4914 - val_loss: 35.5436\n",
            "Epoch 846/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.4571 - val_loss: 35.7775\n",
            "Epoch 847/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 29.1510 - val_loss: 35.9327\n",
            "Epoch 848/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.9055 - val_loss: 35.6812\n",
            "Epoch 849/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.7671 - val_loss: 35.6140\n",
            "Epoch 850/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.7437 - val_loss: 35.3599\n",
            "Epoch 851/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.6259 - val_loss: 35.4467\n",
            "Epoch 852/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.5673 - val_loss: 35.8856\n",
            "Epoch 853/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.3981 - val_loss: 35.8466\n",
            "Epoch 854/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3525 - val_loss: 35.1553\n",
            "Epoch 855/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.4915 - val_loss: 35.5395\n",
            "Epoch 856/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.6333 - val_loss: 35.5594\n",
            "Epoch 857/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.5173 - val_loss: 35.5380\n",
            "Epoch 858/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.6473 - val_loss: 35.2647\n",
            "Epoch 859/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 29.2035 - val_loss: 35.4486\n",
            "Epoch 860/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 29.6648 - val_loss: 35.9538\n",
            "Epoch 861/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3563 - val_loss: 35.5027\n",
            "Epoch 862/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.6889 - val_loss: 35.4761\n",
            "Epoch 863/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.3295 - val_loss: 35.8064\n",
            "Epoch 864/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.4825 - val_loss: 35.6226\n",
            "Epoch 865/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.2446 - val_loss: 35.5696\n",
            "Epoch 866/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.5515 - val_loss: 35.6834\n",
            "Epoch 867/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.5650 - val_loss: 35.3987\n",
            "Epoch 868/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.4008 - val_loss: 35.3916\n",
            "Epoch 869/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.7445 - val_loss: 35.6654\n",
            "Epoch 870/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.4471 - val_loss: 35.3691\n",
            "Epoch 871/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.3524 - val_loss: 35.3901\n",
            "Epoch 872/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.7824 - val_loss: 36.0058\n",
            "Epoch 873/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.5586 - val_loss: 35.7272\n",
            "Epoch 874/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3874 - val_loss: 35.9683\n",
            "Epoch 875/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.3647 - val_loss: 35.7637\n",
            "Epoch 876/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.3987 - val_loss: 35.7275\n",
            "Epoch 877/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.1722 - val_loss: 35.5388\n",
            "Epoch 878/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.2541 - val_loss: 35.5972\n",
            "Epoch 879/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.5065 - val_loss: 35.5497\n",
            "Epoch 880/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.4193 - val_loss: 35.4705\n",
            "Epoch 881/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4271 - val_loss: 35.4327\n",
            "Epoch 882/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.5081 - val_loss: 35.6397\n",
            "Epoch 883/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.6874 - val_loss: 35.9110\n",
            "Epoch 884/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.7981 - val_loss: 35.6850\n",
            "Epoch 885/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.3745 - val_loss: 36.1378\n",
            "Epoch 886/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.4879 - val_loss: 35.7313\n",
            "Epoch 887/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.5727 - val_loss: 35.7833\n",
            "Epoch 888/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.2209 - val_loss: 36.0968\n",
            "Epoch 889/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.3871 - val_loss: 35.9161\n",
            "Epoch 890/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.5013 - val_loss: 35.9899\n",
            "Epoch 891/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3846 - val_loss: 35.8252\n",
            "Epoch 892/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.2977 - val_loss: 35.3811\n",
            "Epoch 893/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.5452 - val_loss: 35.3286\n",
            "Epoch 894/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 29.3794 - val_loss: 35.5781\n",
            "Epoch 895/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.1514 - val_loss: 35.5616\n",
            "Epoch 896/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.0987 - val_loss: 35.6886\n",
            "Epoch 897/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.4824 - val_loss: 35.6927\n",
            "Epoch 898/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.5853 - val_loss: 36.1161\n",
            "Epoch 899/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 29.3104 - val_loss: 35.8245\n",
            "Epoch 900/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.4589 - val_loss: 35.6970\n",
            "Epoch 901/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3365 - val_loss: 35.4491\n",
            "Epoch 902/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.4681 - val_loss: 35.5005\n",
            "Epoch 903/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.3599 - val_loss: 35.3698\n",
            "Epoch 904/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.4056 - val_loss: 35.3970\n",
            "Epoch 905/1000\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 29.5625 - val_loss: 35.9093\n",
            "Epoch 906/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.3454 - val_loss: 35.7156\n",
            "Epoch 907/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.4918 - val_loss: 35.8667\n",
            "Epoch 908/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.2256 - val_loss: 36.2325\n",
            "Epoch 909/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 29.4021 - val_loss: 36.1547\n",
            "Epoch 910/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.6045 - val_loss: 35.4652\n",
            "Epoch 911/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 29.2114 - val_loss: 35.2609\n",
            "Epoch 912/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.5794 - val_loss: 35.7745\n",
            "Epoch 913/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.1296 - val_loss: 35.9126\n",
            "Epoch 914/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.3133 - val_loss: 35.6048\n",
            "Epoch 915/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.3915 - val_loss: 35.6378\n",
            "Epoch 916/1000\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 29.5315 - val_loss: 35.4209\n",
            "Epoch 917/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.3296 - val_loss: 35.5402\n",
            "Epoch 918/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.1665 - val_loss: 35.8430\n",
            "Epoch 919/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.4544 - val_loss: 35.2950\n",
            "Epoch 920/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 28.9955 - val_loss: 35.2071\n",
            "Epoch 921/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.5109 - val_loss: 35.5679\n",
            "Epoch 922/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3375 - val_loss: 35.6756\n",
            "Epoch 923/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.4119 - val_loss: 35.4895\n",
            "Epoch 924/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4934 - val_loss: 35.6373\n",
            "Epoch 925/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.4059 - val_loss: 35.3179\n",
            "Epoch 926/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.4399 - val_loss: 35.6231\n",
            "Epoch 927/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.4844 - val_loss: 35.4170\n",
            "Epoch 928/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.2639 - val_loss: 35.5270\n",
            "Epoch 929/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.1273 - val_loss: 35.6370\n",
            "Epoch 930/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.1747 - val_loss: 35.3646\n",
            "Epoch 931/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.2631 - val_loss: 35.4320\n",
            "Epoch 932/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.4708 - val_loss: 35.4642\n",
            "Epoch 933/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.4260 - val_loss: 35.4501\n",
            "Epoch 934/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.3252 - val_loss: 35.6106\n",
            "Epoch 935/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.5560 - val_loss: 35.7477\n",
            "Epoch 936/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.1995 - val_loss: 35.4985\n",
            "Epoch 937/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.2423 - val_loss: 35.3453\n",
            "Epoch 938/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.5769 - val_loss: 35.6130\n",
            "Epoch 939/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 29.3705 - val_loss: 35.9138\n",
            "Epoch 940/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.1818 - val_loss: 35.6553\n",
            "Epoch 941/1000\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 29.2001 - val_loss: 35.6598\n",
            "Epoch 942/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.1026 - val_loss: 35.5851\n",
            "Epoch 943/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3204 - val_loss: 35.7307\n",
            "Epoch 944/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.3484 - val_loss: 35.5817\n",
            "Epoch 945/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.2384 - val_loss: 35.4562\n",
            "Epoch 946/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.0335 - val_loss: 35.1280\n",
            "Epoch 947/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.2351 - val_loss: 35.4525\n",
            "Epoch 948/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.1595 - val_loss: 35.4333\n",
            "Epoch 949/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.5344 - val_loss: 35.6108\n",
            "Epoch 950/1000\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 28.9622 - val_loss: 35.7127\n",
            "Epoch 951/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.2079 - val_loss: 36.0156\n",
            "Epoch 952/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.3503 - val_loss: 35.6179\n",
            "Epoch 953/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.2303 - val_loss: 35.2627\n",
            "Epoch 954/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.0085 - val_loss: 35.2723\n",
            "Epoch 955/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.2317 - val_loss: 36.0344\n",
            "Epoch 956/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.1391 - val_loss: 35.7007\n",
            "Epoch 957/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.3360 - val_loss: 35.7446\n",
            "Epoch 958/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 28.8520 - val_loss: 35.6773\n",
            "Epoch 959/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.4986 - val_loss: 35.5969\n",
            "Epoch 960/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.1508 - val_loss: 35.7632\n",
            "Epoch 961/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.3737 - val_loss: 35.3019\n",
            "Epoch 962/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.1980 - val_loss: 35.2786\n",
            "Epoch 963/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.1461 - val_loss: 35.5789\n",
            "Epoch 964/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.1634 - val_loss: 36.3744\n",
            "Epoch 965/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.5253 - val_loss: 35.7470\n",
            "Epoch 966/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.1920 - val_loss: 36.0660\n",
            "Epoch 967/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.1666 - val_loss: 35.9982\n",
            "Epoch 968/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.1132 - val_loss: 35.4997\n",
            "Epoch 969/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.6026 - val_loss: 35.3052\n",
            "Epoch 970/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.2843 - val_loss: 35.6786\n",
            "Epoch 971/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.2220 - val_loss: 35.5549\n",
            "Epoch 972/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.4362 - val_loss: 35.7278\n",
            "Epoch 973/1000\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 29.1594 - val_loss: 35.8161\n",
            "Epoch 974/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.0212 - val_loss: 35.5303\n",
            "Epoch 975/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3204 - val_loss: 35.8339\n",
            "Epoch 976/1000\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 29.0929 - val_loss: 35.7856\n",
            "Epoch 977/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.4148 - val_loss: 36.1280\n",
            "Epoch 978/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.0144 - val_loss: 36.0822\n",
            "Epoch 979/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.2971 - val_loss: 35.2904\n",
            "Epoch 980/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.2831 - val_loss: 35.3135\n",
            "Epoch 981/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.1881 - val_loss: 35.7776\n",
            "Epoch 982/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 29.0519 - val_loss: 35.4105\n",
            "Epoch 983/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.4651 - val_loss: 35.8549\n",
            "Epoch 984/1000\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 29.1117 - val_loss: 35.4877\n",
            "Epoch 985/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 28.9970 - val_loss: 35.8869\n",
            "Epoch 986/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 29.3740 - val_loss: 35.5144\n",
            "Epoch 987/1000\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 29.4124 - val_loss: 35.7167\n",
            "Epoch 988/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 29.4939 - val_loss: 35.9621\n",
            "Epoch 989/1000\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 28.9725 - val_loss: 35.3256\n",
            "Epoch 990/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.1771 - val_loss: 35.5006\n",
            "Epoch 991/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 29.0742 - val_loss: 35.3922\n",
            "Epoch 992/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.4750 - val_loss: 35.7289\n",
            "Epoch 993/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3014 - val_loss: 35.5638\n",
            "Epoch 994/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3253 - val_loss: 35.4844\n",
            "Epoch 995/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 28.9874 - val_loss: 35.6048\n",
            "Epoch 996/1000\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 29.0908 - val_loss: 35.2882\n",
            "Epoch 997/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 28.9384 - val_loss: 35.3796\n",
            "Epoch 998/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.0772 - val_loss: 35.7131\n",
            "Epoch 999/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.2041 - val_loss: 35.1194\n",
            "Epoch 1000/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 29.3998 - val_loss: 35.6210\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa71c134790>"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_vae = vae.predict(X)\n",
        "#plot_predictions(y_true, y_pred)\n",
        "\n",
        "new_x = pd.DataFrame(y_pred_vae)\n",
        "new_x.to_csv('3_512_vae_x.csv')\n",
        "\n",
        "n = pd.read_csv('3_512_vae_x.csv')\n",
        "print(n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4BUAtQ05SCZ",
        "outputId": "392026f4-f6ac-4e68-8cbf-9a2f7f671ab9"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Unnamed: 0         0         1         2         3         4         5  \\\n",
            "0             0  0.000000  0.580312  0.894221  0.359923  0.000000  1.235842   \n",
            "1             1  0.207925  1.075836  0.000000  0.811016  0.000000  1.065301   \n",
            "2             2  0.137868  0.926265  0.000000  1.062172  0.000000  1.028330   \n",
            "3             3  0.000000  0.537253  0.445388  0.149085  0.229414  0.308316   \n",
            "4             4  0.000000  0.000000  0.702448  0.054570  0.000000  1.148620   \n",
            "..          ...       ...       ...       ...       ...       ...       ...   \n",
            "926         926  0.707356  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "927         927  0.902292  0.025674  0.000000  0.000000  0.000000  0.000000   \n",
            "928         928  0.743435  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "929         929  0.000000  0.066935  0.000000  0.000000  0.000000  0.169841   \n",
            "930         930  0.666401  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "            6         7         8  ...       502       503  504       505  \\\n",
            "0    0.000000  0.031385  0.000000  ...  0.916444  0.000000  0.0  0.000000   \n",
            "1    0.000000  0.000000  0.000000  ...  0.224503  0.080010  0.0  0.000000   \n",
            "2    0.177719  0.000000  0.000000  ...  0.142151  0.233902  0.0  0.000000   \n",
            "3    0.000000  0.000000  0.364842  ...  0.806129  0.000000  0.0  0.000000   \n",
            "4    0.000000  0.000000  0.899808  ...  0.817008  0.000000  0.0  0.000000   \n",
            "..        ...       ...       ...  ...       ...       ...  ...       ...   \n",
            "926  0.000000  0.136980  0.000000  ...  0.240610  0.000000  0.0  0.000000   \n",
            "927  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.0  0.000000   \n",
            "928  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.0  0.000000   \n",
            "929  0.000000  0.044828  0.000000  ...  0.196269  0.286335  0.0  0.085165   \n",
            "930  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.0  0.000000   \n",
            "\n",
            "     506       507  508  509       510       511  \n",
            "0    0.0  0.241567  0.0  0.0  0.000000  0.000000  \n",
            "1    0.0  1.114988  0.0  0.0  0.000000  0.000000  \n",
            "2    0.0  1.002503  0.0  0.0  0.000000  0.000000  \n",
            "3    0.0  0.722373  0.0  0.0  0.000000  0.000000  \n",
            "4    0.0  1.015675  0.0  0.0  0.000000  0.000000  \n",
            "..   ...       ...  ...  ...       ...       ...  \n",
            "926  0.0  0.416846  0.0  0.0  0.107942  0.234551  \n",
            "927  0.0  1.073600  0.0  0.0  0.000000  0.761539  \n",
            "928  0.0  0.920215  0.0  0.0  0.000000  0.602476  \n",
            "929  0.0  0.074611  0.0  0.0  0.000000  0.000000  \n",
            "930  0.0  0.152286  0.0  0.0  0.000000  0.433229  \n",
            "\n",
            "[931 rows x 513 columns]\n"
          ]
        }
      ]
    }
  ]
}