{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/SGLD_ynorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneWjImThPub",
        "outputId": "56e4213a-8b8c-4cc2-b0a6-ce116c55a019"
      },
      "id": "YneWjImThPub",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94c44267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c44267",
        "outputId": "16ab2277-aa25-408a-8baa-34ebe0c51ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.7.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565125 sha256=7f2f05ee8b6db5bf718e6ad7a55efee244e862d12fc80dcac8d8772dc01c6cb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=5a96437784e4ef84e4edbb947c3f74958c11c1ce8cc04e65481a2a2641ca960d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XkP-XKI5mOhv"
      },
      "id": "XkP-XKI5mOhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f11def2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f11def2",
        "outputId": "5f5c4004-0207-4344-9c53-6cbf33658c6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "613d2696",
      "metadata": {
        "id": "613d2696"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b95dd8e4",
      "metadata": {
        "id": "b95dd8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6a5ba769",
      "metadata": {
        "id": "6a5ba769"
      },
      "outputs": [],
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dc555aa8",
      "metadata": {
        "id": "dc555aa8"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "24ee5406",
      "metadata": {
        "id": "24ee5406"
      },
      "outputs": [],
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "189bc6ae",
      "metadata": {
        "id": "189bc6ae"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, num_units)\n",
        "        self.layer3 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        #x_train = (x_train - x_means)/x_stds\n",
        "        y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        #x_test = (x_test - x_means)/x_stds\n",
        "        y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=Langevin_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=batch_size, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 0: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    return nets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3b9afbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9afbcd",
        "outputId": "d5a08590-613b-400f-9f23-251ff87a14ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning:This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:    0, Train loss:  0.500 Test loss:  0.743 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.410 Test loss:  0.644 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.025 Test loss:  0.510 Ensemble loss:  0.485 RMSE: 1.030 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.185 Test loss:  0.945 Ensemble loss:  0.441 RMSE: 0.979 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.197 Test loss:  0.740 Ensemble loss:  0.377 RMSE: 0.938 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.336 Test loss:  2.065 Ensemble loss:  0.318 RMSE: 0.877 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.278 Test loss:  1.973 Ensemble loss:  0.311 RMSE: 0.872 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.296 Test loss:  1.626 Ensemble loss:  0.325 RMSE: 0.881 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.377 Test loss:  1.607 Ensemble loss:  0.284 RMSE: 0.871 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.247 Test loss:  1.480 Ensemble loss:  0.272 RMSE: 0.868 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.390 Test loss:  2.515 Ensemble loss:  0.271 RMSE: 0.867 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.233 Test loss:  5.311 Ensemble loss:  0.291 RMSE: 0.878 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.241 Test loss:  7.315 Ensemble loss:  0.301 RMSE: 0.881 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.280 Test loss:  5.237 Ensemble loss:  0.289 RMSE: 0.873 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.410 Test loss:  7.942 Ensemble loss:  0.284 RMSE: 0.871 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.101 Test loss:  4.603 Ensemble loss:  0.280 RMSE: 0.875 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.489 Test loss:  8.386 Ensemble loss:  0.274 RMSE: 0.871 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.423 Test loss:  5.803 Ensemble loss:  0.269 RMSE: 0.867 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.435 Test loss:  4.412 Ensemble loss:  0.266 RMSE: 0.867 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.447 Test loss: 13.397 Ensemble loss:  0.267 RMSE: 0.866 Num. networks: 30\n",
            "FOLD 1:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.783 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.388 Test loss:  0.690 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.049 Test loss:  0.450 Ensemble loss:  0.466 RMSE: 1.019 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.179 Test loss:  0.959 Ensemble loss:  0.481 RMSE: 1.044 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.190 Test loss:  1.045 Ensemble loss:  0.460 RMSE: 1.029 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.342 Test loss:  1.173 Ensemble loss:  0.439 RMSE: 1.023 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.257 Test loss:  1.491 Ensemble loss:  0.461 RMSE: 1.039 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.365 Test loss:  2.020 Ensemble loss:  0.467 RMSE: 1.041 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.298 Test loss:  2.099 Ensemble loss:  0.460 RMSE: 1.030 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.389 Test loss:  3.393 Ensemble loss:  0.451 RMSE: 0.997 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.444 Test loss:  4.004 Ensemble loss:  0.442 RMSE: 0.980 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.488 Test loss:  5.894 Ensemble loss:  0.420 RMSE: 0.956 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.408 Test loss:  6.716 Ensemble loss:  0.407 RMSE: 0.937 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.390 Test loss:  2.084 Ensemble loss:  0.396 RMSE: 0.922 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.311 Test loss:  3.877 Ensemble loss:  0.389 RMSE: 0.909 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.443 Test loss:  7.359 Ensemble loss:  0.385 RMSE: 0.899 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.373 Test loss: 29.550 Ensemble loss:  0.385 RMSE: 0.892 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.398 Test loss: 101.899 Ensemble loss:  0.384 RMSE: 0.890 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.358 Test loss: 52.558 Ensemble loss:  0.373 RMSE: 0.881 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.411 Test loss: 10.412 Ensemble loss:  0.368 RMSE: 0.880 Num. networks: 30\n",
            "FOLD 2:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.338 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.389 Test loss:  0.272 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.066 Test loss:  0.683 Ensemble loss:  0.356 RMSE: 0.880 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.208 Test loss:  0.524 Ensemble loss:  0.257 RMSE: 0.856 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.263 Test loss:  1.440 Ensemble loss:  0.224 RMSE: 0.855 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.278 Test loss:  1.677 Ensemble loss:  0.233 RMSE: 0.859 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.362 Test loss:  3.398 Ensemble loss:  0.267 RMSE: 0.876 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.254 Test loss:  3.543 Ensemble loss:  0.287 RMSE: 0.880 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.254 Test loss:  2.332 Ensemble loss:  0.296 RMSE: 0.888 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.358 Test loss:  6.670 Ensemble loss:  0.284 RMSE: 0.874 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.406 Test loss:  6.133 Ensemble loss:  0.287 RMSE: 0.867 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.282 Test loss:  4.064 Ensemble loss:  0.268 RMSE: 0.859 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.303 Test loss: 15.667 Ensemble loss:  0.282 RMSE: 0.862 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.431 Test loss:  5.773 Ensemble loss:  0.286 RMSE: 0.860 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.310 Test loss:  2.315 Ensemble loss:  0.315 RMSE: 0.861 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.459 Test loss:  3.416 Ensemble loss:  0.309 RMSE: 0.857 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.359 Test loss: 10.608 Ensemble loss:  0.310 RMSE: 0.856 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.443 Test loss:  9.720 Ensemble loss:  0.296 RMSE: 0.850 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.510 Test loss:  8.016 Ensemble loss:  0.294 RMSE: 0.847 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.420 Test loss:  1.821 Ensemble loss:  0.295 RMSE: 0.848 Num. networks: 30\n",
            "FOLD 3:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.481 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.417 Test loss:  0.358 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.023 Test loss:  0.296 Ensemble loss:  0.273 RMSE: 0.849 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.190 Test loss:  0.625 Ensemble loss:  0.206 RMSE: 0.840 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.168 Test loss:  0.810 Ensemble loss:  0.195 RMSE: 0.837 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.370 Test loss:  2.684 Ensemble loss:  0.211 RMSE: 0.850 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.351 Test loss:  3.136 Ensemble loss:  0.204 RMSE: 0.833 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.316 Test loss:  3.725 Ensemble loss:  0.208 RMSE: 0.836 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.390 Test loss:  3.162 Ensemble loss:  0.202 RMSE: 0.833 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.318 Test loss:  1.867 Ensemble loss:  0.161 RMSE: 0.824 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.323 Test loss:  4.167 Ensemble loss:  0.152 RMSE: 0.820 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.274 Test loss:  4.379 Ensemble loss:  0.145 RMSE: 0.819 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.372 Test loss:  8.979 Ensemble loss:  0.148 RMSE: 0.822 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.466 Test loss:  9.397 Ensemble loss:  0.152 RMSE: 0.821 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.487 Test loss:  9.906 Ensemble loss:  0.153 RMSE: 0.819 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.444 Test loss:  4.232 Ensemble loss:  0.141 RMSE: 0.811 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.521 Test loss:  3.967 Ensemble loss:  0.141 RMSE: 0.807 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.468 Test loss:  3.440 Ensemble loss:  0.143 RMSE: 0.802 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.551 Test loss:  8.415 Ensemble loss:  0.154 RMSE: 0.804 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.524 Test loss: 15.692 Ensemble loss:  0.159 RMSE: 0.805 Num. networks: 30\n",
            "FOLD 4:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.543 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.419 Test loss:  0.514 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.021 Test loss:  0.771 Ensemble loss:  0.434 RMSE: 1.025 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.241 Test loss:  0.629 Ensemble loss:  0.331 RMSE: 0.935 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.179 Test loss:  2.055 Ensemble loss:  0.341 RMSE: 0.919 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.278 Test loss:  2.033 Ensemble loss:  0.331 RMSE: 0.912 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.073 Test loss:  1.357 Ensemble loss:  0.317 RMSE: 0.905 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.048 Test loss:  1.453 Ensemble loss:  0.325 RMSE: 0.913 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.400 Test loss:  4.844 Ensemble loss:  0.341 RMSE: 0.927 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.326 Test loss:  5.363 Ensemble loss:  0.361 RMSE: 0.946 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.265 Test loss:  3.281 Ensemble loss:  0.369 RMSE: 0.952 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.326 Test loss: 14.071 Ensemble loss:  0.397 RMSE: 0.961 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.313 Test loss: 24.907 Ensemble loss:  0.426 RMSE: 0.988 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.331 Test loss: 12.587 Ensemble loss:  0.431 RMSE: 0.993 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.400 Test loss: 11.062 Ensemble loss:  0.449 RMSE: 1.003 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.147 Test loss:  3.978 Ensemble loss:  0.461 RMSE: 1.014 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.236 Test loss:  2.661 Ensemble loss:  0.470 RMSE: 1.016 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.241 Test loss:  2.168 Ensemble loss:  0.479 RMSE: 1.028 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.221 Test loss:  3.164 Ensemble loss:  0.486 RMSE: 1.036 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.397 Test loss:  6.455 Ensemble loss:  0.484 RMSE: 1.038 Num. networks: 30\n",
            "FOLD 5:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.278 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.450 Test loss:  0.276 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.039 Test loss:  0.552 Ensemble loss:  0.246 RMSE: 0.868 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.208 Test loss:  0.659 Ensemble loss:  0.227 RMSE: 0.846 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.264 Test loss:  2.111 Ensemble loss:  0.212 RMSE: 0.834 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.312 Test loss:  4.622 Ensemble loss:  0.195 RMSE: 0.830 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.331 Test loss:  2.911 Ensemble loss:  0.179 RMSE: 0.821 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.432 Test loss:  3.045 Ensemble loss:  0.164 RMSE: 0.809 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.095 Test loss:  1.336 Ensemble loss:  0.169 RMSE: 0.814 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.217 Test loss:  1.946 Ensemble loss:  0.165 RMSE: 0.818 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.357 Test loss:  3.736 Ensemble loss:  0.166 RMSE: 0.818 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.308 Test loss:  2.454 Ensemble loss:  0.147 RMSE: 0.809 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.390 Test loss:  0.810 Ensemble loss:  0.133 RMSE: 0.797 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.258 Test loss:  1.417 Ensemble loss:  0.125 RMSE: 0.795 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.193 Test loss:  2.042 Ensemble loss:  0.120 RMSE: 0.794 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.353 Test loss:  4.510 Ensemble loss:  0.123 RMSE: 0.794 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.324 Test loss:  2.670 Ensemble loss:  0.129 RMSE: 0.795 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.449 Test loss:  5.207 Ensemble loss:  0.143 RMSE: 0.803 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.423 Test loss:  4.958 Ensemble loss:  0.152 RMSE: 0.805 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.440 Test loss:  5.076 Ensemble loss:  0.159 RMSE: 0.811 Num. networks: 30\n",
            "FOLD 6:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.459 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.411 Test loss:  0.437 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.053 Test loss:  0.333 Ensemble loss:  0.339 RMSE: 0.904 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.133 Test loss:  0.251 Ensemble loss:  0.214 RMSE: 0.816 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.102 Test loss:  0.663 Ensemble loss:  0.202 RMSE: 0.812 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.255 Test loss:  0.442 Ensemble loss:  0.178 RMSE: 0.796 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.162 Test loss:  0.176 Ensemble loss:  0.142 RMSE: 0.777 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.279 Test loss:  1.314 Ensemble loss:  0.134 RMSE: 0.770 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.252 Test loss:  1.230 Ensemble loss:  0.110 RMSE: 0.751 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.124 Test loss:  2.252 Ensemble loss:  0.093 RMSE: 0.727 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.323 Test loss:  1.291 Ensemble loss:  0.087 RMSE: 0.720 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.379 Test loss:  1.576 Ensemble loss:  0.093 RMSE: 0.717 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.360 Test loss:  0.556 Ensemble loss:  0.088 RMSE: 0.711 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.395 Test loss:  0.625 Ensemble loss:  0.087 RMSE: 0.712 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.444 Test loss:  1.693 Ensemble loss:  0.074 RMSE: 0.708 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.350 Test loss:  0.769 Ensemble loss:  0.061 RMSE: 0.706 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.360 Test loss:  3.337 Ensemble loss:  0.048 RMSE: 0.701 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.407 Test loss:  1.923 Ensemble loss:  0.034 RMSE: 0.696 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.387 Test loss:  3.095 Ensemble loss:  0.032 RMSE: 0.698 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.371 Test loss:  2.498 Ensemble loss:  0.027 RMSE: 0.698 Num. networks: 30\n",
            "FOLD 7:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.451 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.375 Test loss:  0.381 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.053 Test loss:  0.282 Ensemble loss:  0.233 RMSE: 0.801 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.162 Test loss:  0.653 Ensemble loss:  0.168 RMSE: 0.732 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.189 Test loss:  0.399 Ensemble loss:  0.170 RMSE: 0.706 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.230 Test loss:  0.506 Ensemble loss:  0.094 RMSE: 0.667 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.320 Test loss:  2.692 Ensemble loss:  0.071 RMSE: 0.686 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.195 Test loss:  2.469 Ensemble loss:  0.080 RMSE: 0.702 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.344 Test loss:  2.679 Ensemble loss:  0.062 RMSE: 0.710 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.329 Test loss: 22.304 Ensemble loss:  0.089 RMSE: 0.740 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.380 Test loss:  5.744 Ensemble loss:  0.105 RMSE: 0.761 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.411 Test loss:  5.543 Ensemble loss:  0.103 RMSE: 0.773 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.388 Test loss:  4.309 Ensemble loss:  0.097 RMSE: 0.775 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.349 Test loss:  2.075 Ensemble loss:  0.099 RMSE: 0.780 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.266 Test loss:  4.302 Ensemble loss:  0.112 RMSE: 0.794 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.442 Test loss:  3.213 Ensemble loss:  0.117 RMSE: 0.809 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.429 Test loss: 22.159 Ensemble loss:  0.114 RMSE: 0.816 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.373 Test loss: 94.164 Ensemble loss:  0.121 RMSE: 0.831 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.434 Test loss:  8.869 Ensemble loss:  0.124 RMSE: 0.834 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.444 Test loss:  4.181 Ensemble loss:  0.128 RMSE: 0.836 Num. networks: 30\n",
            "FOLD 8:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.618 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.409 Test loss:  0.495 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.049 Test loss:  0.202 Ensemble loss:  0.234 RMSE: 0.894 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.162 Test loss:  0.800 Ensemble loss:  0.162 RMSE: 0.854 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.263 Test loss:  1.179 Ensemble loss:  0.114 RMSE: 0.824 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.257 Test loss:  1.380 Ensemble loss:  0.079 RMSE: 0.797 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.170 Test loss:  1.217 Ensemble loss:  0.087 RMSE: 0.788 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.199 Test loss:  1.747 Ensemble loss:  0.085 RMSE: 0.785 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.253 Test loss:  2.985 Ensemble loss:  0.109 RMSE: 0.777 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.261 Test loss:  3.075 Ensemble loss:  0.103 RMSE: 0.774 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.350 Test loss:  3.459 Ensemble loss:  0.102 RMSE: 0.774 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.168 Test loss:  3.406 Ensemble loss:  0.117 RMSE: 0.779 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.334 Test loss:  3.380 Ensemble loss:  0.114 RMSE: 0.780 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.271 Test loss:  4.198 Ensemble loss:  0.116 RMSE: 0.780 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.439 Test loss:  5.415 Ensemble loss:  0.113 RMSE: 0.780 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.290 Test loss:  5.270 Ensemble loss:  0.114 RMSE: 0.781 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.392 Test loss:  4.490 Ensemble loss:  0.114 RMSE: 0.782 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.256 Test loss:  3.856 Ensemble loss:  0.118 RMSE: 0.783 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.114 Test loss:  2.303 Ensemble loss:  0.120 RMSE: 0.784 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.247 Test loss:  2.588 Ensemble loss:  0.119 RMSE: 0.784 Num. networks: 30\n",
            "FOLD 9:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.601 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.412 Test loss:  0.575 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.003 Test loss:  0.414 Ensemble loss:  0.350 RMSE: 1.005 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.109 Test loss:  0.280 Ensemble loss:  0.193 RMSE: 0.887 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.232 Test loss:  0.785 Ensemble loss:  0.119 RMSE: 0.848 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.172 Test loss:  0.695 Ensemble loss:  0.089 RMSE: 0.834 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.164 Test loss:  1.625 Ensemble loss:  0.068 RMSE: 0.802 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.314 Test loss:  2.464 Ensemble loss:  0.064 RMSE: 0.789 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.237 Test loss:  1.534 Ensemble loss:  0.054 RMSE: 0.771 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.391 Test loss:  3.171 Ensemble loss:  0.029 RMSE: 0.742 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.352 Test loss:  4.452 Ensemble loss:  0.015 RMSE: 0.727 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.378 Test loss:  7.339 Ensemble loss:  0.016 RMSE: 0.706 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.374 Test loss:  9.873 Ensemble loss:  0.025 RMSE: 0.698 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.434 Test loss:  8.143 Ensemble loss:  0.032 RMSE: 0.697 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.358 Test loss:  5.209 Ensemble loss:  0.030 RMSE: 0.689 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.453 Test loss:  5.223 Ensemble loss:  0.033 RMSE: 0.690 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.429 Test loss:  3.063 Ensemble loss:  0.033 RMSE: 0.693 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.180 Test loss:  1.698 Ensemble loss:  0.031 RMSE: 0.692 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.433 Test loss:  6.853 Ensemble loss:  0.023 RMSE: 0.688 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.202 Test loss:  8.000 Ensemble loss:  0.023 RMSE: 0.684 Num. networks: 30\n",
            "Train log. lik. =   0.239 +/-  0.025\n",
            "Test  log. lik. =  -0.298 +/-  0.138\n",
            "Train RMSE      =   0.463 +/-  0.017\n",
            "Test  RMSE      =   0.825 +/-  0.094\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "data = pd.read_csv('/content/drive/MyDrive/bayesian-deep-learning-master/data/Datasets.csv').values\n",
        "ensemble = train_mc_dropout(data=data, n_splits=10, burn_in=350, mix_time=300, num_nets=30,\n",
        "                            num_units=110, learn_rate=1e-2/len(data), weight_decay=0.9, log_every=500)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}