{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/SGLD_ynorm_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneWjImThPub",
        "outputId": "56e4213a-8b8c-4cc2-b0a6-ce116c55a019"
      },
      "id": "YneWjImThPub",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94c44267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c44267",
        "outputId": "16ab2277-aa25-408a-8baa-34ebe0c51ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.7.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565125 sha256=7f2f05ee8b6db5bf718e6ad7a55efee244e862d12fc80dcac8d8772dc01c6cb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=5a96437784e4ef84e4edbb947c3f74958c11c1ce8cc04e65481a2a2641ca960d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XkP-XKI5mOhv"
      },
      "id": "XkP-XKI5mOhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f11def2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f11def2",
        "outputId": "5f5c4004-0207-4344-9c53-6cbf33658c6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "613d2696",
      "metadata": {
        "id": "613d2696"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b95dd8e4",
      "metadata": {
        "id": "b95dd8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6a5ba769",
      "metadata": {
        "id": "6a5ba769"
      },
      "outputs": [],
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dc555aa8",
      "metadata": {
        "id": "dc555aa8"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "24ee5406",
      "metadata": {
        "id": "24ee5406"
      },
      "outputs": [],
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "189bc6ae",
      "metadata": {
        "id": "189bc6ae"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, num_units)\n",
        "        self.layer3 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        #x_train = (x_train - x_means)/x_stds\n",
        "        y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        #x_test = (x_test - x_means)/x_stds\n",
        "        y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=Langevin_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=batch_size, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 0: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    return nets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3b9afbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9afbcd",
        "outputId": "e89fe9f9-4223-4a84-d9c7-8f4df4f3b9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.734 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.414 Test loss:  0.847 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.065 Test loss:  0.786 Ensemble loss:  0.648 RMSE: 1.107 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.212 Test loss:  1.276 Ensemble loss:  0.574 RMSE: 1.026 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.198 Test loss:  1.179 Ensemble loss:  0.560 RMSE: 1.023 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.232 Test loss:  1.421 Ensemble loss:  0.476 RMSE: 0.979 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.224 Test loss:  1.540 Ensemble loss:  0.480 RMSE: 0.981 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.047 Test loss:  1.416 Ensemble loss:  0.463 RMSE: 0.963 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.174 Test loss:  1.240 Ensemble loss:  0.431 RMSE: 0.938 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.207 Test loss:  2.304 Ensemble loss:  0.411 RMSE: 0.924 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.165 Test loss:  1.873 Ensemble loss:  0.414 RMSE: 0.912 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.335 Test loss:  2.960 Ensemble loss:  0.411 RMSE: 0.927 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.255 Test loss:  1.271 Ensemble loss:  0.405 RMSE: 0.930 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.085 Test loss:  2.142 Ensemble loss:  0.407 RMSE: 0.932 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.422 Test loss:  2.862 Ensemble loss:  0.398 RMSE: 0.933 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.085 Test loss:  1.484 Ensemble loss:  0.392 RMSE: 0.935 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.369 Test loss:  6.715 Ensemble loss:  0.392 RMSE: 0.939 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.406 Test loss:  3.280 Ensemble loss:  0.403 RMSE: 0.952 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.415 Test loss:  1.579 Ensemble loss:  0.403 RMSE: 0.959 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.353 Test loss:  4.990 Ensemble loss:  0.406 RMSE: 0.967 Num. networks: 30\n",
            "FOLD 1:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.785 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.408 Test loss:  0.697 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.055 Test loss:  0.858 Ensemble loss:  0.563 RMSE: 1.111 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.130 Test loss:  1.698 Ensemble loss:  0.702 RMSE: 1.172 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.064 Test loss:  1.617 Ensemble loss:  0.712 RMSE: 1.177 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.167 Test loss:  1.182 Ensemble loss:  0.696 RMSE: 1.170 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.241 Test loss:  1.596 Ensemble loss:  0.636 RMSE: 1.144 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.046 Test loss:  1.226 Ensemble loss:  0.629 RMSE: 1.143 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.325 Test loss:  3.062 Ensemble loss:  0.574 RMSE: 1.098 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.249 Test loss:  2.162 Ensemble loss:  0.543 RMSE: 1.064 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.303 Test loss:  3.802 Ensemble loss:  0.521 RMSE: 1.043 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.280 Test loss:  1.807 Ensemble loss:  0.486 RMSE: 1.018 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.353 Test loss:  2.093 Ensemble loss:  0.473 RMSE: 1.018 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.316 Test loss:  4.048 Ensemble loss:  0.468 RMSE: 1.010 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.365 Test loss:  1.767 Ensemble loss:  0.443 RMSE: 0.973 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.421 Test loss: 52.835 Ensemble loss:  0.429 RMSE: 0.949 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.478 Test loss:  5.607 Ensemble loss:  0.424 RMSE: 0.937 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.468 Test loss:  4.289 Ensemble loss:  0.406 RMSE: 0.914 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.529 Test loss:  4.948 Ensemble loss:  0.395 RMSE: 0.899 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.474 Test loss:  2.844 Ensemble loss:  0.389 RMSE: 0.891 Num. networks: 30\n",
            "FOLD 2:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.342 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.372 Test loss:  0.307 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.087 Test loss:  0.533 Ensemble loss:  0.280 RMSE: 0.861 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.210 Test loss:  1.034 Ensemble loss:  0.335 RMSE: 0.876 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.234 Test loss:  1.862 Ensemble loss:  0.304 RMSE: 0.858 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.345 Test loss:  3.354 Ensemble loss:  0.320 RMSE: 0.861 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.408 Test loss:  4.401 Ensemble loss:  0.333 RMSE: 0.864 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.427 Test loss:  2.292 Ensemble loss:  0.325 RMSE: 0.854 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.051 Test loss:  1.617 Ensemble loss:  0.309 RMSE: 0.835 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.442 Test loss:  2.132 Ensemble loss:  0.304 RMSE: 0.829 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.452 Test loss:  2.704 Ensemble loss:  0.303 RMSE: 0.830 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.428 Test loss:  1.103 Ensemble loss:  0.306 RMSE: 0.831 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.362 Test loss:  4.043 Ensemble loss:  0.298 RMSE: 0.820 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.478 Test loss:  3.007 Ensemble loss:  0.284 RMSE: 0.814 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.449 Test loss:  3.445 Ensemble loss:  0.289 RMSE: 0.814 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.191 Test loss:  1.980 Ensemble loss:  0.270 RMSE: 0.807 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.391 Test loss:  1.807 Ensemble loss:  0.273 RMSE: 0.808 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.402 Test loss:  1.817 Ensemble loss:  0.268 RMSE: 0.805 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.022 Test loss:  1.225 Ensemble loss:  0.264 RMSE: 0.809 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.206 Test loss:  1.015 Ensemble loss:  0.263 RMSE: 0.810 Num. networks: 30\n",
            "FOLD 3:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.488 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.389 Test loss:  0.431 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.020 Test loss:  0.177 Ensemble loss:  0.326 RMSE: 0.877 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.214 Test loss:  0.282 Ensemble loss:  0.169 RMSE: 0.789 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.285 Test loss:  0.781 Ensemble loss:  0.191 RMSE: 0.807 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.376 Test loss:  1.874 Ensemble loss:  0.228 RMSE: 0.830 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.084 Test loss:  0.717 Ensemble loss:  0.250 RMSE: 0.837 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.307 Test loss:  1.084 Ensemble loss:  0.255 RMSE: 0.840 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.424 Test loss:  1.063 Ensemble loss:  0.244 RMSE: 0.837 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.316 Test loss:  1.280 Ensemble loss:  0.218 RMSE: 0.824 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.376 Test loss:  1.365 Ensemble loss:  0.201 RMSE: 0.816 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.344 Test loss:  3.255 Ensemble loss:  0.172 RMSE: 0.796 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.513 Test loss:  2.145 Ensemble loss:  0.147 RMSE: 0.779 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.305 Test loss:  1.284 Ensemble loss:  0.144 RMSE: 0.776 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.565 Test loss:  2.790 Ensemble loss:  0.142 RMSE: 0.776 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.478 Test loss:  3.065 Ensemble loss:  0.134 RMSE: 0.770 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.456 Test loss:  1.738 Ensemble loss:  0.131 RMSE: 0.768 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.511 Test loss:  4.397 Ensemble loss:  0.122 RMSE: 0.761 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.533 Test loss:  2.183 Ensemble loss:  0.112 RMSE: 0.756 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.575 Test loss:  6.675 Ensemble loss:  0.101 RMSE: 0.750 Num. networks: 30\n",
            "FOLD 4:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.541 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.414 Test loss:  0.478 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.033 Test loss:  1.011 Ensemble loss:  0.481 RMSE: 0.961 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.212 Test loss:  1.436 Ensemble loss:  0.470 RMSE: 0.905 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.266 Test loss:  1.141 Ensemble loss:  0.452 RMSE: 0.901 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.273 Test loss:  1.885 Ensemble loss:  0.430 RMSE: 0.892 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.304 Test loss:  3.027 Ensemble loss:  0.410 RMSE: 0.893 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.175 Test loss:  2.115 Ensemble loss:  0.399 RMSE: 0.897 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.285 Test loss:  2.029 Ensemble loss:  0.383 RMSE: 0.905 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.331 Test loss:  1.891 Ensemble loss:  0.368 RMSE: 0.915 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.243 Test loss:  4.116 Ensemble loss:  0.366 RMSE: 0.919 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.341 Test loss:  4.817 Ensemble loss:  0.372 RMSE: 0.922 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.417 Test loss:  2.290 Ensemble loss:  0.376 RMSE: 0.926 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.410 Test loss:  2.369 Ensemble loss:  0.377 RMSE: 0.931 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.315 Test loss:  2.612 Ensemble loss:  0.381 RMSE: 0.939 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.166 Test loss:  2.645 Ensemble loss:  0.381 RMSE: 0.945 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.399 Test loss:  2.808 Ensemble loss:  0.377 RMSE: 0.946 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.441 Test loss:  3.713 Ensemble loss:  0.379 RMSE: 0.943 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.472 Test loss:  4.737 Ensemble loss:  0.382 RMSE: 0.937 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.483 Test loss:  5.965 Ensemble loss:  0.383 RMSE: 0.932 Num. networks: 30\n",
            "FOLD 5:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.278 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.348 Test loss:  0.322 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.070 Test loss:  0.562 Ensemble loss:  0.275 RMSE: 0.888 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.235 Test loss:  1.160 Ensemble loss:  0.249 RMSE: 0.853 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.062 Test loss:  3.280 Ensemble loss:  0.181 RMSE: 0.829 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.287 Test loss:  2.003 Ensemble loss:  0.162 RMSE: 0.806 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.269 Test loss:  1.801 Ensemble loss:  0.157 RMSE: 0.809 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.386 Test loss:  2.685 Ensemble loss:  0.157 RMSE: 0.806 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.271 Test loss:  2.055 Ensemble loss:  0.140 RMSE: 0.799 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.373 Test loss:  1.256 Ensemble loss:  0.132 RMSE: 0.796 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.414 Test loss:  0.999 Ensemble loss:  0.121 RMSE: 0.793 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.454 Test loss:  2.506 Ensemble loss:  0.107 RMSE: 0.777 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.336 Test loss:  1.679 Ensemble loss:  0.093 RMSE: 0.772 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.189 Test loss:  1.512 Ensemble loss:  0.084 RMSE: 0.768 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.101 Test loss:  2.629 Ensemble loss:  0.081 RMSE: 0.761 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.326 Test loss:  1.393 Ensemble loss:  0.073 RMSE: 0.753 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.394 Test loss:  1.786 Ensemble loss:  0.068 RMSE: 0.750 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.434 Test loss:  4.592 Ensemble loss:  0.062 RMSE: 0.750 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.388 Test loss:  1.905 Ensemble loss:  0.056 RMSE: 0.751 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.386 Test loss:  3.233 Ensemble loss:  0.054 RMSE: 0.749 Num. networks: 30\n",
            "FOLD 6:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.458 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.361 Test loss:  0.372 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.076 Test loss:  0.200 Ensemble loss:  0.196 RMSE: 0.809 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.214 Test loss:  0.218 Ensemble loss:  0.103 RMSE: 0.724 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.190 Test loss:  1.211 Ensemble loss:  0.085 RMSE: 0.720 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.225 Test loss:  0.510 Ensemble loss:  0.047 RMSE: 0.690 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.283 Test loss:  0.562 Ensemble loss:  0.007 RMSE: 0.654 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.075 Test loss:  0.446 Ensemble loss:  0.008 RMSE: 0.646 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.334 Test loss:  0.937 Ensemble loss:  0.006 RMSE: 0.634 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.262 Test loss:  0.481 Ensemble loss:  0.007 RMSE: 0.629 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.389 Test loss:  1.107 Ensemble loss: -0.000 RMSE: 0.620 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.316 Test loss:  0.990 Ensemble loss: -0.004 RMSE: 0.618 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.453 Test loss:  4.292 Ensemble loss:  0.001 RMSE: 0.626 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.252 Test loss:  1.851 Ensemble loss:  0.003 RMSE: 0.632 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.456 Test loss:  2.359 Ensemble loss:  0.007 RMSE: 0.643 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.089 Test loss:  0.996 Ensemble loss:  0.016 RMSE: 0.646 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.421 Test loss:  1.214 Ensemble loss:  0.011 RMSE: 0.642 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.441 Test loss:  1.512 Ensemble loss:  0.017 RMSE: 0.649 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.415 Test loss:  3.547 Ensemble loss:  0.014 RMSE: 0.648 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.496 Test loss:  3.911 Ensemble loss:  0.017 RMSE: 0.648 Num. networks: 30\n",
            "FOLD 7:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.453 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.427 Test loss:  0.360 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.012 Test loss:  0.391 Ensemble loss:  0.264 RMSE: 0.858 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.207 Test loss:  1.180 Ensemble loss:  0.209 RMSE: 0.852 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.254 Test loss:  0.713 Ensemble loss:  0.223 RMSE: 0.851 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.225 Test loss:  0.994 Ensemble loss:  0.188 RMSE: 0.815 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.161 Test loss:  0.348 Ensemble loss:  0.136 RMSE: 0.761 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.258 Test loss:  1.133 Ensemble loss:  0.133 RMSE: 0.755 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.286 Test loss:  1.064 Ensemble loss:  0.124 RMSE: 0.767 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.396 Test loss:  1.823 Ensemble loss:  0.115 RMSE: 0.772 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.382 Test loss:  2.275 Ensemble loss:  0.102 RMSE: 0.772 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.348 Test loss:  3.748 Ensemble loss:  0.094 RMSE: 0.779 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.398 Test loss:  4.372 Ensemble loss:  0.102 RMSE: 0.797 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.145 Test loss:  2.136 Ensemble loss:  0.104 RMSE: 0.802 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.464 Test loss:  9.225 Ensemble loss:  0.100 RMSE: 0.805 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.257 Test loss:  2.985 Ensemble loss:  0.097 RMSE: 0.813 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.437 Test loss:  3.168 Ensemble loss:  0.084 RMSE: 0.813 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.502 Test loss:  2.030 Ensemble loss:  0.080 RMSE: 0.817 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.324 Test loss:  4.402 Ensemble loss:  0.096 RMSE: 0.827 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.384 Test loss:  1.421 Ensemble loss:  0.106 RMSE: 0.830 Num. networks: 30\n",
            "FOLD 8:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.618 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.364 Test loss:  0.654 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.031 Test loss:  0.410 Ensemble loss:  0.468 RMSE: 1.013 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.145 Test loss:  0.839 Ensemble loss:  0.269 RMSE: 0.894 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.094 Test loss:  1.885 Ensemble loss:  0.266 RMSE: 0.858 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.173 Test loss: 60.288 Ensemble loss:  0.180 RMSE: 0.797 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.159 Test loss: 39.690 Ensemble loss:  0.131 RMSE: 0.773 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.210 Test loss:  1.407 Ensemble loss:  0.134 RMSE: 0.771 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.277 Test loss:  8.321 Ensemble loss:  0.138 RMSE: 0.774 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.095 Test loss:  1.836 Ensemble loss:  0.117 RMSE: 0.771 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.306 Test loss:  1.521 Ensemble loss:  0.114 RMSE: 0.763 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.283 Test loss: 99.382 Ensemble loss:  0.091 RMSE: 0.746 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.266 Test loss:  7.258 Ensemble loss:  0.073 RMSE: 0.756 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.337 Test loss:  4.268 Ensemble loss:  0.072 RMSE: 0.758 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.180 Test loss:  2.588 Ensemble loss:  0.075 RMSE: 0.762 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.203 Test loss:  7.538 Ensemble loss:  0.079 RMSE: 0.770 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.377 Test loss:  6.392 Ensemble loss:  0.082 RMSE: 0.772 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.072 Test loss:  4.952 Ensemble loss:  0.081 RMSE: 0.777 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.029 Test loss:  3.076 Ensemble loss:  0.070 RMSE: 0.779 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.334 Test loss:  3.595 Ensemble loss:  0.058 RMSE: 0.778 Num. networks: 30\n",
            "FOLD 9:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.604 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.409 Test loss:  0.525 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.002 Test loss:  0.279 Ensemble loss:  0.346 RMSE: 0.922 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.235 Test loss:  0.536 Ensemble loss:  0.229 RMSE: 0.825 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.126 Test loss:  1.434 Ensemble loss:  0.233 RMSE: 0.824 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.308 Test loss:  1.073 Ensemble loss:  0.246 RMSE: 0.824 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.163 Test loss:  2.365 Ensemble loss:  0.236 RMSE: 0.811 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.017 Test loss:  1.685 Ensemble loss:  0.238 RMSE: 0.812 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.388 Test loss:  1.547 Ensemble loss:  0.195 RMSE: 0.807 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.304 Test loss:  1.250 Ensemble loss:  0.107 RMSE: 0.770 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.403 Test loss:  3.227 Ensemble loss:  0.085 RMSE: 0.750 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.376 Test loss:  2.908 Ensemble loss:  0.060 RMSE: 0.727 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.390 Test loss:  8.799 Ensemble loss:  0.043 RMSE: 0.704 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.416 Test loss:  8.432 Ensemble loss:  0.039 RMSE: 0.695 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.451 Test loss:  3.958 Ensemble loss:  0.033 RMSE: 0.687 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.493 Test loss: 13.824 Ensemble loss:  0.027 RMSE: 0.679 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.494 Test loss:  5.651 Ensemble loss:  0.027 RMSE: 0.676 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.458 Test loss:  2.000 Ensemble loss:  0.023 RMSE: 0.671 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.509 Test loss:  2.777 Ensemble loss:  0.016 RMSE: 0.665 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.550 Test loss:  6.202 Ensemble loss:  0.014 RMSE: 0.662 Num. networks: 30\n",
            "Train log. lik. =   0.249 +/-  0.032\n",
            "Test  log. lik. =  -0.274 +/-  0.148\n",
            "Train RMSE      =   0.452 +/-  0.022\n",
            "Test  RMSE      =   0.802 +/-  0.101\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "data = pd.read_csv('/content/drive/MyDrive/bayesian-deep-learning-master/data/Datasets.csv').values\n",
        "ensemble = train_mc_dropout(data=data, n_splits=10, burn_in=350, mix_time=300, num_nets=30,\n",
        "                            num_units=110, learn_rate=1e-2/len(data), weight_decay=0.8, log_every=500)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}