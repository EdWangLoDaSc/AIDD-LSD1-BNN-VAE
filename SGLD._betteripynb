{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/SGLD._betteripynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneWjImThPub",
        "outputId": "c931ceab-c216-42c6-a3ab-d459da83a786"
      },
      "id": "YneWjImThPub",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94c44267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c44267",
        "outputId": "fdadb5b1-9377-4fc1-c8f9-b3bc69fc7238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.7.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565154 sha256=0d0fa9b17f1d3925cdee75b6944e0159f84c331468bd70ada039b80dcfe7ffe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=dc5ed5521084d24dc311a7c84fce76c733c001b5509a0351fed7bf1bfdd8c8cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XkP-XKI5mOhv"
      },
      "id": "XkP-XKI5mOhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7f11def2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f11def2",
        "outputId": "5590532b-6048-4471-842b-0e65638e032c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "613d2696",
      "metadata": {
        "id": "613d2696"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b95dd8e4",
      "metadata": {
        "id": "b95dd8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6a5ba769",
      "metadata": {
        "id": "6a5ba769"
      },
      "outputs": [],
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dc555aa8",
      "metadata": {
        "id": "dc555aa8"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "24ee5406",
      "metadata": {
        "id": "24ee5406"
      },
      "outputs": [],
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "189bc6ae",
      "metadata": {
        "id": "189bc6ae"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, num_units)\n",
        "        self.layer3 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        #x_train = (x_train - x_means)/x_stds\n",
        "        #y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        #x_test = (x_test - x_means)/x_stds\n",
        "        #y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=Langevin_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=batch_size, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 0: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    return nets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "3b9afbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9afbcd",
        "outputId": "7d9e9ffb-0047-44db-c19e-139879cfbcd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0:\n",
            "Epoch:    0, Train loss: 19.099 Test loss:  9.587 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.555 Test loss:  0.676 Ensemble loss:    nan RMSE: 1.444 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.039 Test loss:  0.678 Ensemble loss:  0.474 RMSE: 1.013 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.088 Test loss:  0.862 Ensemble loss:  0.382 RMSE: 0.939 Num. networks:  8\n",
            "Epoch: 2000, Train loss:  0.314 Test loss:  0.540 Ensemble loss:  0.367 RMSE: 0.943 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.086 Test loss:  1.541 Ensemble loss:  0.413 RMSE: 0.983 Num. networks: 14\n",
            "Epoch: 3000, Train loss:  0.261 Test loss:  1.367 Ensemble loss:  0.432 RMSE: 1.021 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.045 Test loss:  1.158 Ensemble loss:  0.434 RMSE: 1.019 Num. networks: 20\n",
            "FOLD 1:\n",
            "Epoch:    0, Train loss: 18.796 Test loss: 10.486 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.567 Test loss:  0.493 Ensemble loss:    nan RMSE: 1.113 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.109 Test loss:  1.073 Ensemble loss:  0.501 RMSE: 0.990 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.030 Test loss:  2.935 Ensemble loss:  0.489 RMSE: 1.057 Num. networks:  8\n",
            "Epoch: 2000, Train loss:  0.617 Test loss:  0.966 Ensemble loss:  0.531 RMSE: 1.113 Num. networks: 11\n",
            "Epoch: 2500, Train loss:  0.519 Test loss:  4.157 Ensemble loss:  0.504 RMSE: 1.082 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.075 Test loss:  3.186 Ensemble loss:  0.491 RMSE: 1.062 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.134 Test loss:  8.483 Ensemble loss:  0.471 RMSE: 1.048 Num. networks: 20\n",
            "FOLD 2:\n",
            "Epoch:    0, Train loss: 18.673 Test loss: 12.762 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.596 Test loss:  0.558 Ensemble loss:    nan RMSE: 1.588 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.035 Test loss:  0.969 Ensemble loss:  0.475 RMSE: 1.076 Num. networks:  4\n",
            "Epoch: 1500, Train loss:  0.193 Test loss:  1.344 Ensemble loss:  0.408 RMSE: 1.033 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.133 Test loss:  2.882 Ensemble loss:  0.423 RMSE: 1.062 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.043 Test loss:  6.555 Ensemble loss:  0.417 RMSE: 1.070 Num. networks: 14\n",
            "Epoch: 3000, Train loss:  0.370 Test loss:  1.144 Ensemble loss:  0.451 RMSE: 1.061 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.132 Test loss:  2.007 Ensemble loss:  0.451 RMSE: 1.052 Num. networks: 20\n",
            "FOLD 3:\n",
            "Epoch:    0, Train loss: 17.926 Test loss: 16.097 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.443 Test loss:  0.478 Ensemble loss:    nan RMSE: 1.628 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.081 Test loss:  0.440 Ensemble loss:  0.418 RMSE: 0.979 Num. networks:  4\n",
            "Epoch: 1500, Train loss:  0.143 Test loss:  0.805 Ensemble loss:  0.270 RMSE: 0.847 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.063 Test loss:  0.527 Ensemble loss:  0.274 RMSE: 0.852 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.027 Test loss:  1.163 Ensemble loss:  0.259 RMSE: 0.841 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.003 Test loss:  1.027 Ensemble loss:  0.228 RMSE: 0.835 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.126 Test loss:  2.791 Ensemble loss:  0.216 RMSE: 0.828 Num. networks: 20\n",
            "FOLD 4:\n",
            "Epoch:    0, Train loss: 18.260 Test loss: 14.852 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.371 Test loss:  1.167 Ensemble loss:    nan RMSE: 1.582 Num. networks:  1\n",
            "Epoch: 1000, Train loss: -0.061 Test loss:  2.327 Ensemble loss:  0.873 RMSE: 1.279 Num. networks:  4\n",
            "Epoch: 1500, Train loss:  0.058 Test loss:  3.338 Ensemble loss:  0.799 RMSE: 1.225 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.257 Test loss:  3.755 Ensemble loss:  0.794 RMSE: 1.226 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.066 Test loss:  3.180 Ensemble loss:  0.811 RMSE: 1.236 Num. networks: 14\n",
            "Epoch: 3000, Train loss:  0.075 Test loss:  2.450 Ensemble loss:  0.761 RMSE: 1.225 Num. networks: 18\n",
            "Epoch: 3300, Train loss:  0.209 Test loss:  5.851 Ensemble loss:  0.697 RMSE: 1.210 Num. networks: 20\n",
            "FOLD 5:\n",
            "Epoch:    0, Train loss: 18.552 Test loss: 13.605 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.480 Test loss:  0.488 Ensemble loss:    nan RMSE: 1.270 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.122 Test loss:  0.800 Ensemble loss:  0.373 RMSE: 1.002 Num. networks:  4\n",
            "Epoch: 1500, Train loss:  0.168 Test loss:  0.710 Ensemble loss:  0.321 RMSE: 0.968 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.101 Test loss:  1.856 Ensemble loss:  0.312 RMSE: 0.954 Num. networks: 11\n",
            "Epoch: 2500, Train loss:  0.517 Test loss:  1.058 Ensemble loss:  0.307 RMSE: 0.938 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.178 Test loss:  2.794 Ensemble loss:  0.287 RMSE: 0.909 Num. networks: 18\n",
            "Epoch: 3300, Train loss:  0.131 Test loss:  1.576 Ensemble loss:  0.279 RMSE: 0.897 Num. networks: 20\n",
            "FOLD 6:\n",
            "Epoch:    0, Train loss: 18.411 Test loss: 15.468 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.529 Test loss:  0.680 Ensemble loss:    nan RMSE: 1.636 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.019 Test loss:  0.338 Ensemble loss:  0.462 RMSE: 1.147 Num. networks:  4\n",
            "Epoch: 1500, Train loss:  0.127 Test loss:  0.786 Ensemble loss:  0.351 RMSE: 0.996 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.035 Test loss:  1.117 Ensemble loss:  0.337 RMSE: 0.953 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.071 Test loss:  1.225 Ensemble loss:  0.361 RMSE: 0.928 Num. networks: 14\n",
            "Epoch: 3000, Train loss:  0.190 Test loss:  1.027 Ensemble loss:  0.341 RMSE: 0.879 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.028 Test loss:  2.358 Ensemble loss:  0.338 RMSE: 0.871 Num. networks: 20\n",
            "FOLD 7:\n",
            "Epoch:    0, Train loss: 18.004 Test loss: 15.843 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.603 Test loss:  0.769 Ensemble loss:    nan RMSE: 2.342 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.003 Test loss:  0.638 Ensemble loss:  0.635 RMSE: 1.351 Num. networks:  4\n",
            "Epoch: 1500, Train loss:  0.082 Test loss:  3.235 Ensemble loss:  0.578 RMSE: 1.283 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.162 Test loss:  4.579 Ensemble loss:  0.519 RMSE: 1.209 Num. networks: 11\n",
            "Epoch: 2500, Train loss: -0.136 Test loss:  2.056 Ensemble loss:  0.502 RMSE: 1.201 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.047 Test loss:  1.784 Ensemble loss:  0.464 RMSE: 1.188 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.043 Test loss:  2.311 Ensemble loss:  0.439 RMSE: 1.167 Num. networks: 20\n",
            "FOLD 8:\n",
            "Epoch:    0, Train loss: 18.359 Test loss: 12.829 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.383 Test loss:  2.773 Ensemble loss:    nan RMSE: 1.436 Num. networks:  1\n",
            "Epoch: 1000, Train loss: -0.016 Test loss:  2.162 Ensemble loss:  0.728 RMSE: 1.132 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.004 Test loss:  1.746 Ensemble loss:  0.361 RMSE: 1.017 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.026 Test loss:  2.996 Ensemble loss:  0.347 RMSE: 0.996 Num. networks: 11\n",
            "Epoch: 2500, Train loss:  0.032 Test loss:  1.559 Ensemble loss:  0.312 RMSE: 0.967 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.055 Test loss:  1.246 Ensemble loss:  0.300 RMSE: 0.956 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.115 Test loss:  2.931 Ensemble loss:  0.290 RMSE: 0.945 Num. networks: 20\n",
            "FOLD 9:\n",
            "Epoch:    0, Train loss: 18.788 Test loss: 12.596 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.419 Test loss:  0.394 Ensemble loss:    nan RMSE: 1.196 Num. networks:  1\n",
            "Epoch: 1000, Train loss:  0.046 Test loss:  1.413 Ensemble loss:  0.334 RMSE: 1.000 Num. networks:  4\n",
            "Epoch: 1500, Train loss: -0.072 Test loss:  0.673 Ensemble loss:  0.265 RMSE: 0.909 Num. networks:  8\n",
            "Epoch: 2000, Train loss: -0.116 Test loss:  1.788 Ensemble loss:  0.257 RMSE: 0.886 Num. networks: 11\n",
            "Epoch: 2500, Train loss:  0.012 Test loss:  3.053 Ensemble loss:  0.251 RMSE: 0.870 Num. networks: 14\n",
            "Epoch: 3000, Train loss: -0.125 Test loss:  1.272 Ensemble loss:  0.232 RMSE: 0.860 Num. networks: 18\n",
            "Epoch: 3300, Train loss: -0.056 Test loss:  3.399 Ensemble loss:  0.226 RMSE: 0.850 Num. networks: 20\n",
            "Train log. lik. =  -0.104 +/-  0.034\n",
            "Test  log. lik. =  -0.479 +/-  0.136\n",
            "Train RMSE      =   0.631 +/-  0.027\n",
            "Test  RMSE      =   0.989 +/-  0.126\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "data = pd.read_csv('/content/drive/MyDrive/bayesian-deep-learning-master/data/Datasets.csv').values\n",
        "ensemble = train_mc_dropout(data=data, n_splits=10, burn_in=300, mix_time=150, num_nets=20,\n",
        "                            num_units=140, learn_rate=1e-2/len(data), weight_decay=1, log_every=500)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}