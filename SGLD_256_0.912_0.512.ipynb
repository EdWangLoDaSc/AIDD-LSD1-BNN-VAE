{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/SGLD_256_0.912_0.512.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneWjImThPub",
        "outputId": "1beafc00-9d37-4f44-ea89-12bec0d8d6d3"
      },
      "id": "YneWjImThPub",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "94c44267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c44267",
        "outputId": "f2f08acf-8e1c-44c9-d0aa-3d9782e0bb9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.7.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565116 sha256=d8158aaef00232b1d707dffa139ea27ad077583e36dd39ccc4c6212fc6c7991a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=8bfe6e11c8968bc799cfe11c999d8abdf585e70ef100af62808369165caba1b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XkP-XKI5mOhv"
      },
      "id": "XkP-XKI5mOhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7f11def2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f11def2",
        "outputId": "9fadaae9-625c-469f-cffa-1979ff53c5ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "613d2696",
      "metadata": {
        "id": "613d2696"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b95dd8e4",
      "metadata": {
        "id": "b95dd8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6a5ba769",
      "metadata": {
        "id": "6a5ba769"
      },
      "outputs": [],
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dc555aa8",
      "metadata": {
        "id": "dc555aa8"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "24ee5406",
      "metadata": {
        "id": "24ee5406"
      },
      "outputs": [],
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "189bc6ae",
      "metadata": {
        "id": "189bc6ae"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, num_units)\n",
        "        self.layer3 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        #x_train = (x_train - x_means)/x_stds\n",
        "        #y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        #x_test = (x_test - x_means)/x_stds\n",
        "        #y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=Langevin_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=batch_size, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 0: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    return nets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3b9afbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9afbcd",
        "outputId": "f89b9ae0-8e0f-4e7b-8d25-7de63fcf246f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0:\n",
            "Epoch:    0, Train loss: 19.081 Test loss:  9.680 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.048 Test loss:  0.694 Ensemble loss:  0.616 RMSE: 1.153 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -0.111 Test loss:  0.816 Ensemble loss:  0.501 RMSE: 1.069 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.115 Test loss:  2.127 Ensemble loss:  0.475 RMSE: 1.052 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.063 Test loss:  1.107 Ensemble loss:  0.476 RMSE: 1.041 Num. networks: 12\n",
            "Epoch: 5000, Train loss:  0.193 Test loss:  0.973 Ensemble loss:  0.481 RMSE: 1.039 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.160 Test loss:  2.260 Ensemble loss:  0.484 RMSE: 1.025 Num. networks: 19\n",
            "Epoch: 7000, Train loss:  0.018 Test loss:  4.996 Ensemble loss:  0.496 RMSE: 1.029 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -0.183 Test loss:  3.112 Ensemble loss:  0.496 RMSE: 1.022 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -0.173 Test loss:  3.656 Ensemble loss:  0.472 RMSE: 0.997 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.140 Test loss:  2.860 Ensemble loss:  0.470 RMSE: 0.990 Num. networks: 30\n",
            "FOLD 1:\n",
            "Epoch:    0, Train loss: 18.754 Test loss: 10.443 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.098 Test loss:  1.091 Ensemble loss:  0.489 RMSE: 1.039 Num. networks:  2\n",
            "Epoch: 2000, Train loss:  0.071 Test loss:  0.876 Ensemble loss:  0.432 RMSE: 0.994 Num. networks:  5\n",
            "Epoch: 3000, Train loss:  0.052 Test loss:  3.553 Ensemble loss:  0.404 RMSE: 0.954 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.004 Test loss:  2.206 Ensemble loss:  0.398 RMSE: 0.932 Num. networks: 12\n",
            "Epoch: 5000, Train loss:  0.031 Test loss:  1.521 Ensemble loss:  0.399 RMSE: 0.927 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.165 Test loss:  2.521 Ensemble loss:  0.399 RMSE: 0.899 Num. networks: 19\n",
            "Epoch: 7000, Train loss:  0.110 Test loss:  1.237 Ensemble loss:  0.401 RMSE: 0.883 Num. networks: 22\n",
            "Epoch: 8000, Train loss:  0.387 Test loss:  2.070 Ensemble loss:  0.400 RMSE: 0.893 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -0.033 Test loss:  7.048 Ensemble loss:  0.414 RMSE: 0.905 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.158 Test loss: 21.609 Ensemble loss:  0.416 RMSE: 0.910 Num. networks: 30\n",
            "FOLD 2:\n",
            "Epoch:    0, Train loss: 18.460 Test loss: 12.497 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.086 Test loss:  0.612 Ensemble loss:  0.476 RMSE: 1.044 Num. networks:  2\n",
            "Epoch: 2000, Train loss:  0.006 Test loss:  0.543 Ensemble loss:  0.361 RMSE: 0.942 Num. networks:  5\n",
            "Epoch: 3000, Train loss:  0.057 Test loss:  5.044 Ensemble loss:  0.337 RMSE: 0.929 Num. networks:  9\n",
            "Epoch: 4000, Train loss:  0.043 Test loss:  1.713 Ensemble loss:  0.334 RMSE: 0.926 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.148 Test loss:  2.970 Ensemble loss:  0.339 RMSE: 0.936 Num. networks: 15\n",
            "Epoch: 6000, Train loss:  0.056 Test loss:  1.491 Ensemble loss:  0.362 RMSE: 0.964 Num. networks: 19\n",
            "Epoch: 7000, Train loss:  0.053 Test loss:  2.009 Ensemble loss:  0.371 RMSE: 0.978 Num. networks: 22\n",
            "Epoch: 8000, Train loss:  0.006 Test loss:  1.264 Ensemble loss:  0.380 RMSE: 0.996 Num. networks: 25\n",
            "Epoch: 9000, Train loss:  0.476 Test loss:  1.511 Ensemble loss:  0.376 RMSE: 0.998 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.101 Test loss:  1.528 Ensemble loss:  0.379 RMSE: 1.001 Num. networks: 30\n",
            "FOLD 3:\n",
            "Epoch:    0, Train loss: 18.160 Test loss: 16.482 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.104 Test loss:  0.470 Ensemble loss:  0.346 RMSE: 0.977 Num. networks:  2\n",
            "Epoch: 2000, Train loss:  0.106 Test loss:  1.176 Ensemble loss:  0.233 RMSE: 0.822 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.099 Test loss:  3.836 Ensemble loss:  0.244 RMSE: 0.842 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.114 Test loss:  2.071 Ensemble loss:  0.255 RMSE: 0.852 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.042 Test loss:  2.463 Ensemble loss:  0.230 RMSE: 0.831 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.067 Test loss:  1.290 Ensemble loss:  0.228 RMSE: 0.839 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -0.159 Test loss:  4.426 Ensemble loss:  0.209 RMSE: 0.847 Num. networks: 22\n",
            "Epoch: 8000, Train loss:  0.145 Test loss:  0.975 Ensemble loss:  0.210 RMSE: 0.842 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -0.134 Test loss:  1.553 Ensemble loss:  0.233 RMSE: 0.847 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.235 Test loss:  2.821 Ensemble loss:  0.231 RMSE: 0.845 Num. networks: 30\n",
            "FOLD 4:\n",
            "Epoch:    0, Train loss: 17.977 Test loss: 14.579 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.073 Test loss:  0.948 Ensemble loss:  0.659 RMSE: 1.266 Num. networks:  2\n",
            "Epoch: 2000, Train loss:  0.016 Test loss:  1.969 Ensemble loss:  0.518 RMSE: 1.086 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.161 Test loss:  4.821 Ensemble loss:  0.512 RMSE: 1.049 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.146 Test loss:  3.774 Ensemble loss:  0.545 RMSE: 1.063 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.103 Test loss: 11.675 Ensemble loss:  0.539 RMSE: 1.055 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.165 Test loss: 14.006 Ensemble loss:  0.535 RMSE: 1.064 Num. networks: 19\n",
            "Epoch: 7000, Train loss:  0.582 Test loss:  1.015 Ensemble loss:  0.519 RMSE: 1.054 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -0.139 Test loss:  2.597 Ensemble loss:  0.473 RMSE: 1.029 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -0.034 Test loss:  3.626 Ensemble loss:  0.453 RMSE: 1.005 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.022 Test loss:  4.092 Ensemble loss:  0.448 RMSE: 1.000 Num. networks: 30\n",
            "FOLD 5:\n",
            "Epoch:    0, Train loss: 18.747 Test loss: 13.479 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.054 Test loss:  0.612 Ensemble loss:  0.470 RMSE: 1.094 Num. networks:  2\n",
            "Epoch: 2000, Train loss:  0.022 Test loss:  1.281 Ensemble loss:  0.399 RMSE: 1.023 Num. networks:  5\n",
            "Epoch: 3000, Train loss:  0.401 Test loss:  1.369 Ensemble loss:  0.323 RMSE: 0.966 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.027 Test loss:  1.944 Ensemble loss:  0.272 RMSE: 0.936 Num. networks: 12\n",
            "Epoch: 5000, Train loss:  0.029 Test loss:  3.677 Ensemble loss:  0.285 RMSE: 0.927 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.137 Test loss:  3.445 Ensemble loss:  0.295 RMSE: 0.923 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -0.062 Test loss:  1.222 Ensemble loss:  0.314 RMSE: 0.932 Num. networks: 22\n",
            "Epoch: 8000, Train loss:  0.427 Test loss:  0.644 Ensemble loss:  0.311 RMSE: 0.924 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -0.172 Test loss:  2.753 Ensemble loss:  0.299 RMSE: 0.914 Num. networks: 29\n",
            "Epoch: 9350, Train loss:  0.363 Test loss:  1.840 Ensemble loss:  0.296 RMSE: 0.912 Num. networks: 30\n",
            "FOLD 6:\n",
            "Epoch:    0, Train loss: 18.480 Test loss: 15.493 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.132 Test loss:  0.408 Ensemble loss:  0.342 RMSE: 0.917 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -0.063 Test loss:  0.525 Ensemble loss:  0.287 RMSE: 0.820 Num. networks:  5\n",
            "Epoch: 3000, Train loss:  0.013 Test loss:  0.998 Ensemble loss:  0.219 RMSE: 0.801 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.084 Test loss:  0.746 Ensemble loss:  0.234 RMSE: 0.819 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.014 Test loss:  1.655 Ensemble loss:  0.235 RMSE: 0.799 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.069 Test loss:  1.442 Ensemble loss:  0.247 RMSE: 0.778 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -0.016 Test loss:  0.689 Ensemble loss:  0.262 RMSE: 0.780 Num. networks: 22\n",
            "Epoch: 8000, Train loss:  0.386 Test loss:  0.704 Ensemble loss:  0.245 RMSE: 0.772 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -0.092 Test loss:  1.506 Ensemble loss:  0.251 RMSE: 0.786 Num. networks: 29\n",
            "Epoch: 9350, Train loss:  0.276 Test loss:  0.766 Ensemble loss:  0.251 RMSE: 0.785 Num. networks: 30\n",
            "FOLD 7:\n",
            "Epoch:    0, Train loss: 17.958 Test loss: 15.909 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.063 Test loss:  0.542 Ensemble loss:  0.458 RMSE: 1.133 Num. networks:  2\n",
            "Epoch: 2000, Train loss:  0.070 Test loss:  0.709 Ensemble loss:  0.331 RMSE: 0.902 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.037 Test loss:  1.543 Ensemble loss:  0.310 RMSE: 0.873 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.179 Test loss:  5.369 Ensemble loss:  0.371 RMSE: 0.920 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.131 Test loss:  6.447 Ensemble loss:  0.427 RMSE: 0.987 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.087 Test loss:  2.179 Ensemble loss:  0.411 RMSE: 0.999 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -0.082 Test loss:  1.476 Ensemble loss:  0.396 RMSE: 0.995 Num. networks: 22\n",
            "Epoch: 8000, Train loss:  0.112 Test loss:  2.024 Ensemble loss:  0.403 RMSE: 0.996 Num. networks: 25\n",
            "Epoch: 9000, Train loss:  0.075 Test loss:  0.471 Ensemble loss:  0.383 RMSE: 0.971 Num. networks: 29\n",
            "Epoch: 9350, Train loss:  0.226 Test loss:  0.451 Ensemble loss:  0.374 RMSE: 0.964 Num. networks: 30\n",
            "FOLD 8:\n",
            "Epoch:    0, Train loss: 18.781 Test loss: 12.949 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.080 Test loss:  0.768 Ensemble loss:  0.474 RMSE: 1.076 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -0.068 Test loss:  0.761 Ensemble loss:  0.427 RMSE: 1.002 Num. networks:  5\n",
            "Epoch: 3000, Train loss:  0.126 Test loss:  0.782 Ensemble loss:  0.321 RMSE: 0.948 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.070 Test loss:  1.621 Ensemble loss:  0.290 RMSE: 0.926 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.036 Test loss:  2.068 Ensemble loss:  0.269 RMSE: 0.928 Num. networks: 15\n",
            "Epoch: 6000, Train loss:  0.053 Test loss:  2.391 Ensemble loss:  0.274 RMSE: 0.930 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -0.082 Test loss:  1.940 Ensemble loss:  0.243 RMSE: 0.911 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -0.110 Test loss:  1.063 Ensemble loss:  0.249 RMSE: 0.896 Num. networks: 25\n",
            "Epoch: 9000, Train loss:  0.143 Test loss:  1.412 Ensemble loss:  0.232 RMSE: 0.872 Num. networks: 29\n",
            "Epoch: 9350, Train loss:  0.141 Test loss:  0.708 Ensemble loss:  0.225 RMSE: 0.867 Num. networks: 30\n",
            "FOLD 9:\n",
            "Epoch:    0, Train loss: 18.400 Test loss: 12.344 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.058 Test loss:  0.390 Ensemble loss:  0.447 RMSE: 1.063 Num. networks:  2\n",
            "Epoch: 2000, Train loss:  0.437 Test loss:  0.626 Ensemble loss:  0.222 RMSE: 0.868 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.006 Test loss:  2.698 Ensemble loss:  0.235 RMSE: 0.854 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.032 Test loss:  1.778 Ensemble loss:  0.229 RMSE: 0.862 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.147 Test loss:  1.473 Ensemble loss:  0.228 RMSE: 0.865 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.063 Test loss: 10.939 Ensemble loss:  0.237 RMSE: 0.874 Num. networks: 19\n",
            "Epoch: 7000, Train loss:  0.208 Test loss:  1.317 Ensemble loss:  0.223 RMSE: 0.857 Num. networks: 22\n",
            "Epoch: 8000, Train loss:  0.067 Test loss:  6.580 Ensemble loss:  0.227 RMSE: 0.852 Num. networks: 25\n",
            "Epoch: 9000, Train loss:  0.261 Test loss:  5.789 Ensemble loss:  0.230 RMSE: 0.845 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.078 Test loss:  4.870 Ensemble loss:  0.230 RMSE: 0.844 Num. networks: 30\n",
            "Train log. lik. =  -0.031 +/-  0.045\n",
            "Test  log. lik. =  -0.426 +/-  0.089\n",
            "Train RMSE      =   0.512 +/-  0.027\n",
            "Test  RMSE      =   0.912 +/-  0.072\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "data = pd.read_csv('/content/drive/MyDrive/bayesian-deep-learning-master/data/3_256_datsets.csv').values\n",
        "ensemble = train_mc_dropout(data=data, n_splits=10, burn_in=350, mix_time=300, num_nets=30,\n",
        "                            num_units=120, learn_rate=1e-2/len(data), weight_decay=1, log_every=1000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}