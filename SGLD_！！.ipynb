{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/SGLD_%EF%BC%81%EF%BC%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneWjImThPub",
        "outputId": "c931ceab-c216-42c6-a3ab-d459da83a786"
      },
      "id": "YneWjImThPub",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94c44267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c44267",
        "outputId": "fdadb5b1-9377-4fc1-c8f9-b3bc69fc7238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.7.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565154 sha256=0d0fa9b17f1d3925cdee75b6944e0159f84c331468bd70ada039b80dcfe7ffe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=dc5ed5521084d24dc311a7c84fce76c733c001b5509a0351fed7bf1bfdd8c8cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XkP-XKI5mOhv"
      },
      "id": "XkP-XKI5mOhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7f11def2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f11def2",
        "outputId": "5590532b-6048-4471-842b-0e65638e032c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "613d2696",
      "metadata": {
        "id": "613d2696"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b95dd8e4",
      "metadata": {
        "id": "b95dd8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6a5ba769",
      "metadata": {
        "id": "6a5ba769"
      },
      "outputs": [],
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dc555aa8",
      "metadata": {
        "id": "dc555aa8"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "24ee5406",
      "metadata": {
        "id": "24ee5406"
      },
      "outputs": [],
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "189bc6ae",
      "metadata": {
        "id": "189bc6ae"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, num_units)\n",
        "        self.layer3 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        #x_train = (x_train - x_means)/x_stds\n",
        "        #y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        #x_test = (x_test - x_means)/x_stds\n",
        "        #y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=Langevin_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=batch_size, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 0: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    return nets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "3b9afbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9afbcd",
        "outputId": "100a8777-4feb-4c0d-d08a-ad93dd817a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0:\n",
            "Epoch:    0, Train loss: 19.124 Test loss:  9.601 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.620 Test loss:  0.731 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.056 Test loss:  0.565 Ensemble loss:  0.563 RMSE: 1.039 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.094 Test loss:  0.878 Ensemble loss:  0.557 RMSE: 1.063 Num. networks:  4\n",
            "Epoch: 2000, Train loss:  0.041 Test loss:  0.572 Ensemble loss:  0.514 RMSE: 1.020 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.195 Test loss:  0.660 Ensemble loss:  0.477 RMSE: 1.001 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.102 Test loss:  0.973 Ensemble loss:  0.450 RMSE: 0.967 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.406 Test loss:  1.625 Ensemble loss:  0.458 RMSE: 0.980 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.161 Test loss:  1.646 Ensemble loss:  0.461 RMSE: 0.987 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.143 Test loss:  1.747 Ensemble loss:  0.448 RMSE: 0.984 Num. networks: 14\n",
            "Epoch: 5000, Train loss:  0.060 Test loss:  3.074 Ensemble loss:  0.437 RMSE: 0.981 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.036 Test loss:  9.934 Ensemble loss:  0.475 RMSE: 1.012 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.068 Test loss:  4.894 Ensemble loss:  0.480 RMSE: 1.013 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.278 Test loss:  3.279 Ensemble loss:  0.476 RMSE: 1.006 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.193 Test loss:  8.968 Ensemble loss:  0.469 RMSE: 0.991 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.230 Test loss:  2.120 Ensemble loss:  0.455 RMSE: 0.978 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.251 Test loss:  1.574 Ensemble loss:  0.456 RMSE: 0.981 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.168 Test loss:  1.516 Ensemble loss:  0.442 RMSE: 0.968 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.139 Test loss:  2.616 Ensemble loss:  0.431 RMSE: 0.959 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.170 Test loss:  5.038 Ensemble loss:  0.420 RMSE: 0.947 Num. networks: 30\n",
            "FOLD 1:\n",
            "Epoch:    0, Train loss: 18.777 Test loss: 10.392 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.727 Test loss:  0.747 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.087 Test loss:  1.663 Ensemble loss:  0.683 RMSE: 1.199 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.054 Test loss:  1.850 Ensemble loss:  0.795 RMSE: 1.235 Num. networks:  4\n",
            "Epoch: 2000, Train loss:  0.140 Test loss:  1.532 Ensemble loss:  0.727 RMSE: 1.245 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.036 Test loss:  3.030 Ensemble loss:  0.741 RMSE: 1.244 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.179 Test loss:  4.279 Ensemble loss:  0.747 RMSE: 1.257 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.151 Test loss:  6.029 Ensemble loss:  0.735 RMSE: 1.245 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.054 Test loss:  4.590 Ensemble loss:  0.673 RMSE: 1.218 Num. networks: 12\n",
            "Epoch: 4500, Train loss:  0.332 Test loss:  1.485 Ensemble loss:  0.601 RMSE: 1.157 Num. networks: 14\n",
            "Epoch: 5000, Train loss:  0.437 Test loss:  0.657 Ensemble loss:  0.570 RMSE: 1.123 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.014 Test loss:  1.078 Ensemble loss:  0.523 RMSE: 1.087 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.064 Test loss:  2.110 Ensemble loss:  0.497 RMSE: 1.067 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.735 Test loss:  0.620 Ensemble loss:  0.476 RMSE: 1.045 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.225 Test loss:  2.549 Ensemble loss:  0.457 RMSE: 1.022 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.085 Test loss:  0.983 Ensemble loss:  0.433 RMSE: 0.995 Num. networks: 24\n",
            "Epoch: 8000, Train loss:  0.269 Test loss:  0.774 Ensemble loss:  0.433 RMSE: 0.986 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.343 Test loss:  0.677 Ensemble loss:  0.427 RMSE: 0.972 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.212 Test loss:  2.845 Ensemble loss:  0.427 RMSE: 0.963 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.193 Test loss:  1.447 Ensemble loss:  0.423 RMSE: 0.960 Num. networks: 30\n",
            "FOLD 2:\n",
            "Epoch:    0, Train loss: 18.439 Test loss: 12.530 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.840 Test loss:  1.027 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.114 Test loss:  0.732 Ensemble loss:  0.523 RMSE: 1.122 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.071 Test loss:  1.415 Ensemble loss:  0.520 RMSE: 1.100 Num. networks:  4\n",
            "Epoch: 2000, Train loss:  0.132 Test loss:  0.768 Ensemble loss:  0.505 RMSE: 1.067 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.377 Test loss:  0.898 Ensemble loss:  0.502 RMSE: 1.068 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.031 Test loss:  1.442 Ensemble loss:  0.514 RMSE: 1.080 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.094 Test loss:  1.594 Ensemble loss:  0.513 RMSE: 1.084 Num. networks: 10\n",
            "Epoch: 4000, Train loss:  0.253 Test loss:  1.418 Ensemble loss:  0.525 RMSE: 1.104 Num. networks: 12\n",
            "Epoch: 4500, Train loss:  0.137 Test loss:  1.487 Ensemble loss:  0.520 RMSE: 1.117 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.147 Test loss:  3.299 Ensemble loss:  0.525 RMSE: 1.125 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.111 Test loss:  1.901 Ensemble loss:  0.534 RMSE: 1.123 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.193 Test loss:  0.660 Ensemble loss:  0.500 RMSE: 1.101 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.320 Test loss:  0.679 Ensemble loss:  0.474 RMSE: 1.089 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.122 Test loss:  1.679 Ensemble loss:  0.451 RMSE: 1.068 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.573 Test loss:  6.073 Ensemble loss:  0.424 RMSE: 1.043 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.141 Test loss: 15.691 Ensemble loss:  0.414 RMSE: 1.035 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.238 Test loss:  0.680 Ensemble loss:  0.393 RMSE: 1.009 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.105 Test loss:  5.443 Ensemble loss:  0.371 RMSE: 0.983 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.082 Test loss:  2.400 Ensemble loss:  0.363 RMSE: 0.978 Num. networks: 30\n",
            "FOLD 3:\n",
            "Epoch:    0, Train loss: 18.277 Test loss: 16.689 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.745 Test loss:  0.619 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.012 Test loss:  0.916 Ensemble loss:  0.130 RMSE: 0.765 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.098 Test loss:  0.954 Ensemble loss:  0.132 RMSE: 0.729 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.086 Test loss:  1.032 Ensemble loss:  0.115 RMSE: 0.714 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.086 Test loss:  1.007 Ensemble loss:  0.149 RMSE: 0.739 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.197 Test loss:  0.921 Ensemble loss:  0.162 RMSE: 0.734 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.148 Test loss:  3.149 Ensemble loss:  0.156 RMSE: 0.730 Num. networks: 10\n",
            "Epoch: 4000, Train loss:  0.180 Test loss:  1.031 Ensemble loss:  0.172 RMSE: 0.736 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.056 Test loss:  0.911 Ensemble loss:  0.164 RMSE: 0.728 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.164 Test loss:  1.514 Ensemble loss:  0.174 RMSE: 0.731 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.260 Test loss:  1.430 Ensemble loss:  0.181 RMSE: 0.738 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.115 Test loss:  1.606 Ensemble loss:  0.204 RMSE: 0.757 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.269 Test loss:  2.498 Ensemble loss:  0.205 RMSE: 0.762 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.187 Test loss:  2.818 Ensemble loss:  0.199 RMSE: 0.766 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.189 Test loss:  2.883 Ensemble loss:  0.202 RMSE: 0.765 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.248 Test loss:  1.883 Ensemble loss:  0.209 RMSE: 0.761 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.095 Test loss:  2.571 Ensemble loss:  0.213 RMSE: 0.759 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.191 Test loss:  1.577 Ensemble loss:  0.215 RMSE: 0.770 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.187 Test loss:  1.271 Ensemble loss:  0.219 RMSE: 0.775 Num. networks: 30\n",
            "FOLD 4:\n",
            "Epoch:    0, Train loss: 18.050 Test loss: 14.737 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.692 Test loss:  0.903 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.011 Test loss:  1.049 Ensemble loss:  0.966 RMSE: 1.260 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.149 Test loss:  0.936 Ensemble loss:  0.710 RMSE: 1.190 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.194 Test loss:  4.212 Ensemble loss:  0.646 RMSE: 1.133 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.203 Test loss:  3.298 Ensemble loss:  0.620 RMSE: 1.100 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.114 Test loss: 11.334 Ensemble loss:  0.624 RMSE: 1.068 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.103 Test loss:  2.125 Ensemble loss:  0.623 RMSE: 1.054 Num. networks: 10\n",
            "Epoch: 4000, Train loss:  0.171 Test loss:  2.568 Ensemble loss:  0.547 RMSE: 1.040 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.038 Test loss:  1.835 Ensemble loss:  0.522 RMSE: 1.032 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.193 Test loss:  2.252 Ensemble loss:  0.512 RMSE: 1.027 Num. networks: 15\n",
            "Epoch: 5500, Train loss:  0.389 Test loss:  0.704 Ensemble loss:  0.483 RMSE: 1.024 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.240 Test loss:  4.785 Ensemble loss:  0.472 RMSE: 1.025 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.049 Test loss: 13.132 Ensemble loss:  0.467 RMSE: 1.031 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.250 Test loss: 14.239 Ensemble loss:  0.466 RMSE: 1.031 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.149 Test loss:  9.771 Ensemble loss:  0.458 RMSE: 1.029 Num. networks: 24\n",
            "Epoch: 8000, Train loss:  0.176 Test loss:  5.405 Ensemble loss:  0.454 RMSE: 1.024 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.016 Test loss:  1.509 Ensemble loss:  0.445 RMSE: 1.013 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.103 Test loss:  1.505 Ensemble loss:  0.438 RMSE: 1.009 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.231 Test loss:  2.760 Ensemble loss:  0.433 RMSE: 1.006 Num. networks: 30\n",
            "FOLD 5:\n",
            "Epoch:    0, Train loss: 18.286 Test loss: 13.437 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.690 Test loss:  0.606 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.070 Test loss:  0.591 Ensemble loss:  0.467 RMSE: 1.044 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.013 Test loss:  0.830 Ensemble loss:  0.335 RMSE: 0.924 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.023 Test loss:  1.188 Ensemble loss:  0.326 RMSE: 0.896 Num. networks:  5\n",
            "Epoch: 2500, Train loss:  0.166 Test loss:  1.820 Ensemble loss:  0.365 RMSE: 0.900 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.517 Test loss:  1.309 Ensemble loss:  0.269 RMSE: 0.879 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.134 Test loss:  1.219 Ensemble loss:  0.279 RMSE: 0.887 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.160 Test loss:  3.293 Ensemble loss:  0.280 RMSE: 0.891 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.129 Test loss:  6.960 Ensemble loss:  0.264 RMSE: 0.897 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.191 Test loss:  7.651 Ensemble loss:  0.273 RMSE: 0.900 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.108 Test loss:  3.218 Ensemble loss:  0.296 RMSE: 0.900 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.011 Test loss:  8.522 Ensemble loss:  0.301 RMSE: 0.900 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.025 Test loss:  1.759 Ensemble loss:  0.302 RMSE: 0.900 Num. networks: 20\n",
            "Epoch: 7000, Train loss:  0.080 Test loss:  2.264 Ensemble loss:  0.296 RMSE: 0.910 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.042 Test loss:  1.569 Ensemble loss:  0.302 RMSE: 0.916 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.109 Test loss:  2.331 Ensemble loss:  0.305 RMSE: 0.918 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.079 Test loss:  7.752 Ensemble loss:  0.318 RMSE: 0.926 Num. networks: 27\n",
            "Epoch: 9000, Train loss:  0.319 Test loss:  1.469 Ensemble loss:  0.321 RMSE: 0.925 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.128 Test loss:  1.807 Ensemble loss:  0.330 RMSE: 0.930 Num. networks: 30\n",
            "FOLD 6:\n",
            "Epoch:    0, Train loss: 18.013 Test loss: 15.192 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.708 Test loss:  0.809 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.109 Test loss:  0.404 Ensemble loss:  0.522 RMSE: 1.128 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.052 Test loss:  0.392 Ensemble loss:  0.376 RMSE: 0.913 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.029 Test loss:  0.320 Ensemble loss:  0.359 RMSE: 0.863 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.045 Test loss:  0.798 Ensemble loss:  0.323 RMSE: 0.792 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.125 Test loss:  0.745 Ensemble loss:  0.265 RMSE: 0.781 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.160 Test loss:  1.943 Ensemble loss:  0.246 RMSE: 0.767 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.111 Test loss:  0.921 Ensemble loss:  0.240 RMSE: 0.750 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.160 Test loss:  1.554 Ensemble loss:  0.233 RMSE: 0.753 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.138 Test loss:  2.798 Ensemble loss:  0.242 RMSE: 0.763 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.098 Test loss:  2.117 Ensemble loss:  0.250 RMSE: 0.773 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.211 Test loss:  1.085 Ensemble loss:  0.248 RMSE: 0.777 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.135 Test loss:  0.658 Ensemble loss:  0.249 RMSE: 0.779 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.125 Test loss:  1.123 Ensemble loss:  0.256 RMSE: 0.777 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.250 Test loss:  1.138 Ensemble loss:  0.254 RMSE: 0.768 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.091 Test loss:  1.307 Ensemble loss:  0.251 RMSE: 0.768 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.081 Test loss:  1.201 Ensemble loss:  0.241 RMSE: 0.766 Num. networks: 27\n",
            "Epoch: 9000, Train loss:  0.025 Test loss:  2.245 Ensemble loss:  0.245 RMSE: 0.766 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.157 Test loss:  8.495 Ensemble loss:  0.244 RMSE: 0.769 Num. networks: 30\n",
            "FOLD 7:\n",
            "Epoch:    0, Train loss: 18.339 Test loss: 16.109 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.768 Test loss:  0.937 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.051 Test loss:  0.426 Ensemble loss:  0.366 RMSE: 1.128 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.027 Test loss:  1.247 Ensemble loss:  0.387 RMSE: 1.147 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.146 Test loss:  1.207 Ensemble loss:  0.353 RMSE: 1.109 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.074 Test loss:  1.404 Ensemble loss:  0.339 RMSE: 1.058 Num. networks:  7\n",
            "Epoch: 3000, Train loss: -0.111 Test loss:  1.468 Ensemble loss:  0.326 RMSE: 1.032 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.079 Test loss:  1.770 Ensemble loss:  0.346 RMSE: 1.046 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.160 Test loss:  1.255 Ensemble loss:  0.358 RMSE: 1.045 Num. networks: 12\n",
            "Epoch: 4500, Train loss:  0.101 Test loss:  1.775 Ensemble loss:  0.386 RMSE: 1.063 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.121 Test loss:  4.888 Ensemble loss:  0.385 RMSE: 1.065 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.122 Test loss:  2.098 Ensemble loss:  0.359 RMSE: 1.039 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.168 Test loss:  4.285 Ensemble loss:  0.345 RMSE: 1.016 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.032 Test loss:  2.850 Ensemble loss:  0.352 RMSE: 1.016 Num. networks: 20\n",
            "Epoch: 7000, Train loss: -0.132 Test loss:  2.131 Ensemble loss:  0.342 RMSE: 0.992 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.039 Test loss:  1.078 Ensemble loss:  0.344 RMSE: 0.989 Num. networks: 24\n",
            "Epoch: 8000, Train loss:  0.115 Test loss:  9.598 Ensemble loss:  0.341 RMSE: 0.984 Num. networks: 25\n",
            "Epoch: 8500, Train loss:  0.872 Test loss:  1.114 Ensemble loss:  0.340 RMSE: 0.980 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.051 Test loss: 11.655 Ensemble loss:  0.333 RMSE: 0.965 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.142 Test loss: 15.896 Ensemble loss:  0.350 RMSE: 0.972 Num. networks: 30\n",
            "FOLD 8:\n",
            "Epoch:    0, Train loss: 18.865 Test loss: 13.032 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.654 Test loss:  1.730 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.087 Test loss: 19.442 Ensemble loss:  1.022 RMSE: 1.154 Num. networks:  2\n",
            "Epoch: 1500, Train loss: -0.019 Test loss:  3.653 Ensemble loss:  0.807 RMSE: 1.090 Num. networks:  4\n",
            "Epoch: 2000, Train loss:  0.104 Test loss:  1.372 Ensemble loss:  0.521 RMSE: 1.038 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.049 Test loss:  1.094 Ensemble loss:  0.426 RMSE: 1.022 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.194 Test loss:  2.184 Ensemble loss:  0.400 RMSE: 1.009 Num. networks:  9\n",
            "Epoch: 3500, Train loss:  0.160 Test loss: 41.027 Ensemble loss:  0.417 RMSE: 1.007 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.208 Test loss: 209.652 Ensemble loss:  0.457 RMSE: 0.999 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.162 Test loss: 366.899 Ensemble loss:  0.478 RMSE: 1.009 Num. networks: 14\n",
            "Epoch: 5000, Train loss: -0.097 Test loss: 33.661 Ensemble loss:  0.490 RMSE: 1.013 Num. networks: 15\n",
            "Epoch: 5500, Train loss: -0.095 Test loss:  2.481 Ensemble loss:  0.487 RMSE: 1.011 Num. networks: 17\n",
            "Epoch: 6000, Train loss:  0.226 Test loss:  1.695 Ensemble loss:  0.384 RMSE: 1.001 Num. networks: 19\n",
            "Epoch: 6500, Train loss: -0.033 Test loss:  3.064 Ensemble loss:  0.385 RMSE: 0.995 Num. networks: 20\n",
            "Epoch: 7000, Train loss:  0.220 Test loss: 44.224 Ensemble loss:  0.380 RMSE: 1.002 Num. networks: 22\n",
            "Epoch: 7500, Train loss: -0.175 Test loss: 17.040 Ensemble loss:  0.395 RMSE: 1.012 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.006 Test loss: 13.980 Ensemble loss:  0.393 RMSE: 1.030 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.004 Test loss: 45.068 Ensemble loss:  0.389 RMSE: 1.048 Num. networks: 27\n",
            "Epoch: 9000, Train loss:  0.155 Test loss: 13.474 Ensemble loss:  0.385 RMSE: 1.063 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.185 Test loss: 13.786 Ensemble loss:  0.371 RMSE: 1.061 Num. networks: 30\n",
            "FOLD 9:\n",
            "Epoch:    0, Train loss: 18.337 Test loss: 12.398 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch:  500, Train loss:  0.631 Test loss:  0.667 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss:  0.145 Test loss:  0.706 Ensemble loss:  0.472 RMSE: 1.105 Num. networks:  2\n",
            "Epoch: 1500, Train loss:  0.176 Test loss:  1.747 Ensemble loss:  0.396 RMSE: 1.027 Num. networks:  4\n",
            "Epoch: 2000, Train loss: -0.088 Test loss:  0.633 Ensemble loss:  0.348 RMSE: 1.013 Num. networks:  5\n",
            "Epoch: 2500, Train loss: -0.058 Test loss:  1.075 Ensemble loss:  0.264 RMSE: 0.976 Num. networks:  7\n",
            "Epoch: 3000, Train loss:  0.061 Test loss:  1.325 Ensemble loss:  0.240 RMSE: 0.955 Num. networks:  9\n",
            "Epoch: 3500, Train loss: -0.144 Test loss:  3.076 Ensemble loss:  0.231 RMSE: 0.948 Num. networks: 10\n",
            "Epoch: 4000, Train loss: -0.059 Test loss:  2.315 Ensemble loss:  0.244 RMSE: 0.919 Num. networks: 12\n",
            "Epoch: 4500, Train loss: -0.006 Test loss:  1.191 Ensemble loss:  0.233 RMSE: 0.889 Num. networks: 14\n",
            "Epoch: 5000, Train loss:  0.002 Test loss:  2.005 Ensemble loss:  0.220 RMSE: 0.874 Num. networks: 15\n",
            "Epoch: 5500, Train loss:  0.130 Test loss:  1.403 Ensemble loss:  0.209 RMSE: 0.857 Num. networks: 17\n",
            "Epoch: 6000, Train loss: -0.069 Test loss:  3.085 Ensemble loss:  0.221 RMSE: 0.851 Num. networks: 19\n",
            "Epoch: 6500, Train loss:  0.085 Test loss:  2.847 Ensemble loss:  0.237 RMSE: 0.840 Num. networks: 20\n",
            "Epoch: 7000, Train loss:  0.124 Test loss:  3.188 Ensemble loss:  0.242 RMSE: 0.822 Num. networks: 22\n",
            "Epoch: 7500, Train loss:  0.018 Test loss:  2.912 Ensemble loss:  0.245 RMSE: 0.814 Num. networks: 24\n",
            "Epoch: 8000, Train loss: -0.092 Test loss:  4.044 Ensemble loss:  0.251 RMSE: 0.818 Num. networks: 25\n",
            "Epoch: 8500, Train loss: -0.065 Test loss:  5.896 Ensemble loss:  0.249 RMSE: 0.813 Num. networks: 27\n",
            "Epoch: 9000, Train loss: -0.091 Test loss:  3.999 Ensemble loss:  0.246 RMSE: 0.808 Num. networks: 29\n",
            "Epoch: 9350, Train loss: -0.124 Test loss:  3.709 Ensemble loss:  0.249 RMSE: 0.804 Num. networks: 30\n",
            "Train log. lik. =  -0.023 +/-  0.040\n",
            "Test  log. lik. =  -0.435 +/-  0.070\n",
            "Train RMSE      =   0.498 +/-  0.026\n",
            "Test  RMSE      =   0.920 +/-  0.096\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "data = pd.read_csv('/content/drive/MyDrive/bayesian-deep-learning-master/data/Datasets.csv').values\n",
        "ensemble = train_mc_dropout(data=data, n_splits=10, burn_in=350, mix_time=300, num_nets=30,\n",
        "                            num_units=110, learn_rate=1e-2/len(data), weight_decay=1, log_every=500)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}