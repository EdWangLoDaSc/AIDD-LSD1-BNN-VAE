{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/Dropout-as-a-Grid-Search_Representing-Model-Uncertainty-in-Deep-Learning/blob/main/SGLD_hybrid_120.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneWjImThPub",
        "outputId": "964eb184-50a9-4fa0-d26e-6c541e3e26db"
      },
      "id": "YneWjImThPub",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94c44267",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c44267",
        "outputId": "cc7f44b5-8498-4089-92b8-b9c16844ac78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.7.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565105 sha256=7ca15d376a6d05a92e78b520fc3f93e8eee104956159d9aad521d87dc90d8bfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=ecbbf57810d2de8fd48af15d76895c70afab74eee05ca9f18365f7c7dad57ef2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install GPy\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XkP-XKI5mOhv"
      },
      "id": "XkP-XKI5mOhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f11def2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7f11def2",
        "outputId": "23604b07-47f2-4cb2-fc59-02a3a3612a12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "613d2696",
      "metadata": {
        "id": "613d2696"
      },
      "outputs": [],
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out\n",
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b95dd8e4",
      "metadata": {
        "id": "b95dd8e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()\n",
        "\n",
        "\n",
        "def get_kl_divergence(weights, prior, varpost):\n",
        "    prior_loglik = prior.loglik(weights)\n",
        "    \n",
        "    varpost_loglik = varpost.loglik(weights)\n",
        "    varpost_lik = varpost_loglik.exp()\n",
        "    \n",
        "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
        "\n",
        "\n",
        "class gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def loglik(self, weights):\n",
        "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
        "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
        "        \n",
        "        return (exponent + log_coeff).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6a5ba769",
      "metadata": {
        "id": "6a5ba769"
      },
      "outputs": [],
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dc555aa8",
      "metadata": {
        "id": "dc555aa8"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "24ee5406",
      "metadata": {
        "id": "24ee5406"
      },
      "outputs": [],
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "189bc6ae",
      "metadata": {
        "id": "189bc6ae"
      },
      "outputs": [],
      "source": [
        "class Langevin_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(Langevin_Model_UCI, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # network with two hidden and one output layer\n",
        "        self.layer1 = Langevin_Layer(input_dim, num_units)\n",
        "        self.layer2 = Langevin_Layer(num_units, num_units)\n",
        "        self.layer3 = Langevin_Layer(num_units, 2*output_dim)\n",
        "        \n",
        "        # activation to be used between hidden layers\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x.view(-1, self.input_dim)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer2(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    \n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        x_train = (x_train - x_means)/x_stds\n",
        "        y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        x_test = (x_test - x_means)/x_stds\n",
        "        y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=Langevin_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=batch_size, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 0: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    return nets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from DataProcessing import DataProcessing as dp\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive')\n"
      ],
      "metadata": {
        "id": "engVhJi45CpW"
      },
      "id": "engVhJi45CpW",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install miceforest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT9MGz466Nnr",
        "outputId": "b17901f5-b732-4a5e-c433-57deac0ed889"
      },
      "id": "NT9MGz466Nnr",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting miceforest\n",
            "  Downloading miceforest-5.5.4-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting blosc\n",
            "  Downloading blosc-1.10.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from miceforest) (1.21.6)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from miceforest) (0.3.5.1)\n",
            "Collecting lightgbm>=3.3.1\n",
            "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 21.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.1->miceforest) (0.37.1)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.1->miceforest) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.1->miceforest) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm>=3.3.1->miceforest) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm>=3.3.1->miceforest) (3.1.0)\n",
            "Installing collected packages: lightgbm, blosc, miceforest\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed blosc-1.10.6 lightgbm-3.3.2 miceforest-5.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/DropoutUncertaintyExps-master/Hydrid_modeling')"
      ],
      "metadata": {
        "id": "sKERd3es7XXI"
      },
      "id": "sKERd3es7XXI",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3b9afbcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9afbcd",
        "outputId": "2c359509-5b43-4d31-c3e6-ecd2ac6dd426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " /content/drive/MyDrive/DataProcessing.py:19: SettingWithCopyWarning:\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            " /content/drive/MyDrive/DataProcessing.py:21: SettingWithCopyWarning:\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            " /content/drive/MyDrive/DataProcessing.py:23: SettingWithCopyWarning:\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            " /content/drive/MyDrive/DataProcessing.py:25: SettingWithCopyWarning:\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            " /content/drive/MyDrive/DataProcessing.py:26: SettingWithCopyWarning:\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.330 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -1.208 Test loss: -1.256 Ensemble loss: -0.468 RMSE: 248.286 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -1.084 Test loss: -1.007 Ensemble loss: -0.636 RMSE: 140.259 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -1.520 Test loss: -1.489 Ensemble loss: -0.832 RMSE: 112.777 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.498 Test loss: -0.436 Ensemble loss: -0.573 RMSE: 177.652 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -1.458 Test loss: -1.484 Ensemble loss: -0.320 RMSE: 150.384 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.109 Test loss: -0.194 Ensemble loss: -0.350 RMSE: 135.049 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -1.014 Test loss: -1.080 Ensemble loss: -0.326 RMSE: 147.446 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -1.463 Test loss: -1.440 Ensemble loss: -0.376 RMSE: 143.090 Num. networks: 25\n",
            "Epoch: 9000, Train loss:  0.117 Test loss: -0.115 Ensemble loss: -0.359 RMSE: 154.769 Num. networks: 29\n",
            "Epoch: 10000, Train loss: -1.315 Test loss: -1.184 Ensemble loss: -0.375 RMSE: 127.623 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -1.445 Test loss: -1.308 Ensemble loss: -0.388 RMSE: 136.122 Num. networks: 35\n",
            "Epoch: 12000, Train loss: -1.681 Test loss: -1.349 Ensemble loss: -0.440 RMSE: 134.739 Num. networks: 39\n",
            "Epoch: 12400, Train loss: -1.480 Test loss: -1.338 Ensemble loss: -0.451 RMSE: 134.305 Num. networks: 40\n",
            "FOLD 1:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.631 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -1.048 Test loss: -1.048 Ensemble loss: -0.402 RMSE: 236.787 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -1.450 Test loss: -1.496 Ensemble loss: -0.433 RMSE: 214.532 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.953 Test loss: -0.951 Ensemble loss: -0.568 RMSE: 145.441 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -1.238 Test loss: -1.269 Ensemble loss: -0.671 RMSE: 130.959 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -1.516 Test loss: -1.537 Ensemble loss: -0.704 RMSE: 134.111 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -1.500 Test loss: -0.473 Ensemble loss: -0.775 RMSE: 131.450 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -0.433 Test loss: -0.449 Ensemble loss: -0.767 RMSE: 118.187 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -0.873 Test loss: -0.934 Ensemble loss: -0.784 RMSE: 117.731 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -1.331 Test loss:  1.263 Ensemble loss: -0.783 RMSE: 104.614 Num. networks: 29\n",
            "Epoch: 10000, Train loss: -1.523 Test loss: -1.522 Ensemble loss: -0.800 RMSE: 99.678 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -1.309 Test loss: -1.359 Ensemble loss: -0.824 RMSE: 98.387 Num. networks: 35\n",
            "Epoch: 12000, Train loss: -1.399 Test loss: -1.442 Ensemble loss: -0.858 RMSE: 97.678 Num. networks: 39\n",
            "Epoch: 12400, Train loss: -0.599 Test loss: -0.716 Ensemble loss: -0.872 RMSE: 97.465 Num. networks: 40\n",
            "FOLD 2:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.719 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -1.249 Test loss: -1.301 Ensemble loss:  0.260 RMSE: 400.917 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -0.697 Test loss: -0.612 Ensemble loss: -0.221 RMSE: 122.231 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -1.115 Test loss: -1.156 Ensemble loss: -0.489 RMSE: 96.555 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -1.249 Test loss: -1.247 Ensemble loss: -0.555 RMSE: 89.264 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.120 Test loss: -0.326 Ensemble loss: -0.530 RMSE: 103.304 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -1.481 Test loss: -1.416 Ensemble loss: -0.608 RMSE: 97.553 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -1.421 Test loss: -1.308 Ensemble loss: -0.672 RMSE: 96.345 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -0.240 Test loss: -0.155 Ensemble loss: -0.553 RMSE: 93.061 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -1.497 Test loss: -1.399 Ensemble loss: -0.622 RMSE: 95.118 Num. networks: 29\n",
            "Epoch: 10000, Train loss: -1.454 Test loss: -1.417 Ensemble loss: -0.665 RMSE: 97.107 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -0.502 Test loss: -0.450 Ensemble loss: -0.694 RMSE: 99.451 Num. networks: 35\n",
            "Epoch: 12000, Train loss: -0.997 Test loss: -0.962 Ensemble loss: -0.652 RMSE: 105.240 Num. networks: 39\n",
            "Epoch: 12400, Train loss: -0.981 Test loss: -0.883 Ensemble loss: -0.663 RMSE: 105.085 Num. networks: 40\n",
            "FOLD 3:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.457 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.483 Test loss:  4.995 Ensemble loss: -0.577 RMSE: 269.840 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -1.342 Test loss: -1.381 Ensemble loss: -0.856 RMSE: 171.920 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.610 Test loss: -0.659 Ensemble loss: -0.495 RMSE: 182.302 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -1.193 Test loss: -1.230 Ensemble loss: -0.371 RMSE: 123.099 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.488 Test loss: -0.628 Ensemble loss: -0.435 RMSE: 123.225 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -1.518 Test loss: -1.527 Ensemble loss: -0.341 RMSE: 131.665 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -0.269 Test loss: -0.265 Ensemble loss: -0.403 RMSE: 128.857 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -1.656 Test loss: -1.627 Ensemble loss: -0.467 RMSE: 120.914 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -0.221 Test loss: -0.303 Ensemble loss: -0.502 RMSE: 107.442 Num. networks: 29\n",
            "Epoch: 10000, Train loss: -1.299 Test loss: -1.291 Ensemble loss: -0.547 RMSE: 101.038 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -1.465 Test loss: -1.445 Ensemble loss: -0.551 RMSE: 104.039 Num. networks: 35\n",
            "Epoch: 12000, Train loss: -1.529 Test loss: -0.477 Ensemble loss: -0.552 RMSE: 87.466 Num. networks: 39\n",
            "Epoch: 12400, Train loss: -1.721 Test loss: -1.699 Ensemble loss: -0.557 RMSE: 86.694 Num. networks: 40\n",
            "FOLD 4:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.700 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -1.301 Test loss: -1.277 Ensemble loss: -0.151 RMSE: 701.818 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -1.357 Test loss: -1.218 Ensemble loss: -0.517 RMSE: 387.892 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.490 Test loss: -0.307 Ensemble loss: -0.562 RMSE: 267.414 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -1.375 Test loss: -1.305 Ensemble loss: -0.627 RMSE: 239.540 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -1.430 Test loss: -0.340 Ensemble loss: -0.722 RMSE: 210.644 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.473 Test loss: -0.497 Ensemble loss: -0.747 RMSE: 182.500 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -0.532 Test loss: -0.477 Ensemble loss: -0.715 RMSE: 174.512 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -0.486 Test loss: -0.491 Ensemble loss: -0.770 RMSE: 166.847 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -1.081 Test loss: -0.999 Ensemble loss: -0.656 RMSE: 158.157 Num. networks: 29\n",
            "Epoch: 10000, Train loss: -0.694 Test loss: -0.667 Ensemble loss: -0.676 RMSE: 153.115 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -0.716 Test loss: -0.696 Ensemble loss: -0.710 RMSE: 149.610 Num. networks: 35\n",
            "Epoch: 12000, Train loss: -1.052 Test loss: -1.015 Ensemble loss: -0.735 RMSE: 143.378 Num. networks: 39\n",
            "Epoch: 12400, Train loss: -0.700 Test loss: -0.665 Ensemble loss: -0.746 RMSE: 142.534 Num. networks: 40\n",
            "FOLD 5:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.674 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -1.121 Test loss: -1.249 Ensemble loss: -0.981 RMSE: 166.955 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -1.069 Test loss: -1.093 Ensemble loss: -0.786 RMSE: 206.416 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.777 Test loss: -0.772 Ensemble loss: -0.678 RMSE: 196.593 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -1.362 Test loss: -1.338 Ensemble loss: -0.768 RMSE: 178.605 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -1.438 Test loss: -1.496 Ensemble loss: -0.833 RMSE: 161.782 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -1.616 Test loss: -1.524 Ensemble loss: -0.777 RMSE: 137.853 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -1.324 Test loss: -1.282 Ensemble loss: -0.804 RMSE: 125.724 Num. networks: 22\n",
            "Epoch: 8000, Train loss:  0.686 Test loss:  6.868 Ensemble loss: -0.844 RMSE: 121.683 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -1.050 Test loss: -1.020 Ensemble loss: -0.771 RMSE: 117.006 Num. networks: 29\n",
            "Epoch: 10000, Train loss: -1.529 Test loss: -1.450 Ensemble loss: -0.701 RMSE: 113.589 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -0.899 Test loss: -0.852 Ensemble loss: -0.714 RMSE: 108.308 Num. networks: 35\n",
            "Epoch: 12000, Train loss:  6.335 Test loss:  0.543 Ensemble loss: -0.698 RMSE: 103.310 Num. networks: 39\n",
            "Epoch: 12400, Train loss: -0.418 Test loss: -0.410 Ensemble loss: -0.711 RMSE: 102.837 Num. networks: 40\n",
            "FOLD 6:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.457 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.593 Test loss:  2.694 Ensemble loss: -1.142 RMSE: 175.671 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -0.800 Test loss: -0.829 Ensemble loss: -0.865 RMSE: 185.186 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -1.323 Test loss: -1.278 Ensemble loss: -0.856 RMSE: 143.853 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -1.510 Test loss: -1.361 Ensemble loss: -0.873 RMSE: 147.844 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -1.183 Test loss: -1.023 Ensemble loss: -0.413 RMSE: 168.323 Num. networks: 15\n",
            "Epoch: 6000, Train loss:  0.310 Test loss:  0.165 Ensemble loss: -0.408 RMSE: 157.452 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -1.033 Test loss: -0.956 Ensemble loss: -0.468 RMSE: 151.468 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -1.519 Test loss: -1.266 Ensemble loss: -0.361 RMSE: 133.598 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -1.103 Test loss: -1.015 Ensemble loss: -0.425 RMSE: 134.775 Num. networks: 29\n",
            "Epoch: 10000, Train loss: -1.509 Test loss: -1.288 Ensemble loss: -0.450 RMSE: 133.134 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -0.702 Test loss: -0.700 Ensemble loss: -0.483 RMSE: 132.780 Num. networks: 35\n",
            "Epoch: 12000, Train loss: -0.885 Test loss: -0.841 Ensemble loss: -0.529 RMSE: 133.448 Num. networks: 39\n",
            "Epoch: 12400, Train loss:  0.845 Test loss:  0.423 Ensemble loss: -0.534 RMSE: 133.854 Num. networks: 40\n",
            "FOLD 7:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.367 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -1.398 Test loss: -1.365 Ensemble loss: -0.694 RMSE: 321.708 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -0.999 Test loss:  2.184 Ensemble loss: -0.894 RMSE: 175.170 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -1.011 Test loss: -1.087 Ensemble loss: -0.884 RMSE: 153.867 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.169 Test loss: -0.330 Ensemble loss: -0.802 RMSE: 139.093 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -1.615 Test loss: -1.541 Ensemble loss: -0.415 RMSE: 141.209 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.970 Test loss: -1.018 Ensemble loss: -0.460 RMSE: 146.489 Num. networks: 19\n",
            "Epoch: 7000, Train loss:  2.688 Test loss:  6.715 Ensemble loss: -0.533 RMSE: 139.307 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -0.714 Test loss: -0.846 Ensemble loss: -0.586 RMSE: 134.580 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -0.293 Test loss: -0.360 Ensemble loss: -0.600 RMSE: 138.931 Num. networks: 29\n",
            "Epoch: 10000, Train loss: -1.600 Test loss: -1.612 Ensemble loss: -0.630 RMSE: 135.421 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -0.018 Test loss: -0.330 Ensemble loss: -0.655 RMSE: 133.643 Num. networks: 35\n",
            "Epoch: 12000, Train loss: -1.047 Test loss: -1.147 Ensemble loss: -0.696 RMSE: 129.423 Num. networks: 39\n",
            "Epoch: 12400, Train loss: -1.724 Test loss: -1.580 Ensemble loss: -0.706 RMSE: 128.533 Num. networks: 40\n",
            "FOLD 8:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.527 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -0.738 Test loss: -0.688 Ensemble loss:  0.171 RMSE: 653.847 Num. networks:  2\n",
            "Epoch: 2000, Train loss:  2.489 Test loss:  1.978 Ensemble loss:  0.296 RMSE: 374.485 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.956 Test loss: -0.899 Ensemble loss: -0.043 RMSE: 202.224 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -1.628 Test loss: -0.570 Ensemble loss: -0.129 RMSE: 198.129 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -0.074 Test loss: -0.144 Ensemble loss: -0.130 RMSE: 197.605 Num. networks: 15\n",
            "Epoch: 6000, Train loss: -0.934 Test loss: -0.923 Ensemble loss: -0.219 RMSE: 195.827 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -0.827 Test loss: -0.831 Ensemble loss: -0.276 RMSE: 190.134 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -1.466 Test loss: -1.349 Ensemble loss: -0.289 RMSE: 179.767 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -1.104 Test loss: -1.071 Ensemble loss: -0.347 RMSE: 170.934 Num. networks: 29\n",
            "Epoch: 10000, Train loss: -0.646 Test loss: -0.675 Ensemble loss: -0.389 RMSE: 168.724 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -1.537 Test loss: -1.483 Ensemble loss: -0.428 RMSE: 163.032 Num. networks: 35\n",
            "Epoch: 12000, Train loss: -1.610 Test loss: -1.577 Ensemble loss: -0.458 RMSE: 152.420 Num. networks: 39\n",
            "Epoch: 12400, Train loss: -0.994 Test loss: -1.005 Ensemble loss: -0.468 RMSE: 150.214 Num. networks: 40\n",
            "FOLD 9:\n",
            "Epoch:    0, Train loss:  0.500 Test loss:  0.336 Ensemble loss:    nan RMSE: nan Num. networks:  0\n",
            "Epoch: 1000, Train loss: -1.385 Test loss: -1.248 Ensemble loss: -0.659 RMSE: 183.666 Num. networks:  2\n",
            "Epoch: 2000, Train loss: -0.810 Test loss: -0.901 Ensemble loss: -0.888 RMSE: 148.880 Num. networks:  5\n",
            "Epoch: 3000, Train loss: -0.661 Test loss: -0.814 Ensemble loss: -0.795 RMSE: 165.166 Num. networks:  9\n",
            "Epoch: 4000, Train loss: -0.950 Test loss: -1.020 Ensemble loss: -0.825 RMSE: 160.068 Num. networks: 12\n",
            "Epoch: 5000, Train loss: -1.619 Test loss: -1.432 Ensemble loss: -0.719 RMSE: 167.802 Num. networks: 15\n",
            "Epoch: 6000, Train loss:  0.131 Test loss: -0.133 Ensemble loss: -0.518 RMSE: 209.174 Num. networks: 19\n",
            "Epoch: 7000, Train loss: -1.292 Test loss: -1.254 Ensemble loss: -0.554 RMSE: 203.314 Num. networks: 22\n",
            "Epoch: 8000, Train loss: -1.146 Test loss: -1.142 Ensemble loss: -0.598 RMSE: 188.071 Num. networks: 25\n",
            "Epoch: 9000, Train loss: -0.641 Test loss: -0.781 Ensemble loss: -0.593 RMSE: 169.339 Num. networks: 29\n",
            "Epoch: 10000, Train loss:  0.687 Test loss:  0.481 Ensemble loss: -0.620 RMSE: 165.328 Num. networks: 32\n",
            "Epoch: 11000, Train loss: -1.331 Test loss: -1.292 Ensemble loss: -0.657 RMSE: 161.762 Num. networks: 35\n",
            "Epoch: 12000, Train loss: -1.579 Test loss: -1.442 Ensemble loss: -0.621 RMSE: 176.153 Num. networks: 39\n",
            "Epoch: 12400, Train loss: -0.791 Test loss:  3.746 Ensemble loss: -0.627 RMSE: 175.080 Num. networks: 40\n",
            "Train log. lik. =  -6.155 +/-  0.152\n",
            "Test  log. lik. =  -6.185 +/-  0.134\n",
            "Train RMSE      = 113.236 +/- 12.505\n",
            "Test  RMSE      = 125.660 +/- 25.957\n"
          ]
        }
      ],
      "source": [
        "from DataProcessing import DataProcessing as dp\n",
        "doc_2020 = pd.read_csv('/content/drive/MyDrive/DropoutUncertaintyExps-master/Hydrid_modeling/EndUserData2020P2.csv', encoding = 'ISO-8859-1', skiprows = [0,2]).astype('float')\n",
        "doc_2021 = pd.read_csv('/content/drive/MyDrive/DropoutUncertaintyExps-master/Hydrid_modeling/EndUserData2021P2_fix.csv', encoding = 'ISO-8859-1', skiprows = [0,2]).astype('float')\n",
        "#print(doc_2020)\n",
        "\n",
        "doc_2020_value = dp(doc_2020, doc_2021).data_process_20().astype('float64')\n",
        "#dco_2021_value = dp(doc_2020, doc_2021).data_process_21().astype('float64')\n",
        "\n",
        "BNN_20 = pd.DataFrame(np.c_[doc_2020_value['NB2_S_1_NYZ_sys_x_PcwOut_x'], doc_2020_value['NB2_S_1_NYZ_sys_x_PcwIn_x'], doc_2020_value['NB2_S_1_NYZ_cwp_9_HzSPR_x'], doc_2020_value['NB2_S_1_NYZ_cwp_10_HzSPR_x'], doc_2020_value['NB2_S_1_NYZ_cwp_11_HzSPR_x'], doc_2020_value['NB2_S_1_NYZ_cwp_12_HzSPR_x'], doc_2020_value['p_diff'], doc_2020_value['NB2_S_x_NYZ_x_x_Fcw_x']], columns = ['NB2_S_1_NYZ_sys_x_PcwOut_x', 'NB2_S_1_NYZ_sys_x_PcwIn_x', 'NB2_S_1_NYZ_cwp_9_HzSPR_x', 'NB2_S_1_NYZ_cwp_10_HzSPR_x', 'NB2_S_1_NYZ_cwp_11_HzSPR_x', 'NB2_S_1_NYZ_cwp_12_HzSPR_x', 'diff', 'NB2_S_x_NYZ_x_x_Fcw_x']).values\n",
        "batch_size = 1024\n",
        "data = BNN_20\n",
        "ensemble = train_mc_dropout(data=data, n_splits=10, burn_in=400, mix_time=300, num_nets=40,\n",
        "                            num_units=120, learn_rate=1e-1/len(data), weight_decay=0.8, log_every=1000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}